<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Docker容器数据卷</title>
    <url>/2022/04/05/Docker%E5%AE%B9%E5%99%A8%E6%95%B0%E6%8D%AE%E5%8D%B7/</url>
    <content><![CDATA[<img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405151429046.png" alt="image-20220405151429046" style="zoom:50%;" />

<h2 id="什么是数据卷"><a href="#什么是数据卷" class="headerlink" title="什么是数据卷"></a>什么是数据卷</h2><p>数据不应该放在容器中，如果容器删除，数据就会丢失！==需求：数据 持久化==</p>
<p>MYSQL，容器删了=删库跑路！==需求：MYSQL数据可以存储在本地==</p>
<p>==&gt; 需要容器之间有一个数据共享的技术！Docker容器中产生的数据，同步到本地！</p>
<p>这就是卷技术！说白了就是目录的挂载，将容器内的目录，挂载到 Linux 上</p>
<img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405151429046.png" alt="image-20220405151429046" style="zoom:50%;" />

<p><strong>总结：容器的持久化和同步操作！ 容器间也可以数据共享！</strong></p>
<h2 id="使用数据卷"><a href="#使用数据卷" class="headerlink" title="使用数据卷"></a>使用数据卷</h2><blockquote>
<p>方式一：直接使用命令来挂载 -V</p>
</blockquote>
<span id="more"></span>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker run -it -v 主机目录：容器内目录 -p 主机端口：容器内端口 </span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 测试</span></span><br><span class="line">xxx@data:~$ sudo docker run -it -v /home/dutir/xxx/ceshi:/home centos /bin/bash</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看容器id</span></span><br><span class="line">xxx@data:~$ sudo docker ps</span><br><span class="line">CONTAINER ID        IMAGE                      COMMAND                  CREATED             STATUS                  PORTS                                                  NAMES</span><br><span class="line">43b6593461e7        centos                     &quot;/bin/bash&quot;              2 minutes ago       Up 2 minutes                                                                   quizzical_varahamihira</span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看容器详细信息</span></span><br><span class="line">xxx@data:~$ sudo docker inspect 43b6593461e7</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 看到挂载</span></span><br></pre></td></tr></table></figure>

<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405153329429.png" alt="image-20220405153329429" style="zoom:67%;" />、</p>
<p><strong>测试文件的同步</strong></p>
<img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405183512106.png" alt="image-20220405183512106" style="zoom:80%;" />



<p><strong>再来测试</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 1.停止容器</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 2.宿主机上修改文件</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 3.启动容器</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 4.容器内的数据依旧是同步的</span></span><br></pre></td></tr></table></figure>

<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405183540730.png" alt="image-20220405183540730"></p>
<p>好处：我们以后修改只需要在本地修改即可，容器内会自动同步！</p>
<h2 id="实战：安装MySQL"><a href="#实战：安装MySQL" class="headerlink" title="实战：安装MySQL"></a>实战：安装MySQL</h2><p>思考：MySQL 的数据持久化问题，</p>
<img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405160002622.png" alt="image-20220405160002622" style="zoom: 67%;" />

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 获取镜像</span></span><br><span class="line">docker pull mysql:5.7</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 运行容器，需要做数据挂载！  <span class="comment"># 安装启动mysql。需要配置密码，这是要注意的，去docker hub官方文档看（上图）</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 官方测试 $ docker run --name some-mysql -e MYSQL_ROOT_PASSWORD=my-secret-pw -d mysql:tag</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> -e 配置环境，这里要配置密码</span> </span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动我们的</span></span><br><span class="line">-d 后台运行</span><br><span class="line">-p 端口映射</span><br><span class="line">-v 数据卷挂载，可挂载多个</span><br><span class="line">-e 环境配置</span><br><span class="line">--name 容器名字</span><br><span class="line"></span><br><span class="line">xxx@data:~$ sudo docker run -d -p 3310:3306 -v /home/dutir/xxx/mysql_test/conf:/etc/mysql/conf.d -v /home/dutir/xxx/mysql_test/data:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=123456 --name mysql01 mysql:5.7</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动成功之后，我们在本地使用 sqlyog 连接测试</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> sqlyog 连接到本地的3310，本地3310侦听服务器宿主机的3310，宿主机的3310和容器内的3306映射，这个时候就可以连接上了</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 在本地sqlyog测试创建一个数据库，查看一下我们映射的路径是否ok！</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> test_for_docker以后，docker里多出这个数据库</span></span><br></pre></td></tr></table></figure>

<img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405183614297.png" alt="image-20220405183614297" style="zoom:67%;" />

<p>即使把容器删除</p>
<img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405183721449.png" alt="image-20220405183721449" style="zoom:67%;" />

<p>发现，我们挂载到本地的数据卷依旧没有丢失，这就实现了容器持久化功能！</p>
<h2 id="具名和匿名挂载"><a href="#具名和匿名挂载" class="headerlink" title="具名和匿名挂载"></a>具名和匿名挂载</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 匿名挂载（不指定主机名）</span></span><br><span class="line">-v 容器内路径</span><br><span class="line">docker run -d -P --name nginx01 -v /etc/nginx nginx</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看所有本地volume（数据卷）	乱码的都是匿名卷</span></span><br><span class="line">xxx@data:~$ sudo docker volume ls</span><br><span class="line">DRIVER              VOLUME NAME</span><br><span class="line">local               51ed233d17e5c740a92f70e075335dd59cbcf913dd03390d64880893a6a6b043</span><br><span class="line">local               mysql_lawbda_my-db</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 这里发现，这种就是匿名挂在，我们在 -v 时只写了容器内的路径，没有写容器外的路径！</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 具名挂载</span></span><br><span class="line">docker run -d -P --name nginx01 -v juming-nginx:/etc/nginx nginx</span><br><span class="line"></span><br><span class="line">xxx@data:~$ sudo docker volume ls</span><br><span class="line">DRIVER              VOLUME NAME</span><br><span class="line">local               51ed233d17e5c740a92f70e075335dd59cbcf913dd03390d64880893a6a6b043</span><br><span class="line">local				juming-nginx</span><br><span class="line">local               mysql_lawbda_my-db</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 通过 -v 卷名：容器内路径</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看一下这个卷（这里查看mysql_lawbda_my-db）</span></span><br><span class="line">xxx@data:~$ sudo docker volume inspect mysql_lawbda_my-db</span><br><span class="line">[</span><br><span class="line">    &#123;</span><br><span class="line">        &quot;CreatedAt&quot;: &quot;2022-04-04T17:14:49+08:00&quot;,</span><br><span class="line">        &quot;Driver&quot;: &quot;local&quot;,</span><br><span class="line">        &quot;Labels&quot;: &#123;</span><br><span class="line">            &quot;com.docker.compose.project&quot;: &quot;mysql_lawbda&quot;,</span><br><span class="line">            &quot;com.docker.compose.version&quot;: &quot;1.27.4&quot;,</span><br><span class="line">            &quot;com.docker.compose.volume&quot;: &quot;my-db&quot;</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;Mountpoint&quot;: &quot;/var/lib/docker/volumes/mysql_lawbda_my-db/_data&quot;,</span><br><span class="line">        &quot;Name&quot;: &quot;mysql_lawbda_my-db&quot;,</span><br><span class="line">        &quot;Options&quot;: null,</span><br><span class="line">        &quot;Scope&quot;: &quot;local&quot;</span><br></pre></td></tr></table></figure>

<p>所有达到docker容器内的卷，没有指定目录的情况下都是在<code>/val/lib/docker/volumes/xxxx/_data</code></p>
<img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405164118444.png" alt="image-20220405164118444" style="zoom: 67%;" />

<p>我们通过具名挂载 可以方便找到我们的一个卷，大多数情况在使用<code>具名挂载</code> </p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 如何确定是具名挂载还是匿名挂在，还是指定路径挂载！</span></span><br><span class="line"></span><br><span class="line">-v 容器内路径				 # 匿名挂载</span><br><span class="line">-v 卷名:容器内路径    			# 具名挂载</span><br><span class="line">-v /宿主机路径:容器内路径	   	  # 指定路径挂载 </span><br></pre></td></tr></table></figure>

<p>拓展：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 通过 -v 容器内路径:ro  rw  改变读写权限</span></span><br><span class="line">ro	read only	# 只读</span><br><span class="line">rw  readwrite	# 可读可写</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 一旦设置了容器权限，容器对我们挂载出来的内容就有限定了！</span></span><br><span class="line">docker run -d -P --name nginx01 -v juming-nginx:/etc/nginx:ro nginx</span><br><span class="line">docker run -d -P --name nginx01 -v juming-nginx:/etc/nginx:rw nginx</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> ro  只要看到ro，就说明这个路径只能通过宿主机来改变，容器内部是无法操作的</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="初识Dockerfile"><a href="#初识Dockerfile" class="headerlink" title="初识Dockerfile"></a>初识Dockerfile</h2><blockquote>
<p>方式二：生成镜像的时候就挂载出来</p>
</blockquote>
<p>Dockerfile 就是用来构建 docker 镜像的构建文件！ 就是一段命令脚本！先体验一下！</p>
<p>通过这个脚本可以生成镜像，镜像是一层一层的，脚本就是一个一个的命令，每个命令就是一层。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 创建一个dockerfile文件，名字可以随机， 建议 Dockerfile</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 文件中的内容  指令（都是大写） 参数</span></span><br><span class="line"></span><br><span class="line">FROM centos</span><br><span class="line"></span><br><span class="line">VOLUME [&quot;volume01&quot;,&quot;volume02&quot;]	</span><br><span class="line"></span><br><span class="line">CMD echo &quot;----end----&quot;</span><br><span class="line"></span><br><span class="line">CMD /bin/bash</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 这里的每个命令，就是镜像的一层！</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> docker build 构建镜像</span></span><br><span class="line">root@data:/home/dutir/xxx/docker-test-volume# docker build -f /home/dutir/xxx/docker-test-volume/dockerfile1 -t wjm/centos:1.0 .</span><br><span class="line">Sending build context to Docker daemon  2.048kB</span><br><span class="line">Step 1/4 : FROM centos</span><br><span class="line"><span class="meta"> ---&gt;</span><span class="bash"> 5d0da3dc9764</span></span><br><span class="line">Step 2/4 : VOLUME [&quot;volume01&quot;,&quot;volume02&quot;]</span><br><span class="line"><span class="meta"> ---&gt;</span><span class="bash"> Running <span class="keyword">in</span> eedc2cf7a5b3</span></span><br><span class="line">Removing intermediate container eedc2cf7a5b3</span><br><span class="line"><span class="meta"> ---&gt;</span><span class="bash"> 80396b388983</span></span><br><span class="line">Step 3/4 : CMD echo &quot;----end----&quot;</span><br><span class="line"><span class="meta"> ---&gt;</span><span class="bash"> Running <span class="keyword">in</span> 734dc31604b0</span></span><br><span class="line">Removing intermediate container 734dc31604b0</span><br><span class="line"><span class="meta"> ---&gt;</span><span class="bash"> 5ef9da00ac40</span></span><br><span class="line">Step 4/4 : CMD /bin/bash</span><br><span class="line"><span class="meta"> ---&gt;</span><span class="bash"> Running <span class="keyword">in</span> 593084c92533</span></span><br><span class="line">Removing intermediate container 593084c92533</span><br><span class="line"><span class="meta"> ---&gt;</span><span class="bash"> 8edda645979e</span></span><br><span class="line">Successfully built 8edda645979e</span><br><span class="line">Successfully tagged wjm/centos:1.0</span><br><span class="line">root@data:/home/dutir/xxx/docker-test-volume# </span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动一下自己生成的镜像</span></span><br><span class="line">root@data:/home/dutir/xxx/docker-test-volume# docker images</span><br><span class="line">REPOSITORY                  TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">wjm/centos                  1.0                 8edda645979e        4 minutes ago       231MB</span><br><span class="line">root@data:/home/dutir/xxx/docker-test-volume# docker run -it 8edda645979e /bin/bash</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405170430808.png" alt="image-20220405170430808"></p>
<p>这个目录就是我们生成镜像时自动挂载的，数据卷目录。</p>
<p>这个和外部一定有一个同步的目录！dockerfile里只指定了容器内目录，一定是匿名挂载，所以容器外、宿主机上肯定有乱码的挂载。</p>
<img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405170800369.png" alt="image-20220405170800369" style="zoom: 67%;" />

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> docker inspect 查看容器信息</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 找到 Mounts，查看卷挂载的路径，符合匿名挂载的默认路径/var/lib/docker/volumes/xxx/</span></span><br></pre></td></tr></table></figure>

<img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405171126981.png" alt="image-20220405171126981" style="zoom:67%;" />

<p>在容器里的 volume01 下新建一个 container.txt 文件。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@4b4b467cf777 /]# ls</span><br><span class="line">bin  etc   lib	  lost+found  mnt  proc  run   srv  tmp  var	   volume02</span><br><span class="line">dev  home  lib64  media       opt  root  sbin  sys  usr  volume01</span><br><span class="line">[root@4b4b467cf777 /]# cd volume01</span><br><span class="line">[root@4b4b467cf777 volume01]# ls</span><br><span class="line">[root@4b4b467cf777 volume01]# touch container.txt</span><br><span class="line">[root@4b4b467cf777 volume01]# ls</span><br><span class="line">container.txt</span><br><span class="line">[root@4b4b467cf777 volume01]# </span><br></pre></td></tr></table></figure>

<p>测试一下刚才的文件是否同步出去。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 复制 Mounts 下的路径 /var/lib/docker/volumes/951ce898d681397c29606b61bb9da1102d7594f4d0cd069c4b63fbd96b171f96/_data</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">cd</span> 到这里</span></span><br><span class="line"></span><br><span class="line">root@data:~# cd /var/lib/docker/volumes/951ce898d681397c29606b61bb9da1102d7594f4d0cd069c4b63fbd96b171f96/_data</span><br><span class="line">root@data:/var/lib/docker/volumes/951ce898d681397c29606b61bb9da1102d7594f4d0cd069c4b63fbd96b171f96/_data# ls</span><br><span class="line">container.txt		# 发现新建的 container.txt 文件</span><br><span class="line">root@data:/var/lib/docker/volumes/951ce898d681397c29606b61bb9da1102d7594f4d0cd069c4b63fbd96b171f96/_data#</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>这种方式我们未来使用的十分多，因为我们通常会构建自己的镜像！</p>
<p>假设构建镜像时没有挂载卷，要手动镜像挂载 -v 卷名:容器内路径 </p>
<h2 id="数据卷容器"><a href="#数据卷容器" class="headerlink" title="数据卷容器"></a>数据卷容器</h2><blockquote>
<p>即容器和容器间同步</p>
</blockquote>
<p>多个 Mysql 数据如何同步？</p>
<p>==所有 docker 之间共享的卷都是独立的，每个docker有自己的数据卷。新建时拷贝父容器的备份。所以只要有一份存在，数据都不会丢失。==</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405172422841.png" alt="image-20220405172422841"></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 启动3个容器，通过我们刚才自己写的镜像启动</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 首先启动 docker01</span> </span><br></pre></td></tr></table></figure>

<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405172959557.png" alt="image-20220405172959557"></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 启动 docker02，volume 从 docker01 继承</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> docker01 就叫做数据卷容器</span></span><br></pre></td></tr></table></figure>

<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405173218738.png" alt="image-20220405173218738"></p>
<p>在 docker01 修改数据， docker02 会同步吗？测试一下。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405173548023.png" alt="image-20220405173548023"></p>
<p>回到 docker02 查看</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405173743825.png" alt="image-20220405173743825"></p>
<p>docker01 创建的内容同步到 docker02 了</p>
<p>再从 docker01 继承，启动 docker03，新建 docker03.txt</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405174426597.png" alt="image-20220405174426597"></p>
<p>在 docker03 里创建的文件，在 docker01 和 docker02 里都同步了。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405175122829.png" alt="image-20220405175122829"></p>
<p>只要是通过 –volume-from ，我们就可以实现容器间的数据共享。</p>
<p>把 docker01 整个 rm 掉，docker02 和 docker03 的文件都还在。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 测试：可以删除docker01，查看一下docker02和docker03是否还可以访问这个文件</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 依旧可以访问</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 测试：查看宿主机的volume，还是存在</span></span><br><span class="line">root@data:~# cd /var/lib/docker/volumes/2bdf8b38df84ffea344d0f758e7be2c2045c54f270a9da362b12246a1d78d636/_data</span><br><span class="line">root@data:/var/lib/docker/volumes/2bdf8b38df84ffea344d0f758e7be2c2045c54f270a9da362b12246a1d78d636/_data# ls</span><br><span class="line">docker01.txt  docker03.txt</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>==所有 docker 之间共享的卷都是独立的，每个docker有自己的数据卷。新建时 <strong>拷贝</strong> 父容器的备份。所以只要有一份存在，数据都不会丢失。==</p>
<p>回到问题：多个 Mysql 数据如何同步？</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> -v 容器内路径，匿名挂载</span></span><br><span class="line"></span><br><span class="line">xxx@data:~$ sudo docker run -d -p 3310:3306 -v /etc/mysql/conf.d -v /var/lib/mysql -e MYSQL_ROOT_PASSWORD=123456 --name mysql01 mysql:5.7</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 用 --volumes-from 可以实现两个容器同步</span></span><br><span class="line">xxx@data:~$ sudo docker run -d -p 3310:3306  -e MYSQL_ROOT_PASSWORD=123456 --name mysql01 --volumes-from mysql01 mysql:5.7</span><br></pre></td></tr></table></figure>



<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>容器之间配置信息的传递，数据卷容器的生命周期一直持续到没有容器使用为止（所有容器都停止）。</p>
<p>但是一旦你持久化到了本地，这个时候，本地的数据是不会删除的。所以重要的文件存在宿主机上就好啦。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>以上是看 b 站教学视频记的笔记。</p>
<p>教程地址：<a href="https://www.bilibili.com/video/BV1og4y1q7M4?p=9">【狂神说Java】Docker最新超详细版教程通俗易懂_哔哩哔哩_bilibili</a></p>
]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker常用命令</title>
    <url>/2022/04/05/Docker%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[<h2 id="帮助命令"><a href="#帮助命令" class="headerlink" title="帮助命令"></a>帮助命令</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker version		# 显示docker 的版本信息</span><br><span class="line">docker info			# 显示docker 的系统信息，包括镜像和容器的数量</span><br><span class="line">docker 命令 --help 	# 帮助命令</span><br></pre></td></tr></table></figure>

<p>帮助文档的地址：<a href="https://docs.docker.com/reference/">参考文档|Docker 文档</a></p>
<h2 id="镜像命令"><a href="#镜像命令" class="headerlink" title="镜像命令"></a>镜像命令</h2><p><strong>docker images 查看所有本地的主机上的镜像</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">xxx@data:~/solrdata$ sudo docker images </span><br><span class="line">REPOSITORY                        TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">solr                              8                   ce1fcccc6f5e        5 days ago          563MB</span><br><span class="line">solr                              latest              ce1fcccc6f5e        5 days ago          563MB</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 解释</span></span><br><span class="line">REPOSITORY 镜像的仓库源</span><br><span class="line">TAG        镜像的标签</span><br><span class="line">IMAGE ID   镜像的id</span><br><span class="line">CREATE     镜像的创建时间</span><br><span class="line">SIZE       镜像的大小</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 可选项</span></span><br><span class="line">Options:</span><br><span class="line">  -a, --all             # 列出所有镜像</span><br><span class="line">  -q, --quiet           # 只显示镜像的id</span><br><span class="line"></span><br><span class="line">xxx@data:~/solrdata$ sudo docker images -aq</span><br><span class="line">ce1fcccc6f5e</span><br><span class="line">ce1fcccc6f5e</span><br></pre></td></tr></table></figure>

<span id="more"></span>

<p><strong>docker search 搜索镜像</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">xxx@data:~/solrdata$ sudo docker search mysql</span><br><span class="line">NAME                             DESCRIPTION                                     STARS               OFFICIAL            AUTOMATED</span><br><span class="line">mysql                            MySQL is a widely used, open-source relation…   12350               [OK]                </span><br><span class="line">mariadb                          MariaDB Server is a high performing open sou…   4753                [OK]                </span><br><span class="line">mysql/mysql-server               Optimized MySQL Server Docker images. Create…   916                                     [OK]</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 可选项，通过收藏来过滤</span></span><br><span class="line">--filter=STARS=3000		# 搜索出来的镜像就是STARS大于3000的</span><br><span class="line"></span><br><span class="line">xxx@data:~/solrdata$ sudo docker search mysql --filter=STARS=3000</span><br><span class="line">NAME                DESCRIPTION                                     STARS               OFFICIAL            AUTOMATED</span><br><span class="line">mysql               MySQL is a widely used, open-source relation…   12350               [OK]                </span><br><span class="line">mariadb             MariaDB Server is a high performing open sou…   4753                [OK]                </span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p><strong>docker pull 下载镜像</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 下载镜像 docker pull 镜像名[:tag]</span></span><br><span class="line">xxx@data:~/solrdata$ sudo docker pull mysql</span><br><span class="line">Using default tag: latest	# 如果不写tag,就默认latest最新版</span><br><span class="line">latest: Pulling from library/mysql</span><br><span class="line">f003217c5aae: Pull complete 	# 分层下载,docker image的核心 联合文件系统</span><br><span class="line">65d94f01a09f: Pull complete </span><br><span class="line">43d78aaa6078: Pull complete </span><br><span class="line">a0f91ffbdf69: Pull complete </span><br><span class="line">59ee9e07e12f: Pull complete </span><br><span class="line">04d82978082c: Pull complete </span><br><span class="line">70f46ebb971a: Pull complete </span><br><span class="line">db6ea71d471d: Pull complete </span><br><span class="line">c2920c795b25: Pull complete </span><br><span class="line">26c3bdf75ff5: Pull complete </span><br><span class="line">9ec1f1f78b0e: Pull complete </span><br><span class="line">4607fa685ac6: Pull complete </span><br><span class="line">Digest: sha256:1c75ba7716c6f73fc106dacedfdcf13f934ea8c161c8b3b3e4618bcd5fbcf195	# 签名</span><br><span class="line">Status: Downloaded newer image for mysql:latest</span><br><span class="line">docker.io/library/mysql:latest	# 真实地址</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 等价于它</span></span><br><span class="line">docker pull mysql</span><br><span class="line">docker pull docker.io/library/mysql:latest</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 指定版本下载</span></span><br><span class="line">xxx@data:~/solrdata$ sudo docker pull mysql:5.7</span><br><span class="line">5.7: Pulling from library/mysql</span><br><span class="line">f003217c5aae: Already exists </span><br><span class="line">65d94f01a09f: Already exists </span><br><span class="line">43d78aaa6078: Already exists </span><br><span class="line">a0f91ffbdf69: Already exists </span><br><span class="line">59ee9e07e12f: Already exists </span><br><span class="line">04d82978082c: Already exists </span><br><span class="line">70f46ebb971a: Already exists 	</span><br><span class="line">ba61822c65c2: Pull complete 	# 指定版本只需要下载和之前不同的部分</span><br><span class="line">dec59acdf78a: Pull complete </span><br><span class="line">0a05235a6981: Pull complete </span><br><span class="line">c87d621d6916: Pull complete </span><br><span class="line">Digest: sha256:1a73b6a8f507639a8f91ed01ace28965f4f74bb62a9d9b9e7378d5f07fab79dc</span><br><span class="line">Status: Downloaded newer image for mysql:5.7</span><br><span class="line">docker.io/library/mysql:5.7</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405183232675.png" alt="image-20220405183232675"></p>
<p><strong>docker rmi 删除镜像</strong></p>
<p>rmi=rm 删除 + i 镜像(image),删除镜像</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker rmi 镜像id 					# 删除指定镜像</span><br><span class="line">docker rmi 镜像id 镜像id 镜像id	 	 # 删除多个镜像</span><br><span class="line">docker rmi -f $(docker images -aq)	 # 删除全部镜像</span><br><span class="line"></span><br><span class="line">xxx@data:~/solrdata$ sudo docker rmi f26e21ddd20d	# 镜像id</span><br></pre></td></tr></table></figure>







<h2 id="容器命令"><a href="#容器命令" class="headerlink" title="容器命令"></a>容器命令</h2><p><strong>说明:我们有了镜像才可以创建容器,linux,下载一个centos镜像来测试学习</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker pull centos</span><br></pre></td></tr></table></figure>

<p><strong>新建容器并启动</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker run [可选参数] image</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 参数说明</span></span><br><span class="line">--name=“Nmae” 	容器名字  solr01 solr02 用来区分容器</span><br><span class="line">-d            	后台方式运行，类型nohup，docker里只要加上-d</span><br><span class="line">-it				使用交互方式运行，进入容器查看内容</span><br><span class="line">-p				指定容器的端口 -p 8080:8080</span><br><span class="line">	-p ip:主机端口：容器端口</span><br><span class="line">	-p 主机端口:容器端口（容器）</span><br><span class="line">	-p 容器端口</span><br><span class="line">	容器端口</span><br><span class="line">-P				随机指定端口</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 测试,启动并进入容器 -it交互模式</span></span><br><span class="line">xxx@data:~/solrdata$ sudo docker run -it centos /bin/bash</span><br><span class="line">[root@17e53f625cbc /]# ls	# 查看容器内部的centos,基础版本,很多命令是不完善的</span><br><span class="line">bin  dev  etc  home  lib  lib64  lost+found  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 从容器中退回主机</span></span><br><span class="line">[root@17e53f625cbc /]# exit</span><br><span class="line">exit</span><br><span class="line">xxx@data:~/solrdata$ ls</span><br><span class="line">data  log4j2.xml  logs</span><br><span class="line">xxx@data:~/solrdata$ </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>列出所有运行中的容器</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> docker ps 命令</span></span><br><span class="line"><span class="meta">		#</span><span class="bash"> 列出当前正在运行的容器</span></span><br><span class="line">-a		# 列出当前正在运行的容器+带出历史运行过的容器</span><br><span class="line">-n=?	# 显示最近创建的容器</span><br><span class="line">-q		# 只显示容器的编号</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">xxx@data:~/solrdata$ sudo docker ps</span><br><span class="line">CONTAINER ID        IMAGE                      COMMAND             CREATED             STATUS               PORTS               NAMES</span><br><span class="line">xxx@data:~/solrdata$ sudo docker ps -a</span><br><span class="line">CONTAINER ID        IMAGE                      COMMAND             CREATED             STATUS               PORTS               NAMES</span><br><span class="line">17e53f625cbc        centos                                                 &quot;/bin/bash&quot;              3 minutes ago       Exited (0) 58 seconds ago                                                            magical_haibt</span><br></pre></td></tr></table></figure>

<p><strong>退出容器</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">exit	# 直接容器停止并退出</span><br><span class="line">Crtl + P + Q # 容器不停止退出</span><br></pre></td></tr></table></figure>

<p><strong>删除容器</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker rm 容器id					# 删除指定容器,不能删除正在运行的容器,如果要强制删除,rm -f</span><br><span class="line">docker rm -f $(docker ps -aq)	 # 删除所有容器</span><br><span class="line">docker ps -a -q|xargs docker rm	 # 删除所有容器(高级)</span><br></pre></td></tr></table></figure>

<p><strong>启动和停止容器的操作</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker start 容器id		# 启动容器</span><br><span class="line">docker restart 容器id		# 重启容器</span><br><span class="line">docker stop 容器id		# 停止当前正在运行的容器</span><br><span class="line">docker kill 容器id		# 强制停止当前容器</span><br></pre></td></tr></table></figure>

<h2 id="常用其他命令"><a href="#常用其他命令" class="headerlink" title="常用其他命令"></a>常用其他命令</h2><p><strong>后台启动容器</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 命令 docker run -d 镜像名</span></span><br><span class="line">xxx@data:~/solrdata$ sudo docker run -d centos</span><br><span class="line">7c5bca7cedbe3d4c32aa5273b9068a9d154a2e5f7a665f5861f8369a1a592862</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 问题 docker ps 发现centos停止了</span></span><br><span class="line">xxx@data:~/solrdata$ sudo docker ps</span><br><span class="line">CONTAINER ID        IMAGE                      COMMAND             CREATED             STATUS               PORTS               NAMES</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 常见的坑,docker 容器使用后台运行,就必须要有一个前台进程,docker发现没有应用,就会自动停止</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> nginx,容器启动后,发现自己没有提供服务,就会立刻停止,就是没有程序了</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>查看日志</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 可选项</span></span><br><span class="line">Options:</span><br><span class="line">  -f, --follow         Follow log output</span><br><span class="line">      --tail string    Number of lines to show from the end of the logs (default &quot;all&quot;)</span><br><span class="line">  -t, --timestamps     Show timestamps</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 测试</span></span><br><span class="line">xxx@data:~/solrdata$ sudo docker logs -f -t 508f4663f65e # 没有日志</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 自己编写一段shell脚本</span></span><br><span class="line">&quot;while true;do echo wjm;sleep 1;done&quot;</span><br><span class="line">xxx@data:~/solrdata$ sudo docker run -d centos /bin/bash -c &quot;while true;do echo wjm;sleep 1;done&quot;</span><br><span class="line"></span><br><span class="line">xxx@data:~/solrdata$ sudo docker ps</span><br><span class="line">CONTAINER ID        IMAGE                      COMMAND                  CREATED             STATUS                  PORTS                                                  NAMES</span><br><span class="line">5a96132edf5d        centos                     &quot;/bin/bash -c &#x27;while…&quot;   55 seconds ago      Up 53 seconds                                                                  priceless_elbakyan</span><br><span class="line"><span class="meta">#</span><span class="bash"> 显示日志</span></span><br><span class="line"> -tf		   # 显示日志</span><br><span class="line"> --tail number # 要显示的日志条数</span><br></pre></td></tr></table></figure>

<p><strong>查看容器中的进程信息</strong> ps</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 命令 docker top 容器id</span></span><br><span class="line">xxx@data:~/solrdata$ sudo docker top 5a96132edf5d</span><br><span class="line">UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD</span><br><span class="line">root                4109297             4109277             0                   15:33               ?                   00:00:00            /bin/bash -c while true;do echo wjm;sleep 1;done</span><br><span class="line">root                4112148             4109297             0                   15:40               ?                   00:00:00            /usr/bin/coreutils --coreutils-prog-shebang=sleep /usr/bin/sleep 1</span><br></pre></td></tr></table></figure>

<p><strong>查看镜像的元数据</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 命令 docker inspect 容器id</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 测试</span></span><br><span class="line">xxx@data:~/solrdata$ sudo docker inspect 5a96132edf5d</span><br><span class="line">[</span><br><span class="line">    &#123;</span><br><span class="line">        &quot;Id&quot;: &quot;5a96132edf5d25f7b434f20e843e50129d6dbdacb101785827b75c0f96bf5c67&quot;,</span><br><span class="line">        &quot;Created&quot;: &quot;2022-04-04T07:33:47.224853805Z&quot;,</span><br><span class="line">        &quot;Path&quot;: &quot;/bin/bash&quot;,</span><br><span class="line">        &quot;Args&quot;: [</span><br><span class="line">            &quot;-c&quot;,</span><br><span class="line">            &quot;while true;do echo wjm;sleep 1;done&quot;</span><br><span class="line">        ],</span><br><span class="line">        &quot;State&quot;: &#123;</span><br><span class="line">            &quot;Status&quot;: &quot;running&quot;,</span><br><span class="line">            &quot;Running&quot;: true,</span><br><span class="line">            &quot;Paused&quot;: false,</span><br><span class="line">            &quot;Restarting&quot;: false,</span><br><span class="line">            &quot;OOMKilled&quot;: false,</span><br><span class="line">            &quot;Dead&quot;: false,</span><br><span class="line">            &quot;Pid&quot;: 4109297,</span><br><span class="line">            &quot;ExitCode&quot;: 0,</span><br><span class="line">            &quot;Error&quot;: &quot;&quot;,</span><br><span class="line">            &quot;StartedAt&quot;: &quot;2022-04-04T07:33:48.561816958Z&quot;,</span><br><span class="line">            &quot;FinishedAt&quot;: &quot;0001-01-01T00:00:00Z&quot;</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;Image&quot;: &quot;sha256:5d0da3dc976460b72c77d94c8a1ad043720b0416bfc16c52c45d4847e53fadb6&quot;,</span><br><span class="line">        &quot;ResolvConfPath&quot;: &quot;/var/lib/docker/containers/5a96132edf5d25f7b434f20e843e50129d6dbdacb101785827b75c0f96bf5c67/resolv.conf&quot;,</span><br><span class="line">        &quot;HostnamePath&quot;: &quot;/var/lib/docker/containers/5a96132edf5d25f7b434f20e843e50129d6dbdacb101785827b75c0f96bf5c67/hostname&quot;,</span><br><span class="line">        &quot;HostsPath&quot;: &quot;/var/lib/docker/containers/5a96132edf5d25f7b434f20e843e50129d6dbdacb101785827b75c0f96bf5c67/hosts&quot;,</span><br><span class="line">        &quot;LogPath&quot;: &quot;/var/lib/docker/containers/5a96132edf5d25f7b434f20e843e50129d6dbdacb101785827b75c0f96bf5c67/5a96132edf5d25f7b434f20e843e50129d6dbdacb101785827b75c0f96bf5c67-json.log&quot;,</span><br><span class="line">        &quot;Name&quot;: &quot;/priceless_elbakyan&quot;,</span><br><span class="line">        &quot;RestartCount&quot;: 0,</span><br><span class="line">        &quot;Driver&quot;: &quot;overlay2&quot;,</span><br><span class="line">        &quot;Platform&quot;: &quot;linux&quot;,</span><br><span class="line">        &quot;MountLabel&quot;: &quot;&quot;,</span><br><span class="line">        &quot;ProcessLabel&quot;: &quot;&quot;,</span><br><span class="line">        &quot;AppArmorProfile&quot;: &quot;docker-default&quot;,</span><br><span class="line">        &quot;ExecIDs&quot;: null,</span><br><span class="line">        &quot;HostConfig&quot;: &#123;</span><br><span class="line">            &quot;Binds&quot;: null,</span><br><span class="line">            &quot;ContainerIDFile&quot;: &quot;&quot;,</span><br><span class="line">            &quot;LogConfig&quot;: &#123;</span><br><span class="line">                &quot;Type&quot;: &quot;json-file&quot;,</span><br><span class="line">                &quot;Config&quot;: &#123;&#125;</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;NetworkMode&quot;: &quot;default&quot;,</span><br><span class="line">            &quot;PortBindings&quot;: &#123;&#125;,</span><br><span class="line">            &quot;RestartPolicy&quot;: &#123;</span><br><span class="line">                &quot;Name&quot;: &quot;no&quot;,</span><br><span class="line">                &quot;MaximumRetryCount&quot;: 0</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;AutoRemove&quot;: false,</span><br><span class="line">            &quot;VolumeDriver&quot;: &quot;&quot;,</span><br><span class="line">            &quot;VolumesFrom&quot;: null,</span><br><span class="line">            &quot;CapAdd&quot;: null,</span><br><span class="line">            &quot;CapDrop&quot;: null,</span><br><span class="line">            &quot;Capabilities&quot;: null,</span><br><span class="line">            &quot;Dns&quot;: [],</span><br><span class="line">            &quot;DnsOptions&quot;: [],</span><br><span class="line">            &quot;DnsSearch&quot;: [],</span><br><span class="line">            &quot;ExtraHosts&quot;: null,</span><br><span class="line">            &quot;GroupAdd&quot;: null,</span><br><span class="line">            &quot;IpcMode&quot;: &quot;private&quot;,</span><br><span class="line">            &quot;Cgroup&quot;: &quot;&quot;,</span><br><span class="line">            &quot;Links&quot;: null,</span><br><span class="line">            &quot;OomScoreAdj&quot;: 0,</span><br><span class="line">            &quot;PidMode&quot;: &quot;&quot;,</span><br><span class="line">            &quot;Privileged&quot;: false,</span><br><span class="line">            &quot;PublishAllPorts&quot;: false,</span><br><span class="line">            &quot;ReadonlyRootfs&quot;: false,</span><br><span class="line">            &quot;SecurityOpt&quot;: null,</span><br><span class="line">            &quot;UTSMode&quot;: &quot;&quot;,</span><br><span class="line">            &quot;UsernsMode&quot;: &quot;&quot;,</span><br><span class="line">            &quot;ShmSize&quot;: 67108864,</span><br><span class="line">            &quot;Runtime&quot;: &quot;runc&quot;,</span><br><span class="line">            &quot;ConsoleSize&quot;: [</span><br><span class="line">                0,</span><br><span class="line">                0</span><br><span class="line">            ],</span><br><span class="line">            &quot;Isolation&quot;: &quot;&quot;,</span><br><span class="line">            &quot;CpuShares&quot;: 0,</span><br><span class="line">            &quot;Memory&quot;: 0,</span><br><span class="line">            &quot;NanoCpus&quot;: 0,</span><br><span class="line">            &quot;CgroupParent&quot;: &quot;&quot;,</span><br><span class="line">            &quot;BlkioWeight&quot;: 0,</span><br><span class="line">            &quot;BlkioWeightDevice&quot;: [],</span><br><span class="line">            &quot;BlkioDeviceReadBps&quot;: null,</span><br><span class="line">            &quot;BlkioDeviceWriteBps&quot;: null,</span><br><span class="line">            &quot;BlkioDeviceReadIOps&quot;: null,</span><br><span class="line">            &quot;BlkioDeviceWriteIOps&quot;: null,</span><br><span class="line">            &quot;CpuPeriod&quot;: 0,</span><br><span class="line">            &quot;CpuQuota&quot;: 0,</span><br><span class="line">            &quot;CpuRealtimePeriod&quot;: 0,</span><br><span class="line">            &quot;CpuRealtimeRuntime&quot;: 0,</span><br><span class="line">            &quot;CpusetCpus&quot;: &quot;&quot;,</span><br><span class="line">            &quot;CpusetMems&quot;: &quot;&quot;,</span><br><span class="line">            &quot;Devices&quot;: [],</span><br><span class="line">            &quot;DeviceCgroupRules&quot;: null,</span><br><span class="line">            &quot;DeviceRequests&quot;: null,</span><br><span class="line">            &quot;KernelMemory&quot;: 0,</span><br><span class="line">            &quot;KernelMemoryTCP&quot;: 0,</span><br><span class="line">            &quot;MemoryReservation&quot;: 0,</span><br><span class="line">            &quot;MemorySwap&quot;: 0,</span><br><span class="line">            &quot;MemorySwappiness&quot;: null,</span><br><span class="line">            &quot;OomKillDisable&quot;: false,</span><br><span class="line">            &quot;PidsLimit&quot;: null,</span><br><span class="line">            &quot;Ulimits&quot;: null,</span><br><span class="line">            &quot;CpuCount&quot;: 0,</span><br><span class="line">            &quot;CpuPercent&quot;: 0,</span><br><span class="line">            &quot;IOMaximumIOps&quot;: 0,</span><br><span class="line">            &quot;IOMaximumBandwidth&quot;: 0,</span><br><span class="line">            &quot;MaskedPaths&quot;: [</span><br><span class="line">                &quot;/proc/asound&quot;,</span><br><span class="line">                &quot;/proc/acpi&quot;,</span><br><span class="line">                &quot;/proc/kcore&quot;,</span><br><span class="line">                &quot;/proc/keys&quot;,</span><br><span class="line">                &quot;/proc/latency_stats&quot;,</span><br><span class="line">                &quot;/proc/timer_list&quot;,</span><br><span class="line">                &quot;/proc/timer_stats&quot;,</span><br><span class="line">                &quot;/proc/sched_debug&quot;,</span><br><span class="line">                &quot;/proc/scsi&quot;,</span><br><span class="line">                &quot;/sys/firmware&quot;</span><br><span class="line">            ],</span><br><span class="line">            &quot;ReadonlyPaths&quot;: [</span><br><span class="line">                &quot;/proc/bus&quot;,</span><br><span class="line">                &quot;/proc/fs&quot;,</span><br><span class="line">                &quot;/proc/irq&quot;,</span><br><span class="line">                &quot;/proc/sys&quot;,</span><br><span class="line">                &quot;/proc/sysrq-trigger&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;GraphDriver&quot;: &#123;</span><br><span class="line">            &quot;Data&quot;: &#123;</span><br><span class="line">                &quot;LowerDir&quot;: &quot;/var/lib/docker/overlay2/a18d17416e95b7779000e3864c1a96bf7eed2d4784d3c84789971bbf77a49b5e-init/diff:/var/lib/docker/overlay2/81761618a33f3926b117dce5b1a9ae7094d898b3d32f20d50da147a2c0c1dfd0/diff&quot;,</span><br><span class="line">                &quot;MergedDir&quot;: &quot;/var/lib/docker/overlay2/a18d17416e95b7779000e3864c1a96bf7eed2d4784d3c84789971bbf77a49b5e/merged&quot;,</span><br><span class="line">                &quot;UpperDir&quot;: &quot;/var/lib/docker/overlay2/a18d17416e95b7779000e3864c1a96bf7eed2d4784d3c84789971bbf77a49b5e/diff&quot;,</span><br><span class="line">                &quot;WorkDir&quot;: &quot;/var/lib/docker/overlay2/a18d17416e95b7779000e3864c1a96bf7eed2d4784d3c84789971bbf77a49b5e/work&quot;</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;Name&quot;: &quot;overlay2&quot;</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;Mounts&quot;: [],</span><br><span class="line">        &quot;Config&quot;: &#123;</span><br><span class="line">            &quot;Hostname&quot;: &quot;5a96132edf5d&quot;,</span><br><span class="line">            &quot;Domainname&quot;: &quot;&quot;,</span><br><span class="line">            &quot;User&quot;: &quot;&quot;,</span><br><span class="line">            &quot;AttachStdin&quot;: false,</span><br><span class="line">            &quot;AttachStdout&quot;: false,</span><br><span class="line">            &quot;AttachStderr&quot;: false,</span><br><span class="line">            &quot;Tty&quot;: false,</span><br><span class="line">            &quot;OpenStdin&quot;: false,</span><br><span class="line">            &quot;StdinOnce&quot;: false,</span><br><span class="line">            &quot;Env&quot;: [</span><br><span class="line">                &quot;PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin&quot;</span><br><span class="line">            ],</span><br><span class="line">            &quot;Cmd&quot;: [</span><br><span class="line">                &quot;/bin/bash&quot;,</span><br><span class="line">                &quot;-c&quot;,</span><br><span class="line">                &quot;while true;do echo wjm;sleep 1;done&quot;</span><br><span class="line">            ],</span><br><span class="line">            &quot;Image&quot;: &quot;centos&quot;,</span><br><span class="line">            &quot;Volumes&quot;: null,</span><br><span class="line">            &quot;WorkingDir&quot;: &quot;&quot;,</span><br><span class="line">            &quot;Entrypoint&quot;: null,</span><br><span class="line">            &quot;OnBuild&quot;: null,</span><br><span class="line">            &quot;Labels&quot;: &#123;</span><br><span class="line">                &quot;org.label-schema.build-date&quot;: &quot;20210915&quot;,</span><br><span class="line">                &quot;org.label-schema.license&quot;: &quot;GPLv2&quot;,</span><br><span class="line">                &quot;org.label-schema.name&quot;: &quot;CentOS Base Image&quot;,</span><br><span class="line">                &quot;org.label-schema.schema-version&quot;: &quot;1.0&quot;,</span><br><span class="line">                &quot;org.label-schema.vendor&quot;: &quot;CentOS&quot;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;NetworkSettings&quot;: &#123;</span><br><span class="line">            &quot;Bridge&quot;: &quot;&quot;,</span><br><span class="line">            &quot;SandboxID&quot;: &quot;e8757375dc8ce5c604047bace5801859349b35be01c4d0983f6e18aefad6d7c8&quot;,</span><br><span class="line">            &quot;HairpinMode&quot;: false,</span><br><span class="line">            &quot;LinkLocalIPv6Address&quot;: &quot;&quot;,</span><br><span class="line">            &quot;LinkLocalIPv6PrefixLen&quot;: 0,</span><br><span class="line">            &quot;Ports&quot;: &#123;&#125;,</span><br><span class="line">            &quot;SandboxKey&quot;: &quot;/var/run/docker/netns/e8757375dc8c&quot;,</span><br><span class="line">            &quot;SecondaryIPAddresses&quot;: null,</span><br><span class="line">            &quot;SecondaryIPv6Addresses&quot;: null,</span><br><span class="line">            &quot;EndpointID&quot;: &quot;35abfaa04e627a268c7c0a2279bbd919c831c66dcb7e87ca7afa95694b2c6cd0&quot;,</span><br><span class="line">            &quot;Gateway&quot;: &quot;172.17.0.1&quot;,</span><br><span class="line">            &quot;GlobalIPv6Address&quot;: &quot;&quot;,</span><br><span class="line">            &quot;GlobalIPv6PrefixLen&quot;: 0,</span><br><span class="line">            &quot;IPAddress&quot;: &quot;172.17.0.5&quot;,</span><br><span class="line">            &quot;IPPrefixLen&quot;: 16,</span><br><span class="line">            &quot;IPv6Gateway&quot;: &quot;&quot;,</span><br><span class="line">            &quot;MacAddress&quot;: &quot;02:42:ac:11:00:05&quot;,</span><br><span class="line">            &quot;Networks&quot;: &#123;</span><br><span class="line">                &quot;bridge&quot;: &#123;</span><br><span class="line">                    &quot;IPAMConfig&quot;: null,</span><br><span class="line">                    &quot;Links&quot;: null,</span><br><span class="line">                    &quot;Aliases&quot;: null,</span><br><span class="line">                    &quot;NetworkID&quot;: &quot;99b4b446329155dc0d91f3989f0cd78ac1c354467b34c72c1ac8a51085632eb8&quot;,</span><br><span class="line">                    &quot;EndpointID&quot;: &quot;35abfaa04e627a268c7c0a2279bbd919c831c66dcb7e87ca7afa95694b2c6cd0&quot;,</span><br><span class="line">                    &quot;Gateway&quot;: &quot;172.17.0.1&quot;,</span><br><span class="line">                    &quot;IPAddress&quot;: &quot;172.17.0.5&quot;,</span><br><span class="line">                    &quot;IPPrefixLen&quot;: 16,</span><br><span class="line">                    &quot;IPv6Gateway&quot;: &quot;&quot;,</span><br><span class="line">                    &quot;GlobalIPv6Address&quot;: &quot;&quot;,</span><br><span class="line">                    &quot;GlobalIPv6PrefixLen&quot;: 0,</span><br><span class="line">                    &quot;MacAddress&quot;: &quot;02:42:ac:11:00:05&quot;,</span><br><span class="line">                    &quot;DriverOpts&quot;: null</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">]</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>进入当前正在运行的容器</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 我们通常容器都是使用后台方式运行的,需要进入容器修改一些配置</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 命令1 docker <span class="built_in">exec</span> -it 容器id bashShell</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 测试</span></span><br><span class="line">xxx@data:~/solrdata$ sudo docker ps</span><br><span class="line">CONTAINER ID        IMAGE        COMMAND        CREATED        STATUS        PORTS        NAMES</span><br><span class="line">5a96132edf5d        centos        &quot;/bin/bash -c &#x27;while…&quot;        12 minutes ago        Up 12 minutes        priceless_elbakyan</span><br><span class="line"></span><br><span class="line">xxx@data:~/solrdata$ sudo docker exec -it 5a96132edf5d /bin/bash</span><br><span class="line">[root@5a96132edf5d /]# ls</span><br><span class="line">bin  dev  etc  home  lib  lib64  lost+found  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var</span><br><span class="line">[root@5a96132edf5d /]# ps -ef</span><br><span class="line">UID          PID    PPID  C STIME TTY          TIME CMD</span><br><span class="line">root           1       0  0 07:33 ?        00:00:00 /bin/bash -c while true;do echo wjm;sleep 1;done</span><br><span class="line">root         750       0  0 07:46 pts/0    00:00:00 /bin/bash</span><br><span class="line">root         771       1  0 07:46 ?        00:00:00 /usr/bin/coreutils --coreutils-prog-shebang=sleep /usr/bin/sleep 1</span><br><span class="line">root         772     750  0 07:46 pts/0    00:00:00 ps -ef</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 命令2 docker attach 容器id</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 测试</span></span><br><span class="line">xxx@data:~/solrdata$ sudo docker attach 5a96132edf5d </span><br><span class="line">正在执行当前代码...</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> docker <span class="built_in">exec</span>		<span class="comment"># 进入容器后开启一个新的终端,可以在里面操作(常用)</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> docker attach		<span class="comment"># 进入容器正在执行的终端,不会启动新的进程!,while true的话就死循环了</span></span></span><br></pre></td></tr></table></figure>

<p><strong>从容器内拷贝文件到主机上</strong></p>
<p>容器内和容器外是隔离的,如何拷贝?</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220404155319337.png" alt="image-20220404155319337"></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 命令 docker cp 容器id:容器内路径 目的的主机路径</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看帮助 看到是可以双向拷贝的</span></span><br><span class="line">xxx@data:~$ sudo docker cp --help</span><br><span class="line">Usage:	docker cp [OPTIONS] CONTAINER:SRC_PATH DEST_PATH|-</span><br><span class="line">	docker cp [OPTIONS] SRC_PATH|- CONTAINER:DEST_PATH</span><br><span class="line"></span><br><span class="line">Copy files/folders between a container and the local filesystem</span><br><span class="line"></span><br><span class="line">Use &#x27;-&#x27; as the source to read a tar archive from stdin</span><br><span class="line">and extract it to a directory destination in a container.</span><br><span class="line">Use &#x27;-&#x27; as the destination to stream a tar archive of a</span><br><span class="line">container source to stdout.</span><br><span class="line"></span><br><span class="line">Options:</span><br><span class="line">  -a, --archive       Archive mode (copy all uid/gid information)</span><br><span class="line">  -L, --follow-link   Always follow symbol link in SRC_PATH</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 测试</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看当前主机目录下的文件</span></span><br><span class="line">xxx@data:~$ ls</span><br><span class="line">solrdata  total.csv</span><br><span class="line"><span class="meta">#</span><span class="bash"> 进入docker容器内部</span></span><br><span class="line">xxx@data:~$ sudo docker attach 825f1e107b64</span><br><span class="line">[root@825f1e107b64 /]# ls</span><br><span class="line">bin  dev  etc  home  lib  lib64  lost+found  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var</span><br><span class="line">[root@825f1e107b64 /]# cd home/</span><br><span class="line"><span class="meta">#</span><span class="bash"> 在容器内新建一个文件</span></span><br><span class="line">[root@825f1e107b64 home]# touch test.java</span><br><span class="line">[root@825f1e107b64 home]# ls</span><br><span class="line">test.java</span><br><span class="line">[root@825f1e107b64 home]# exit</span><br><span class="line">exit</span><br><span class="line"><span class="meta">#</span><span class="bash"> 容器内/home/test.java拷贝到主机上的默认目录</span></span><br><span class="line">xxx@data:~$ sudo docker cp 825f1e107b64:/home/test.java .	</span><br><span class="line">xxx@data:~$ ls</span><br><span class="line">solrdata  test.java  total.csv</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 拷贝是一个手动过程,未来我们使用 -v 卷的技术可以实现自动同步 /home /home</span></span><br></pre></td></tr></table></figure>

<h2 id="小节"><a href="#小节" class="headerlink" title="小节"></a>小节</h2><p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220404163030861.png" alt="image-20220404163030861"></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Commands:</span><br><span class="line">  attach      Attach to a running container						# 当前 shell 下 attach 连接指定运行镜像</span><br><span class="line">  build       Build an image from a Dockerfile					# 通过 Dockerfile 定制镜像</span><br><span class="line">  commit      Create a new image from a container&#x27;s changes		# 提交当前容器为新镜像</span><br><span class="line">  cp          Copy files/folders between a container and the local filesystem	# 从容器中拷贝文件到宿主机中</span><br><span class="line">  create      Create a new container							# 创建一个新的容器,同 run ,但不启动容器</span><br><span class="line">  diff        Inspect changes  on a container&#x27;s filesystem		# 查看 docker 容器的变化</span><br><span class="line">  events      Get real time events from the server				# 从 docker 服务获取容器实时事件</span><br><span class="line">  exec        Run a command in a running container				# 再已存在的容器中运行命令</span><br><span class="line">  export      Export a container&#x27;s filesystem as a tar archive	# 导出容器的内容流作为一个 tar 归档文件[对应 import ]</span><br><span class="line">  history     Show the history of an image						# 展示一个镜像形成历史</span><br><span class="line">  images      List images										# 列出系统当前镜像</span><br><span class="line">  import      Import the contents from a tarball to create a filesystem image	# 从 tar 包中的内容创建一个新的文件系统映像[对应 import ]</span><br><span class="line">  info        Display system-wide information					# 展示系统相关信息</span><br><span class="line">  inspect     Return low-level information on Docker objects	# 查看容器详细信息</span><br><span class="line">  kill        Kill one or more running containers				# kill 指定容器</span><br><span class="line">  load        Load an image from a tar archive or STDIN			# 从 tar 包中加载一个镜像[对应 sava ]</span><br><span class="line">  login       Log in to a Docker registry						# 注册或者登录一个 docker 源服务器</span><br><span class="line">  logout      Log out from a Docker registry					# 退出登录</span><br><span class="line">  logs        Fetch the logs of a container						# 输出容器的日志</span><br><span class="line">  pause       Pause all processes within one or more containers	# 暂停容器</span><br><span class="line">  port        List port mappings or a specific mapping for the container	# 查看映射端口对应的容器内部源端口</span><br><span class="line">  ps          List containers									# 列出容器列表</span><br><span class="line">  pull        Pull an image or a repository from a registry		# 从 docker 镜像源服务器拉取指定镜像</span><br><span class="line">  push        Push an image or a repository to a registry		# 推送指定镜像至 docker 镜像源服务器</span><br><span class="line">  rename      Rename a container								# 重命名容器</span><br><span class="line">  restart     Restart one or more containers					# 重构其容器</span><br><span class="line">  rm          Remove one or more containers						# 删除一个或多个容器</span><br><span class="line">  rmi         Remove one or more images							# 删除一个或多个镜像[无容器使用该镜像时才能删除,否则需要删除相关容器才能继续,或者 -f 强制删除]</span><br><span class="line">  run         Run a command in a new container					# 创建一个新的容器并运行命令</span><br><span class="line">  save        Save one or more images to a tar archive (streamed to STDOUT by default)	# 保存镜像为 tar 包[对应 load ]</span><br><span class="line">  search      Search the Docker Hub for images					# 在 docker hub 中搜索镜像</span><br><span class="line">  start       Start one or more stopped containers				# 启动容器</span><br><span class="line">  stats       Display a live stream of container(s) resource usage statistics	# 展示容器的实时资源占用情况</span><br><span class="line">  stop        Stop one or more running containers				# 停止容器</span><br><span class="line">  tag         Create a tag TARGET_IMAGE that refers to SOURCE_IMAGE	# 给源中镜像打标签</span><br><span class="line">  top         Display the running processes of a container		# 查看容器中运行的进程信息</span><br><span class="line">  unpause     Unpause all processes within one or more containers	# 取消暂停容器</span><br><span class="line">  update      Update configuration of one or more containers	# 更新容器的配置</span><br><span class="line">  version     Show the Docker version information				# 查看 docker 版本号</span><br><span class="line">  wait        Block until one or more containers stop, then print their exit codes	# 截取容器停止时的退出状态值</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>docker命令很多,上面都是最常用的.</p>
<h2 id="作业练习"><a href="#作业练习" class="headerlink" title="作业练习"></a>作业练习</h2><h3 id="Docker-安装-Nginx"><a href="#Docker-安装-Nginx" class="headerlink" title="Docker 安装 Nginx"></a>Docker 安装 Nginx</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 1. 搜索镜像 search ,建议到 docker hub 上搜索查看详细文档</span> </span><br><span class="line"><span class="meta">#</span><span class="bash"> 2. 下载镜像 pull</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 3. 运行测试</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> -d 后台运行</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> --name 给容器命名</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> -p 宿主机(linux服务器)端口,容器(docker)内部端口</span></span><br><span class="line">xxx@data:~$ sudo docker run -d --name nginx01 -p 3344:80 nginx</span><br><span class="line">659cd3e2b3825a85c7e4b02ad2fe52be267fa2fd5425391162b859fec5f3c9c4</span><br><span class="line">xxx@data:~$ sudo docker ps</span><br><span class="line">CONTAINER ID        IMAGE             COMMAND             CREATED             STATUS             PORTS             NAMES</span><br><span class="line">659cd3e2b382        nginx             &quot;/docker-entrypoint.…&quot;             33 seconds ago             Up 31 seconds             0.0.0.0:3344-&gt;80/tcp             nginx01</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 测试访问成功</span></span><br><span class="line">xxx@data:~$ curl localhost:3344</span><br><span class="line">&lt;!DOCTYPE html&gt;</span><br><span class="line">&lt;html&gt;</span><br><span class="line">&lt;head&gt;</span><br><span class="line">&lt;title&gt;Welcome to nginx!&lt;/title&gt;</span><br><span class="line">.......</span><br><span class="line">&lt;/body&gt;</span><br><span class="line">&lt;/html&gt;</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 进入容器</span></span><br><span class="line">xxx@data:~$ sudo docker exec -it nginx01 /bin/bash</span><br><span class="line">root@659cd3e2b382:/# whereis nginx</span><br><span class="line">nginx: /usr/sbin/nginx /usr/lib/nginx /etc/nginx /usr/share/nginx</span><br><span class="line">root@659cd3e2b382:/# cd /etc/nginx</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p><strong>端口暴露的概念</strong></p>
<img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220404172634182.png" alt="image-20220404172634182" style="zoom:50%;" />





<p>思考问题: 我们每次改动nginx配置文件,都需要进入容器内部 ? 十分麻烦,要是可以在容器外部提供一个映射路径,达到在容器外部修改文件,容器内部就可以自动修改?   <strong>-v 数据卷</strong></p>
<h3 id="Docker-安装-tomcat"><a href="#Docker-安装-tomcat" class="headerlink" title="Docker 安装 tomcat"></a>Docker 安装 tomcat</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 官方的使用</span></span><br><span class="line">docker run -it --rm tomcat:9.0</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 我们之前的使用都是 -d 后台,停止容器后,容器还是可以查到.--rm 表示用完就删.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 所以 docker run -it --rm tomcat:9.0 一般用来测试,用完即删</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 下载再启动</span></span><br><span class="line">docker pull tomcat:9.0</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动运行</span></span><br><span class="line">docker run -d -p 3344:8080 --name tomcat01 tomcat</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 测试访问没有问题</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 进入容器</span></span><br><span class="line"></span><br><span class="line">xxx@data:~$ sudo docker exec -it tomcat01 /bin/bash</span><br><span class="line">root@e145ea35d953:/usr/local/tomcat# ls</span><br><span class="line">BUILDING.txt	 LICENSE  README.md	 RUNNING.txt  conf  logs	    temp     webapps.dist</span><br><span class="line">CONTRIBUTING.md  NOTICE   RELEASE-NOTES  bin	      lib   native-jni-lib  webapps  work</span><br><span class="line">root@e145ea35d953:/usr/local/tomcat# ls -al</span><br><span class="line">total 176</span><br><span class="line">drwxr-xr-x 1 root root  4096 Apr  1 19:34 .</span><br><span class="line">drwxr-xr-x 1 root root  4096 Mar 30 05:14 ..</span><br><span class="line">-rw-r--r-- 1 root root 19004 Mar 31 14:24 BUILDING.txt</span><br><span class="line">-rw-r--r-- 1 root root  6210 Mar 31 14:24 CONTRIBUTING.md</span><br><span class="line">-rw-r--r-- 1 root root 60269 Mar 31 14:24 LICENSE</span><br><span class="line">-rw-r--r-- 1 root root  2333 Mar 31 14:24 NOTICE</span><br><span class="line">-rw-r--r-- 1 root root  3378 Mar 31 14:24 README.md</span><br><span class="line">-rw-r--r-- 1 root root  6905 Mar 31 14:24 RELEASE-NOTES</span><br><span class="line">-rw-r--r-- 1 root root 16507 Mar 31 14:24 RUNNING.txt</span><br><span class="line">drwxr-xr-x 2 root root  4096 Apr  1 19:34 bin</span><br><span class="line">drwxr-xr-x 1 root root  4096 Apr  4 09:43 conf</span><br><span class="line">drwxr-xr-x 2 root root  4096 Apr  1 19:34 lib</span><br><span class="line">drwxrwxrwx 1 root root  4096 Apr  4 09:43 logs</span><br><span class="line">drwxr-xr-x 2 root root  4096 Apr  1 19:34 native-jni-lib</span><br><span class="line">drwxrwxrwx 2 root root  4096 Apr  1 19:34 temp</span><br><span class="line">drwxr-xr-x 2 root root  4096 Apr  1 19:34 webapps</span><br><span class="line">drwxr-xr-x 7 root root  4096 Mar 31 14:24 webapps.dist</span><br><span class="line">drwxrwxrwx 2 root root  4096 Mar 31 14:24 work</span><br><span class="line">root@e145ea35d953:/usr/local/tomcat# cd webapps</span><br><span class="line">root@e145ea35d953:/usr/local/tomcat/webapps# ls</span><br><span class="line">root@e145ea35d953:/usr/local/tomcat/webapps# </span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 发现问题 1.linux命令少了  2. 404 not found --&gt;没有webapps    是镜像的原因,默认是最小的镜像,所有不必要的都剔除了</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 保证最小可运行环境</span></span><br></pre></td></tr></table></figure>



<img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220404174410059.png" alt="image-20220404174410059" style="zoom:50%;" />



<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 更改配置再访问</span></span><br><span class="line">root@e145ea35d953:/usr/local/tomcat# cp -r webapps.dist/* webapps</span><br><span class="line">root@e145ea35d953:/usr/local/tomcat# cd webapps</span><br><span class="line">root@e145ea35d953:/usr/local/tomcat/webapps# ls</span><br><span class="line">ROOT  docs  examples  host-manager  manager</span><br></pre></td></tr></table></figure>

<p>404 变成了完整页面, 全程都是在docker里修改,同样可以部署</p>
<img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220404175010930.png" alt="image-20220404175010930" style="zoom:50%;" />



<p>思考问题: 我们以后要部署项目,如果每次都要进入容器是不是十分麻烦?要是可以在容器外部提供一个映射路径,webapps,我们在外部放置项目,就自动同步到内部就好了!</p>
<p>现在 docker 容器: tomcat + 网站内容 ; 如果将来 docker 容器放 mysql + 数据库数据 , 再如果把容器删了 , 就相当于删库跑路了…. 非常不科学 . (期待 -v 数据卷 如何解决)</p>
<h3 id="Docker-部署-es-kibana-没有实际部署"><a href="#Docker-部署-es-kibana-没有实际部署" class="headerlink" title="Docker 部署 es + kibana(没有实际部署)"></a>Docker 部署 es + kibana(没有实际部署)</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> es 暴露的端口很多</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> es 十分耗内存</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> es 的数据一般需要放置到安全目录! 挂载</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> --net somenetwork ? 网络配置</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 下载 启动</span></span><br><span class="line">docker run -d --name elasticsearch01 --net somenetwork -p 9200:9200 -p 9300:9300 -e &quot;discovery.type=single-node&quot; elasticsearch:7.6.2</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动了 Linux就卡住了   因为非常占内存</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> docker stats 查看cpu的状态</span></span><br></pre></td></tr></table></figure>



<img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220404181625831.png" alt="image-20220404181625831" style="zoom:67%;" />



<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 赶紧关闭,增加内存限制,修改配置文件 -e 环境配置修改 限制最小最大占用内存</span></span><br><span class="line">docker run -d --name elasticsearch02 --net somenetwork -p 9200:9200 -p 9300:9300 -e &quot;discovery.type=single-node&quot; -e ES_JAVA_OPTS=&quot;-Xms64m -Xmx512m&quot; elasticsearch:7.6.2</span><br><span class="line"><span class="meta">#</span><span class="bash"> docker stats 查看cpu的状态</span></span><br></pre></td></tr></table></figure>



<img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220404181645136.png" alt="image-20220404181645136" style="zoom: 67%;" />



<p>使用 kibana 连接 es ? 思考网络如何连接.</p>
<img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220404182053887.png" alt="image-20220404182053887" style="zoom:50%;" />

<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>以上是看 b 站教学视频记的笔记。</p>
<p>教程地址：<a href="https://www.bilibili.com/video/BV1og4y1q7M4?p=9">【狂神说Java】Docker最新超详细版教程通俗易懂_哔哩哔哩_bilibili</a></p>
]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker镜像原理</title>
    <url>/2022/04/05/Docker%E9%95%9C%E5%83%8F%E5%8E%9F%E7%90%86/</url>
    <content><![CDATA[<img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220404192342184.png" alt="image-20220404192342184" style="zoom:67%;" />

<h2 id="镜像"><a href="#镜像" class="headerlink" title="镜像"></a>镜像</h2><p>镜像：一种轻量级、可执行的独立软件包，用来打包软件运行环境和基于运行环境开发的软件，包含运行某个软件所需的所有内容，包括代码、库、环境变量和配置文件。</p>
<p>每个文件叠加过后就是我们的应用，虽然是叠加而来，但是对外却是一个整体的系统文件</p>
<h2 id="镜像加载原理"><a href="#镜像加载原理" class="headerlink" title="镜像加载原理"></a>镜像加载原理</h2><blockquote>
<p>UFS文件系统</p>
</blockquote>
<p>下载时看到一层层的就是这个。</p>
<p><strong>联合文件系统（UnionFS）</strong>是一种分层、轻量级并且高性能的文件系统，它支持对文件系统的修改作为一次提交来一层层的叠加，同时可以将不同目录挂载到同一个虚拟文件系统下。联合文件系统是 Docker 镜像的基础。镜像可以通过分层来进行继承，基于基础镜像（没有父镜像），可以制作各种具体的应用镜像。另外，不同 Docker 容器就可以共享一些基础的文件系统层，同时再加上自己独有的改动层，大大提高了存储的效率。</p>
<p><strong>特性</strong>：一次性同时加载多个文件系统，但从外面看起来，只能看到一个文件系统，联合加载会把各层文件系统叠加起来，这样最终的文件系统会包含所有底层的文件和目录。</p>
<blockquote>
<p>Docker镜像加载原理</p>
</blockquote>
<span id="more"></span>

<ul>
<li>bootfs(boot file system) 主要包含bootloader和kernel, bootloader 主要是 **引导加载 **kernel,当我们加载镜像的时候，会通过bootloader加载kernal，Docker镜像最底层是bootfs，当boot加载完成后整个kernal内核都在内存中了，bootfs也就可以卸载，值得注意的是，bootfs是被所有镜像共用的，许多镜像images都是在base image(rootfs)基础上叠加的 .</li>
</ul>
<ul>
<li>rootfs (root file system)，在bootfs之 上.包含的就是典型Linux系统中的/dev, /proc, /bin, /etc等标准目录和文件。rootfs就是<strong>各种不同的操作系统发行版</strong>，比如Ubuntu, Centos等等 。所以说每个 docker 就是一个小的虚拟机环境。</li>
</ul>
<img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220404192342184.png" alt="image-20220404192342184" style="zoom:67%;" />

<p>Linux 不同发行版之间，bootfs 都是一样的，只有 rootfs 不一样 。 所以当 Docker 镜像加载时， bootfs 直接使用宿主机的内核，只需要提供 rootfs ，这部分十分精简，所以 centos 镜像可以很小（才不到300M），并且加载速度很快（虚拟机启动分钟级，容器启动秒级）。</p>
<h2 id="分层理解"><a href="#分层理解" class="headerlink" title="分层理解"></a>分层理解</h2><blockquote>
<p> 镜像分层</p>
</blockquote>
<p>用 <code>docker inspect</code> 观察下载下来的镜像：</p>
<img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220404194829940.png" alt="image-20220404194829940" style="zoom: 50%;" />

<p>这里的 Layers 就是每一层的文件，UFS叠加以后成为整个镜像。</p>
<blockquote>
<p>特性</p>
</blockquote>
<p>以 tomcat 镜像为例，它是一个有6个层级的镜像，pull到本地，再创建一个新的容器，此时整个 tomcat 会作为一整个镜像层，而你做的所有操作都会记录在容器层。如果想保存新的镜像，镜像层和容器层会再次打包，形成一个新的镜像。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220404194637448.png" alt="image-20220404194637448"></p>
<h2 id="commit镜像"><a href="#commit镜像" class="headerlink" title="commit镜像"></a>commit镜像</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker commit 提交热熔器成为一个新的副本</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 命令和git类似</span></span><br><span class="line">docker commit -m=&quot;提交的描述信息&quot; -a=&quot;作者&quot; 容器id 目标镜像名：[TAG]</span><br></pre></td></tr></table></figure>

<p>测试</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 1.启动一个默认的tomcat</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 2.发现这个默认的tomcat 是没有webapps应用，这是镜像的原因，官方的镜像默认 webapps下面是没有文件的</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 3.自己拷贝了一些基本文件</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 4.将修改过的容器通过commit提交为一个镜像！我们以后就是用修改过的镜像就可以，这就是我们自己的一个修改过的镜像</span></span><br></pre></td></tr></table></figure>



<img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220405183351927.png" alt="image-20220405183351927" style="zoom:80%;" />

<p>入门Docker！接下来，容器数据卷！DokcerFile！Docker网络！</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>以上是看 b 站教学视频记的笔记。</p>
<p>教程地址：<a href="https://www.bilibili.com/video/BV1og4y1q7M4?p=9">【狂神说Java】Docker最新超详细版教程通俗易懂_哔哩哔哩_bilibili</a></p>
]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title>论文笔记：《GAG：Global Attributed Graph Neural Network for Streaming Session-based Recommendation》</title>
    <url>/2022/03/13/GAG/</url>
    <content><![CDATA[<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220309191028621.png" alt="image-20220309191028621"></p>
<hr>
<p>原paper：<a href="https://doi.org/10.1145/3397271.3401109"><em>GAG: Global Attributed Graph Neural Network for Streaming Session-Based Recommendation</em></a></p>
<p>源码解读：（近期发布）</p>
<hr>
<p>中译：基于流会话推荐的全局属性图神经网络</p>
<p>总结：将SSRM的encoder部分换成了图神经网络模型，并且沿用了NARM、SRGNN等采用的注意力机制，将用户信息作为全局信息融入GNN模型中，解决了保存用户长期兴趣的问题；改进了reservior的采样策略：计算推荐结果和真实交互的Wasserstein距离作为信息量指标，从而计算采样概率，改进采样策略。</p>
<p>展望：如何引入跨会话信息到SSR问题中，十分值得研究。</p>
<hr>
<span id="more"></span>

<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li><p>question作者想解决什么问题？ </p>
<p>1）SR任务中，用户信息常常被忽略，所以难以抓住用户的长期兴趣。SSR任务中，流数据常常是单个交互而不是会话数据。</p>
<p>2）如何设计适合SSR的通用的reservoir。</p>
</li>
<li><p>method作者通过什么理论/模型来解决这个问题？</p>
<p>针对1），作者提出 <strong>G</strong>lobal <strong>A</strong>ttributed <strong>G</strong>raph （GAG）neural network，全局属性的图神经网络。每当新数据到达时，GAG可以同时考虑全局属性和当前场景，以获得会话和用户的更全面的表示。</p>
<p>针对2）作者提出了 Wasserstein 存储库，帮助保存历史数据的<strong>代表性</strong>画像。</p>
</li>
<li><p>answer作者给出的答案是什么？</p>
<p>GAG + Wasserstein reservoir，取得了SOTA。</p>
</li>
</ul>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li><p>why作者为什么研究这个课题？    </p>
<p>会话推荐的发展现状：大部分会话推荐模型都专注与静态场景，“流会话”(Streaming session)的设定更贴近实际，但却很少研究。由于用户的喜好在随时间变化，所以对新数据做预测，仍然使用在原来数据上训练的静态模型是不合理的，为了更准确地捕捉用户兴趣，模型应该用最新的数据在线更新。</p>
<p>流推荐的发展现状：一些方法利用了存储技术解决流任务，但是它们的缺点是，交互数据都已相同概率存储到存储库中，这是一种离散的方式存储会话，有信息损失，捕捉不了连续的会话序列模式。还有一些在线学习的方法，每当新数据到来，模型就相应地更新，这会导致模型对新数据过拟合并且无法有效保留用户的长期兴趣。</p>
<p>只有SSRM提出了一个结合两者的解决方案，但有不足之处。</p>
</li>
<li><p>how当前研究到了哪一阶段？</p>
<p>SSRM。SSRM有两方面不足：1）计算信息量时，需要预先获得所有物品的隐含表示（MF方法得到的），别的方法不适用。2）模型将会话推荐的方法（GRU4Rec）和矩阵分解（MF）直接结合，这里原文是用MF计算的权重，对GRU4Rec的隐藏状态加权求和。很难学到用户和物品间更复杂的关联。</p>
</li>
</ul>
<h2 id="Dataset-amp-Metric"><a href="#Dataset-amp-Metric" class="headerlink" title="Dataset &amp; Metric"></a>Dataset &amp; Metric</h2><ul>
<li>数据来源 </li>
</ul>
<p>LastFM：<a href="http://mtg.upf.edu/static/datasets/last.fm/lastfm-dataset-1K.tar.gz">http://mtg.upf.edu/static/datasets/last.fm/lastfm-dataset-1K.tar.gz</a></p>
<p>Gowalla：<a href="https://snap.stanford.edu/data/loc-gowalla.html">https://snap.stanford.edu/data/loc-gowalla.html</a></p>
<ul>
<li>数据划分</li>
</ul>
<p>给数据集 $D$ 中的会话按时间排序，分成前60%作为训练集，和后40%作为候选集。为了模拟线上的流数据输入，将候选集再划分成5个等长切片作为测试机。第一个测试机和10%的训练集作为验证集。实验中，若要预测第 $i$ 个测试集的序列行为，那么 $i$ 之前的测试集切片都用作在线训练。</p>
<ul>
<li>重要指标 </li>
</ul>
<p>MRR@20、Recall@20</p>
<h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><ul>
<li>会话推荐Session-based Recommendation<ul>
<li>属于序列推荐</li>
</ul>
</li>
<li>流推荐Streaming Recommendation<ul>
<li>属于在线学习Online Learning，大多数在线学习更关注新数据，往往不能记忆历史交互</li>
<li>随机采样Random Sampling 方法是为了解决 “历史遗忘“问题而提出的，它通过引入一个储存库来保存用户的长期交互。</li>
</ul>
</li>
</ul>
<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><h3 id="任务定义"><a href="#任务定义" class="headerlink" title="任务定义"></a>任务定义</h3><p>Item embdding： $ x_i =Embed_v(v_i)$ ；User embdding： $ p_j =Embed_u(u_j)$  ；</p>
<p>User u 在时间步 t 的会话序列：$ S_{u,t}=[v_1,v_2,…,v_l]$ </p>
<p>任务目标，根据用户 $u$ 的历史会话 ${S_{u,0},S_{u,1},…,S_{u,t}}$ 预测下一个可能交互的物品 $ v_{t+1}$ </p>
<p>与会话推荐不同的是，这里假设所有会话 $S$ 都以很快的速度达到，所以受限于算力，必须选取高效的方式处理历史会话信息和当前会话信息。而会话推荐，没有流数据这一设定，可以同时处理用户所有序列，不必考虑效率。</p>
<h3 id="GAG模型框架"><a href="#GAG模型框架" class="headerlink" title="GAG模型框架"></a>GAG模型框架</h3><p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220309205703438.png" alt="image-20220309205703438"></p>
<p>GAG的主要工作由两部分构成，1）GAG model：将用户信息转化为全局属性并将其融入到会话图中；2）Wasserstein reservoir存储库策略用来学习流数据。</p>
<h3 id="Global-Attributed-Graph-GAG"><a href="#Global-Attributed-Graph-GAG" class="headerlink" title="Global Attributed Graph (GAG)"></a>Global Attributed Graph (GAG)</h3><h4 id="全局属性的会话图"><a href="#全局属性的会话图" class="headerlink" title="全局属性的会话图"></a>全局属性的会话图</h4><p>建图方式和SRGNN一样，建成有向图，不同的是加入了 <strong>全局属性（用户属性）</strong> $u$  变成三元组 $G_s = (u,V_s,E_s)$ ，图的边定义为：$E_s=(w_{s,(n-1)n},v_{n-1},v_n)$ ，也是三元组， $w$ 是权重，和SRGNN计算方式一样，基于该边出现的频率。</p>
<h4 id="全局属性的图神经网络"><a href="#全局属性的图神经网络" class="headerlink" title="全局属性的图神经网络"></a>全局属性的图神经网络</h4><p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/lenovo/image-20220312132405693.png" alt="image-20220312132405693"></p>
<p>会话图作为输入进到GAG模型，模型的计算从边、节点到全局属性。</p>
<ol>
<li>逐边更新 per-edge update</li>
</ol>
<p>边特征在这里指的是边的权值，是固定值，不是dense vector，不会更新。所以边信息只用来更新节点特征和全局特征。因为是有向图，所以对于一条边来说，需要双向更新，一个节点既作为sender，也作为receiver。更新公式：<br>$$<br>\begin{equation}<br>\begin{aligned}<br>e_{k,in}’ &amp;= \phi ^e_{in}(e_k,v_{r_k},v_{s_k},u)<br>\&amp;= w_k \cdot MLP(v_{s_k}||u),<br>\<br>e_{k,out}’ &amp;= \phi ^e_{out}(e_k,v_{r_k},v_{s_k},u)<br>\&amp;= w_k \cdot MLP(v_{s_k}||u),<br>\end{aligned}<br>\end{equation}<br>$$<br>其中两个MLP是不共享权重的，因为含义不同，一个是计算sender方向的特征，一个是计算receiver方向的特征。</p>
<ol start="2">
<li>逐点更新 per-node update</li>
</ol>
<p>逐点更新是基于逐边更新的结果的。逐点更新的结果是包含所有入or出的邻居信息的标准化后的加和。以第一个公式为例， $s_j$ 是所有指向 $r_i$（也即 $i$ ） 的邻居。节点 $i$ 的<strong>节点</strong>的 in-coming feature 是所有指向 $i$ 的<strong>边的</strong>标准化后的 out-going feature 的加和。 标准化是将 $j$ 指向 $i$ 的这条边的 out-going feature 除以 $\sqrt{i的入度 \cdot j的出度}$ ，可以看出， $j$ 的出度越大（从 $j$ 发出的边越多），$i$ 的入度越大（指向 $i$ 的边越多），都会导致 $i$ 来自节点 $j$ 的 in-coming feature 值越小。这是符合直觉的。<br>$$<br>\begin{equation}<br>\begin{aligned}<br>v_{i,in}’ =\sum_{j \in { v_{s_j} = v_{r_i}}} \frac{e_{j,out’}}{\sqrt{N_{in}(i)N_{out}(j)}}<br>\<br>v_{i,out}’ =\sum_{j \in { v_{r_j} = v_{s_i}}} \frac{e_{j,in’}}{\sqrt{N_{out}(i)N_{in}(j)}} </p>
<p>\end{aligned}<br>\end{equation}<br>$$</p>
<ol start="3">
<li>节点的最终表示</li>
</ol>
<p>最后节点 $i$ 表示融合了自己的 in-coming feature 和 out-going feature，节点最后的表示实际上包含了 1）自身的节点信息；2）邻居信息（通过边传播）；3）连接的边的权重；4）全局属性：<br>$$<br>v_i’= MLP (v_{i,in}’ || v_{i,out}’)<br>$$</p>
<ol start="4">
<li>会话表示</li>
</ol>
<p>与NARM、STAMP、SRGNN工作类似，也用序列中的最后一个节点对其它节点做 self-attention 。有的工作用内积计算最后一个节点与其他节点的相似度作为权重，如NARM；也有用MLP获得权重的，如STAMP、SRGNN。这里用MLP。<br>$$<br>\begin{equation}<br>\begin{aligned}<br>\<br>u’ &amp;= \phi ^ u (V’,u)<br>\<br>&amp;= Self-Atten(v_l’,v_i’,u) + u</p>
<p>\end{aligned}<br>\end{equation}<br>$$<br>Self-Atten 分为以下两个部分：<br>$$<br>\begin{equation}<br>\begin{aligned}</p>
<p>\alpha <em>i &amp;= MLP(v_l’||v_i’||u)<br>\<br>u</em>{s,g} &amp;= \sum ^n _{i=1} \alpha_i v_i’<br>\end{aligned}<br>\end{equation}<br>$$</p>
<p>这样得到的 $u_{s,g}$ 可以看作short-term兴趣增强的会话表示，再融合全局属性 $u$ ，得到长短期兴趣结合的会话表示： $u’ = u_{s,g} + u$ 。这种residual connection残差连接的方式，还可以减轻直接学习全局属性 $u$ 的负担。</p>
<h3 id="推荐-预测"><a href="#推荐-预测" class="headerlink" title="推荐/预测"></a>推荐/预测</h3><p>Session embedding 和 item embedding 做内积，再经过softmax得到概率分布： $\hat y = Softmax(u’^T X)$ 。</p>
<h3 id="Wasserstein-Reservoir"><a href="#Wasserstein-Reservoir" class="headerlink" title="Wasserstein Reservoir"></a>Wasserstein Reservoir</h3><p>将离线模型拓展到流设定下，提出Wasserstein reservoir方法。提出该方法的目标是：用新来的数据更新模型，并且保持从历史交互中学到的知识。</p>
<p>传统的在线学习方法通常只用新数据更新模型，所以导致模型会忘记过去的知识。为了避免这一点，本文利用reservoir来保持对历史数据的长期记忆，reservoir技术在流数据库管理系统中非常常见。</p>
<p>如何选择reservoir中的数据？之前的方法是：使用随机采样方法。每个新数据都以 $\frac{|C|}{t}$ 的概率随机替换掉已经在 $C$ 中的数据。这种方法被证明是从当前数据集中随机采样，并且可以保持模型的long-term memory。</p>
<p>作者认为，使用以上的随机采样方法得到 $C$ ，把它当作训练数据来训练模型的方式不好，原因如下：随机采样难以更关心新数据（time descent probability），但是最近的数据又是非常重要的。所以应该用新来的数据和reservoir $C$ 中的老数据一起更新预训练的模型，而不光光是reservoir $C$ 中的老数据。</p>
<p>但是，即便用新老数据一起更新模型，由于随机采样策略没变，训练数据中的大部分数据都是long-term数据，模型早就学得很好了，所以用它来训练对模型更新帮助不大。</p>
<p>如果当前模型在最新会话上预测结果不好，可能意味着用户兴趣转移or当前模型无法捕捉一些转换模式。这样的数据称之为”有信息量的数据“，对模型更新意义更大。</p>
<p>在本文中，一个会话的信息量被定义为模型预测的分布 $\hat y$ 和真实交互 $y$ 的距离。下面是三种计算距离的算法：</p>
<ol>
<li>Wasserstein 距离（EMD 距离）：</li>
</ol>
<p>$$<br>d_{W}\left(\mathbb{P}<em>{r}, \mathbb{P}</em>{g}\right)=\inf <em>{\gamma \in \Pi\left(\mathbb{P}</em>{r}, \mathbb{P}<em>{g}\right)} \mathbb{E}</em>{(x, y) \sim \gamma}[|x-y|]<br>$$</p>
<ol start="2">
<li>Kullback-Leibler（KL）散度：</li>
</ol>
<p>$$<br>d_{K L}\left(\mathbb{P}<em>{r} | \mathbb{P}</em>{g}\right)=\sum_{i=1}^{n} P_{r}(x) \log \frac{P_{r}(x)}{P_{g}(x)}<br>$$</p>
<ol start="3">
<li>Total Variation（全变分）距离：</li>
</ol>
<p>$$<br>d_{T V}\left(\mathbb{P}<em>{r}, \mathbb{P}</em>{g}\right)=\sup <em>{A \in \Sigma}\left|\mathbb{P}</em>{r}(A)-\mathbb{P}_{g}(A)\right|<br>$$</p>
<p>在推荐任务中，真实分布是one-hot向量，只有真实标签处为1。</p>
<p>KL散度，也称交叉熵、相对熵。不选择KL散度的原因是：在推荐任务中，公式简化为：$d_{K L}(\mathbf{y} | \hat{\mathbf{y}})=-\log P_{g}\left(v_{i}\right)$ ，实际上只衡量了真实标签处的差异，没有考虑整个分布之间的差异。而且KL散度本身就是非对称性函数：$D(p | q) \neq D(q | p)$ ，用它作为一个真正的距离度量可能不是很合适。</p>
<p>不选择全变分距离的原因是：在推荐任务中，公式简化为： $d_{T V}(\mathbf{y}, \hat{\mathbf{y}})=\max <em>{j \neq i}\left(1-P</em>{g}\left(v_{i}\right), P_{g}\left(v_{j}\right)\right)$  ，这个结果要么只衡量了真实标签以外的差异，要么只衡量了真实标签。</p>
<h4 id="在线训练算法描述"><a href="#在线训练算法描述" class="headerlink" title="在线训练算法描述"></a>在线训练算法描述</h4><p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220313210103872.png" alt="image-20220313210103872"></p>
<p>$S$ 是用来更新模型的，与 $C$ 无关。</p>
<p>因为流数据中会有新用户和新物品出现，为了防止模型忽略这些新的会话，它们会被直接加入 $S$ 当作训练数据。</p>
<p>$C \cup C^{n e w} - S$ 即其余的会话数据，分别计算它们的Wasserstein距离，再根据以下公式计算各自的采样概率：<br>$$<br>p_{\text {sample }}\left(s_{i}\right)=\frac{d_{i}}{\sum_{s_{j} \in C \cup C^{n e w}-S} d_{j}},<br>$$<br>采样完以后就得到训练数据 $S$ ，这个 $S$ 是对当前模型来说信息量最大的数据集，用它来更新模型最有效。</p>
<p>最后还要更新 reservoir $C$ ，用随机采样算法来更新，以保持模型的 long-term 记忆。</p>
<h3 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h3><p>$ Cross \ Entropy \ Loss=-\sum_{i=1}^{l} \mathrm{y}<em>{i} \log \left(\hat{\mathrm{y}}</em>{i}\right) $</p>
<h2 id="Experiment-amp-Table"><a href="#Experiment-amp-Table" class="headerlink" title="Experiment &amp; Table"></a>Experiment &amp; Table</h2><h3 id="对比实验结果"><a href="#对比实验结果" class="headerlink" title="对比实验结果"></a>对比实验结果</h3><p>S-POP居然比GRU4Rec结果好，可能因为S-POP能抽取出会话间的信息。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220314084551972.png" alt="image-20220314084551972"></p>
<h3 id="细化评价指标的top-k"><a href="#细化评价指标的top-k" class="headerlink" title="细化评价指标的top k"></a>细化评价指标的top k</h3><p>SSRM方法相比于另外三个基于图的方法，效果下降得幅度更大，说明图结构更适合做会话表示任务，也说明图结构有一定的泛化能力。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220314085304068.png" alt="image-20220314085304068"></p>
<h3 id="全局属性的影响"><a href="#全局属性的影响" class="headerlink" title="全局属性的影响"></a>全局属性的影响</h3><p>三个消融实验的对比模型如下：</p>
<ul>
<li><p>FGNN：节点更新层和输出层都不加入用户信息。</p>
</li>
<li><p>GAG-FGNN：将节点更新函数换成FGNN的节点更新层，但是保留全局属性更新  $u’ = u_{s,g} + u$ 。</p>
</li>
<li><p>GAG-NoGA：节点更新层不变，但是去掉全局属性更新函数 $u’ = u_{s,g} + u$ 。</p>
</li>
</ul>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220314091613738.png" alt="image-20220314091613738"></p>
<p>结果如下。GAG-FGNN和GAG-NoGA都在模型中融入了全局信息，前者在用户信息更新部分，后者在GNN的节点更新部分。相比于没有全局信息的FGNN，两者都有提升，说明融入全局信息对推荐是有用的。GAG-NoGA比GAG-FGNN提升更大，说明在节点更新阶段融入全局信息更有效。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220314092750543.png" alt="image-20220314092750543"></p>
<h3 id="Wasserstein-Reservoir的影响"><a href="#Wasserstein-Reservoir的影响" class="headerlink" title="Wasserstein Reservoir的影响"></a>Wasserstein Reservoir的影响</h3><p>消融实验的对比模型如下：</p>
<ul>
<li>GAG-Static：直接去掉在线训练部分</li>
<li>GAG-RanUni：从原reservoir和新数据的并集里，随机采样。这也是最普遍的设计。</li>
<li>GAG-FixNew：直接保留新数据，剩下的数据随机采样。</li>
<li>GAG-WassUni：对所有原reservoir和新数据计算Wasserstein距离，然后根据这个距离采样。（原模型是先全部保存所有带有新用户和新物品的会话，再根据Wass距离采样）</li>
</ul>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/lenovo/image-20220314100934020.png" alt="image-20220314100934020"></p>
<p>结果如下：</p>
<p>Static结果最差，因为：1）兴趣漂移；2）新用户、新物品出现。</p>
<p>纯随机采样的GAG-RanUni，在在线模型中结果最差，不如有策略地选择。</p>
<p>GAG-WassUni优于大部分策略，说明采用Wass距离的有效性。</p>
<p>GAG和GAG-WassUni相比，GAG结果更好，说明保留新用户和新物品的重要性。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/lenovo/image-20220314100603124.png" alt="image-20220314100603124"></p>
<h3 id="Reservoir-效率分析"><a href="#Reservoir-效率分析" class="headerlink" title="Reservoir 效率分析"></a>Reservoir 效率分析</h3><p>有两个参数reservoir的设计影响很大：reservoir大小（ $C$ ）和窗口大小（ $S$ ）。一方面，reservoir的大小表示reservoir的容量，这决定了推荐系统在线更新的存储要求。另一方面，窗口大小限制了多少数据实例将被抽样用于在线训练，这代表了推荐系统在线更新的工作负荷。</p>
<h4 id="Reservoir-size-的影响"><a href="#Reservoir-size-的影响" class="headerlink" title="Reservoir size 的影响"></a>Reservoir size 的影响</h4><p>Reservoir容量越大，新数据保存的概率就越低，模型就更注重历史数据。但是在流设定下，新数据更能代表用户最近的兴趣。SOTA模型SSRM在$\frac{|D|}{20}$ 时表现最好，而GAG是 $\frac{|D|}{100}$ ，所以GAG效率更高。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/lenovo/image-20220314103059323.png" alt="image-20220314103059323"></p>
<h4 id="Window-size-的影响"><a href="#Window-size-的影响" class="headerlink" title="Window size 的影响"></a>Window size 的影响</h4><p>很明显，窗口大小越大，模型表现越好。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/lenovo/image-20220314104151741.png" alt="image-20220314104151741"></p>
<h3 id="超参数的影响"><a href="#超参数的影响" class="headerlink" title="超参数的影响"></a>超参数的影响</h3><h4 id="Embedding-size影响"><a href="#Embedding-size影响" class="headerlink" title="Embedding size影响"></a>Embedding size影响</h4><p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/lenovo/image-20220314104403575.png" alt="image-20220314104403575"></p>
<h4 id="GNN-layer层数影响"><a href="#GNN-layer层数影响" class="headerlink" title="GNN layer层数影响"></a>GNN layer层数影响</h4><p>一般来说，由于梯度爆炸，GNN模型总是受到模型深度增加的影响。在我们的实验中，GAG模型的性能随着GNN 层数增加而下降，这与常见的观察是一致的。此外，会话的连通性比传统的图数据要小，这也限制了更深的GNN模型的能力。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/lenovo/image-20220314104558903.png" alt="image-20220314104558903"></p>
]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>流会话推荐</tag>
        <tag>GAG</tag>
      </tags>
  </entry>
  <entry>
    <title>论文笔记：《Graph-Enhanced Multi-Task Learning of Multi-Level Transition Dynamics for Session-based Recommendation》</title>
    <url>/2021/10/20/MTD/</url>
    <content><![CDATA[<hr>
<p>原paper：<a href="https://ojs.aaai.org/index.php/AAAI/article/view/16534">https://ojs.aaai.org/index.php/AAAI/article/view/16534</a></p>
<p>开源代码：<a href="https://github.com/sessionRec/MTD">https://github.com/sessionRec/MTD</a></p>
<hr>
<h3 id="动机："><a href="#动机：" class="headerlink" title="动机："></a>动机：</h3><p>大多数现有的基于会话的推荐技术并没有很好地设计来捕捉复杂的转换动态(complex transition dynamics)，这些动态表现为时间有序和多层次相互依赖的关系结构。</p>
<p>complex transition dynamics 的”complex”体现在：multi-level relation(intra- and inter-session item relation) . 会话内：short-term and long-term item transition，会话间：long-range cross-session dependencies。复杂依赖的例子见Figure 1.</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/MTD-1.png" alt="MTD-1"></p>
<span id="more"></span>

<h3 id="主要贡献："><a href="#主要贡献：" class="headerlink" title="主要贡献："></a>主要贡献：</h3><p>1.开发了一种新的会话推荐框架，可以捕获会话内和会话间的物品转换模式（多层次转换动态）</p>
<p>2.开发了一种位置感知的注意力机，用于学习会话内的序列行为和session-specific knowledge。此外，在图神经网络范例的基础上，建立了全局上下文增强的会话间关系编码器，赋予MTD来捕获会话间项目依赖关系。</p>
<p>3.在三个数据集上取得了SOTA，Yoochoose、Diginetica、Retailrocket。</p>
<h3 id="网络图："><a href="#网络图：" class="headerlink" title="网络图："></a>网络图：</h3><p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/MTD-2.png" alt="MTD-2"></p>
<h3 id="方法论："><a href="#方法论：" class="headerlink" title="方法论："></a>方法论：</h3><h4 id="1-学习会话内的物品关系"><a href="#1-学习会话内的物品关系" class="headerlink" title="1.学习会话内的物品关系"></a>1.学习会话内的物品关系</h4><p>1）Self-attention层：</p>
<p>QKV self-attention + feed forward network</p>
<p>2）位置感知的物品级的注意力聚合模块：<br>$$<br>\alpha _ i = \delta(\mathbf{g}^T \cdot \sigma(\mathbf{W} _ 3\cdot\mathbf{x} _ {s,I} + \mathbf{W} _ 4\cdot\mathbf{x} _ {s,i}))<br>$$</p>
<p>$$<br>\mathbf{x}^* _ s = \sum_{i=1}^I\alpha _ i\cdot\mathbf{x _ \mathit{s,i}}<br>$$</p>
<p>$$<br>\mathbf{q} _ s = \mathbf{W} _ c[\mathbf{x} _ {s,I},\mathbf{x}^* _ s,\mathbf{p} _ s]<br>$$</p>
<p>$\mathbf{x}_{s,I}$表示最后一次点击，$\mathbf{x^*_s}$表示聚合后的会话表示，$\mathbf{p}_s$表示加入物品相对位置信息的会话表示，$\mathbf{q}_s $是最后的会话表示。</p>
<p>3）loss = $\mathit{L_in}$</p>
<h4 id="2-对全局转换动态建模"><a href="#2-对全局转换动态建模" class="headerlink" title="2.对全局转换动态建模"></a>2.对全局转换动态建模</h4><p>1）用图神经网络结构和GCN对inter-session的依赖建模</p>
<p>2）用<strong>互信息学习</strong>来增强跨会话的建模物品间关系的encoder</p>
<p>3）loss = $\mathit{L_cr}$</p>
<h4 id="3-Model-Inference"><a href="#3-Model-Inference" class="headerlink" title="3.Model Inference"></a>3.Model Inference</h4><p>定义loss：<br>$$<br>\mathit{L}=\mathit{L} _ {cr} + \lambda_1\mathit{L} _ {in}+\lambda_2||\Theta||^2 _ 2<br>$$</p>
]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>GNN</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch Geometric 学习笔记</title>
    <url>/2021/11/14/Pytorch%20Geometric/</url>
    <content><![CDATA[<hr>
<p>官网永远是最好的学习资料：<a href="https://pytorch-geometric.readthedocs.io/en/latest/">https://pytorch-geometric.readthedocs.io/en/latest/</a></p>
<p>跟着配套colaboratory的教程走，大概一天能学完五个教程，学完也算基本入门pytroch-geometric了。</p>
<hr>
<p><a href="https://colab.research.google.com/drive/1h3-vJGRVloF5zStxL5I0rSy4ZUPNsjy8?usp=sharing#scrollTo=gUFSrDPxuQ23">1. Introduction.ipynb - Colaboratory (google.com)</a></p>
<ul>
<li>This concludes the first introduction into the world of GNNs and PyTorch Geometric. In the follow-up sessions, you will learn how to achieve state-of-the-art classification results on a number of real-world graph datasets.</li>
<li>概要：介绍图的基本结构，GCN怎么用。</li>
</ul>
<p><a href="https://colab.research.google.com/drive/14OvFnAXggxB8vM4e8vSURUp1TaKnovzX">2. Node Classification.ipynb - Colaboratory (google.com)</a></p>
<ul>
<li>In this chapter, you have seen how to apply GNNs to real-world problems, and, in particular, how they can effectively be used for boosting a model’s performance. In the next section, we will look into how GNNs can be used for the task of graph classification.</li>
<li>概要：用GNN实现某些真实的节点分类任务，与MLP效果更好。</li>
</ul>
<span id="more"></span>

<p><a href="https://colab.research.google.com/drive/1I8a0DfQ3fI7Njc62__mVXUlcAleUclnb?usp=sharing#scrollTo=CN3sRVuaQ88l">3. Graph Classification.ipynb - Colaboratory (google.com)</a></p>
<ul>
<li><p>In this chapter, you have learned how to apply GNNs to the task of graph classification. You have learned how graphs can be batched together for better GPU utilization, and how to apply readout layers for obtaining graph embeddings rather than node embeddings.</p>
<p>In the next session, you will learn how you can utilize PyTorch Geometric to let Graph Neural Networks scale to single large graphs.</p>
</li>
<li><p>概要：学习了应用GNN实现图分类。学习了GNN上的mini-batch是如何构造以更好利用GPU。学习了如何用readout层获取图的表示。</p>
<ul>
<li>和图像一样用padding和rescaling让图大小相同太浪费空间，所以用对角矩阵相连的方法处理。在torch里是用稀疏矩阵存储的，所以开销不大。</li>
</ul>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211118204147577.png" alt="image-20211118204147577"></p>
<ul>
<li>Dataloader和torch里差不多 <code>train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)</code>。</li>
<li><code>DataBatch(edge_index=[2, 2636], x=[1188, 7], edge_attr=[2636, 4], y=[64], batch=[1188], ptr=[65])</code> 里的batch记录的是每个节点在哪个图里，batch = [0,…,0,1,…,1,2,…,2]表示一个batch里有三张图</li>
<li>nn.GraphConv() 有residual connection。</li>
<li>图的表示可以写成所有节点的均值<code>x = global_mean_pool(x, batch)</code> </li>
</ul>
</li>
</ul>
<p><a href="https://colab.research.google.com/drive/1XAjcjRHrSR_ypCk_feIWFbcBKyT4Lirs#scrollTo=SDOmdUe0C3U1">4. Scaling GNNs.ipynb - Colaboratory (google.com)</a></p>
<ul>
<li><p>In this chapter, you have been presented a method to scale GNNs to large graphs, which otherwise would not fit into GPU memory.</p>
<p>This also concludes the hands-on tutorial on <strong>deep graph learning with PyTorch Geometric</strong>. If you want to learn more about GNNs or PyTorch Geometric, feel free to check out <strong><a href="https://pytorch-geometric.readthedocs.io/en/latest/?badge=latest">PyG’s documentation</a></strong>, <strong><a href="https://github.com/rusty1s/pytorch_geometric">its list of implemented methods</a></strong> as well as <strong><a href="https://github.com/rusty1s/pytorch_geometric/tree/master/examples">its provided examples</a></strong>, which cover additional topics such as <strong>link prediction</strong>, <strong>graph attention</strong>, <strong>mesh or point cloud convolutions</strong> and <strong>other methods for scaling up GNNs</strong>.</p>
<p><em>Happy hacking!</em></p>
</li>
<li><p>概要：介绍了了降低显存的方法，cluster-gnn，使得训练超大图成为可能。</p>
</li>
<li><p>不再在整个图上划分mini-batch，先分成sub-graph再分mini-batch，解决了邻居爆炸（。。邻居数量）问题</p>
</li>
<li><p>分太开也不好，所以cluster以后随机对sub-graph再连接</p>
<ul>
<li><code>ClusterData</code> converts a <code>Data</code> object into a dataset of subgraphs containing <code>num_parts</code> partitions.</li>
<li>Given a user-defined <code>batch_size</code>, <code>ClusterLoader</code> implements the stochastic partitioning scheme in order to create mini-batches.</li>
</ul>
</li>
<li><p>这种采样方法，只用改划分数据的代码，训练过程不变。</p>
</li>
</ul>
<p><a href="https://colab.research.google.com/drive/1D45E5bUK3gQ40YpZo65ozs7hg5l-eo_U?usp=sharing#scrollTo=iWRxB3JYFXNF">5. Point Cloud Classification.ipynb - Colaboratory (google.com)</a></p>
<ul>
<li>概要：介绍了点云分类任务的三大步骤。又在PointNet++和PPFNet的实践中，介绍了如何自定义MessagePassing以及采样策略。</li>
<li>PointNet++<ul>
<li><p>Grouping阶段，用knn graph或者半径图</p>
<ul>
<li><p>```python<br>from torch_cluster import knn_graph<br>根据点的坐标计算最近的k个点，连起来</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">- 邻居聚合阶段。聚合邻居信息</span><br><span class="line"></span><br><span class="line">  - 从</span><br><span class="line"></span><br><span class="line">    ```python</span><br><span class="line">    class PointNetLayer(torch_geometric.nn.MessagePassing)：</span><br><span class="line">    	def __init__(self, in_channels, out_channels):</span><br><span class="line">            pass</span><br><span class="line">        def forward(self, h, pos, edge_index):</span><br><span class="line">            pass</span><br><span class="line">        def message(self, h_j, pos_j, pos_i):</span><br><span class="line">            pass</span><br></pre></td></tr></table></figure>

<p>继承，并定义出一个与<code>GraphConv()</code> 、<code>GCNConv()</code> 同一级别的类，例如一种新的卷积层。</p>
</li>
<li><p>MessagePassing接口通过自动处理消息传播，来帮助我们创建<strong>消息传递图神经网络</strong>。只需要定义 message 功能即可。</p>
</li>
<li><p><code>def message()</code>  定义如何构建一个可学习的message给每条边（每个边对应一个邻居，所以也可以看成定义message给每个邻居），以及传播的规则</p>
</li>
<li><p><code>def forward()</code>  调用propagate()，开始传播</p>
</li>
<li><p>PPFNet，解决旋转不变性</p>
</li>
</ul>
</li>
<li><p>downsampling（下采样）阶段</p>
<ul>
<li><p><strong>Farthest Point Sampling</strong> (FPS) 最远点采样。使得每次采点都和已经采样的点距离最远。这种方式证明比随机采样更能覆盖整个点集。</p>
</li>
<li><p>不同batch中fps是独立的，所以要传入batch向量</p>
<ul>
<li><pre><code class="python">index = fps(pos, batch, ratio=0.5)
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[6. GNN Explanation.ipynb - Colaboratory (google.com)](https://colab.research.google.com/drive/1fLJbFPz0yMCQg81DdCP5I8jXw9LoggKO?usp=sharing#scrollTo=F1op-CbyLuN4)</span><br><span class="line"></span><br><span class="line">- 占坑</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 安装</span><br><span class="line"></span><br><span class="line">建个新环境</span><br><span class="line"></span><br></pre></td></tr></table></figure>
conda create -n pyg python==3.8.0
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">安装torch</span><br><span class="line"></span><br></pre></td></tr></table></figure>
pip install torch==1.10.0
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">安装依赖包</span><br><span class="line"></span><br><span class="line">## 方法1：</span><br><span class="line"></span><br><span class="line">![image-20211115163907936](C:\Users\pc\OneDrive\Typora文档\images\image-20211115163907936.png)</span><br><span class="line"></span><br><span class="line">官网安装比较省事，但是可以看到只有最新的几个版本，如果你的pytorch版本比较旧（旧也是为了稳定...）可以尝试方法2。</span><br><span class="line"></span><br><span class="line">## 方法2：</span><br><span class="line"></span><br><span class="line">https://data.pyg.org/whl/</span><br><span class="line"></span><br><span class="line">根据pytorch版本和cuda版本，在这个网站选择对应版本进入，例如我是torch-1.10和cuda-10.2，所以进入https://data.pyg.org/whl/torch-1.10.0+cu102.html</span><br><span class="line"></span><br><span class="line">然后根据系统类型和python版本，下好安装包，如下</span><br><span class="line"></span><br><span class="line">torch_**scatter**-2.0.9-cp38-cp38-linux_x86_64.whl</span><br><span class="line"></span><br><span class="line">torch_**sparse**-0.6.12-cp38-cp38-linux_x86_64.whl</span><br><span class="line"></span><br><span class="line">torch_**cluster**-1.5.9-cp38-cp38-linux_x86_64.whl</span><br><span class="line"></span><br><span class="line">torch_spline_conv-1.2.1-cp38-cp38-linux_x86_64.whl</span><br><span class="line"></span><br><span class="line">最后用pip离线离线安装</span><br><span class="line"></span><br></pre></td></tr></table></figure>
pip install xxx.whl
</code></pre>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>注意要切换到安装目录，且按顺序安装 scatter–&gt;sparse–&gt;cluster–&gt;spline</p>
]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>Recbole避坑手册</title>
    <url>/2021/12/05/Recbole%E9%81%BF%E5%9D%91%E6%89%8B%E5%86%8C/</url>
    <content><![CDATA[<p>RecBole是个非常好的开源库，这几天做评测的时候用上了，奈何本人能力有限，遇到了非常多bug（可能是自己行为造成的），简单记录一下。可以参考这个：<a href="https://blog.csdn.net/turinger_2000/category_10624007.html">RecBole小白入门系列_Turinger_2000的博客-CSDN博客</a></p>
<p><strong>使用方法</strong>就是：<a href="https://github.com/RUCAIBox/RecBole">RUCAIBox/RecBole (github.com)</a>，下载下来unzip或者clone到设备上。然后再RecBole主目录下编写一个test.yaml文件记录一些配置，再运行run_recbole.py就可以。test.yaml大概要设置4类东西：dateset setting, model setting, train setting, evaluate setting.</p>
<p>整个项目文件如下，几个比较重要的文件夹和文件标出来了，后面会说到。</p>
<span id="more"></span>

<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/lenovo/image-20211205143321872.png" alt="image-20211205143321872"></p>
<p>接下来以一个<strong>用自己数据集跑SASRec模型</strong>的例子说明如何使用RecBole。</p>
<h2 id="配置test-yaml文件"><a href="#配置test-yaml文件" class="headerlink" title="配置test.yaml文件"></a>配置test.yaml文件</h2><h3 id="构造数据集——data-setting"><a href="#构造数据集——data-setting" class="headerlink" title="构造数据集——data setting"></a>构造数据集——data setting</h3><p>如果想跑自己的实验，那么很重要的一件事就是构造自己的数据集，recbole要求个人首先构建可以处理的原子文件，然后就可以传给模型处理了。详细见：<a href="https://recbole.io/cn/data_flow.html">数据流 | 伯乐 (recbole.io)</a></p>
<p>根据<a href="https://recbole.io/cn/atomic_files.html">原子文件 | 伯乐 (recbole.io)</a>，Sequential模型只需要.inter的原子文件，如下图：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/lenovo/image-20211205143831872.png" alt="image-20211205143831872"></p>
<p>虽然不知道.inter 是什么文件，但是可以看以下模型本身给的数据集以及处理好的原子文件模仿着构造。数据集保存在RecBole/dataset/ml-100k下（以ml-100k数据集为例），找到ml-100k.inter，用记事本打开格式如下：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/lenovo/image-20211205144208594.png" alt="image-20211205144208594"></p>
<p>BPR和CF等general model可能会用到rating（用户评分），timestamp。但是Sequential model一般只需要timestamp把点击行为构成对应用户的序列就行，想跑的SASRec论文附的代码里，对数据集的处理是：只保留user_id和item_id两列，按点击顺序存储。我的数据集book长这个样子：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/lenovo/image-20211205144708967.png" alt="image-20211205144708967"></p>
<p>只有两列特征user_id和item_id，看起来好像很接近，但是踩坑穿越回来的我可以告诉你，这里必须得有timestamp一列，recbole就是这么处理sequential model的数据集的，没有办法。那添加什么样的timestamp呢？book数据集是按点击顺序存储的（user_id已经重新从0开始标号），所以其实只要加个递增的timestamp就行了，这里用pandas简单处理下多加一列就行。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/lenovo/image-20211205145228872.png" alt="image-20211205145228872"></p>
<p>注意，这里用pandas处理的时候，顺便把列名改了。user_id和item_id后面加上”:token”，timestamp后面加上”:float”。RecBole要求这么做，后面也会说到。这样处理好文件后，pandas输出的一般是csv，重命名的时候要改成.inter后缀。然后在dataset下新建一个文件夹，起名为你的dataset名称xxx（可以自己起，这个很重要），然后.inter文件也要命名为xxx.inter，如图所示：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/lenovo/image-20211205145659369.png" alt="image-20211205145659369"></p>
<p>然后我们在RecBole主目录下新建一个test.yaml文件，在里面输入：（暂时不明白没事，抄下来就行）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># dataset config</span></span><br><span class="line">field_separator: <span class="string">&quot;,&quot;</span>  <span class="comment">#指定数据集field的分隔符</span></span><br><span class="line">seq_separator: <span class="string">&quot; &quot;</span>   <span class="comment">#指定数据集中token_seq或者float_seq域里的分隔符</span></span><br><span class="line">USER_ID_FIELD: user_id <span class="comment">#指定用户id域</span></span><br><span class="line">ITEM_ID_FIELD: item_id <span class="comment">#指定物品id域</span></span><br><span class="line">TIME_FIELD: timestamp  <span class="comment">#指定时间域</span></span><br><span class="line">NEG_PREFIX: neg_   <span class="comment">#指定负采样前缀</span></span><br><span class="line"><span class="comment">#指定从什么文件里读什么列，这里就是从book.inter里面读取user_id, item_id,timestamp这四列</span></span><br><span class="line">load_col:</span><br><span class="line">  inter: [user_id, item_id, timestamp]</span><br></pre></td></tr></table></figure>

<p>需要注意前两条separator，csv文件的话默认分隔符是”,”，还有最后一行local:这里按照数据集的列指定就行，到此数据集基本构造好了。</p>
<h3 id="用Sequential-model类跑模型——model-setting"><a href="#用Sequential-model类跑模型——model-setting" class="headerlink" title="用Sequential model类跑模型——model setting"></a>用Sequential model类跑模型——model setting</h3><p>以SASRec为例，想跑一个模型，如何看这个模型需要的参数？到 RecBole/recbole/properties/model 底下找到对应模型的yaml文件，打开以后大概长这样。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/lenovo/image-20211205150832821.png" alt="image-20211205150832821"></p>
<p>这里包含了模型需要的参数，每次调用都到这里改很麻烦，所以recbole可以实现用test.yaml的设置覆盖具体模型的设置，所以只要在test.yaml（主目录下的那个配置文件）里改我们添加，并做一点修改：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">n_layers: <span class="number">2</span></span><br><span class="line">n_heads: <span class="number">2</span></span><br><span class="line">hidden_size: <span class="number">100</span></span><br><span class="line">inner_size: <span class="number">256</span></span><br><span class="line">hidden_dropout_prob: <span class="number">0.5</span></span><br><span class="line">attn_dropout_prob: <span class="number">0.5</span></span><br><span class="line">hidden_act: <span class="string">&#x27;gelu&#x27;</span></span><br><span class="line">layer_norm_eps: <span class="number">1e-12</span></span><br><span class="line">initializer_range: <span class="number">0.02</span></span><br><span class="line">loss_type: <span class="string">&#x27;CE&#x27;</span></span><br></pre></td></tr></table></figure>



<h3 id="训练设置——train-setting"><a href="#训练设置——train-setting" class="headerlink" title="训练设置——train setting"></a>训练设置——train setting</h3><p>通用的训练设置也要写到test.yaml中：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># training settings</span><br><span class="line">epochs: 500  				#训练的最大轮数</span><br><span class="line">train_batch_size: 20 #2048 	#训练的batch_size</span><br><span class="line">learner: adam 				#使用的pytorch内置优化器</span><br><span class="line">learning_rate: 0.001 		#学习率</span><br><span class="line">training_neg_sample_num: 0 	#负采样数目</span><br></pre></td></tr></table></figure>



<h3 id="评估设置——evaluate-setting"><a href="#评估设置——evaluate-setting" class="headerlink" title="评估设置——evaluate setting"></a>评估设置——evaluate setting</h3><p>通用的评估设置也要写道test.yaml中：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># evalution settings</span><br><span class="line">eval_step: 1 				#每次训练后做evalaution的次数</span><br><span class="line">eval_setting: RO_RS,full 	#对数据随机重排，设置按比例划分数据集，且使用全排序</span><br><span class="line">group_by_user: True 		#是否将一个user的记录划到一个组里，当eval_setting使用RO_RS的时候该项必须是True</span><br><span class="line">split_ratio: [0.8,0.1,0.1] 	#切分比例</span><br><span class="line">metrics: [&quot;Recall&quot;, &quot;MRR&quot;,&quot;NDCG&quot;,&quot;Hit&quot;,&quot;Precision&quot;] #评测标准</span><br><span class="line">topk: [10] #评测标准使用topk，设置成10评测标准就是[&quot;Recall@10&quot;, &quot;MRR@10&quot;, &quot;NDCG@10&quot;, &quot;Hit@10&quot;, &quot;Precision@10&quot;]</span><br><span class="line">valid_metric: MRR@10 		#选取哪个评测标准作为作为提前停止训练的标准</span><br><span class="line">stopping_step: 10 			#控制训练收敛的步骤数，在该步骤数内若选取的评测标准没有什么变化，就可以提前停止了</span><br><span class="line">eval_batch_size: 4096 		#评测的batch_size</span><br></pre></td></tr></table></figure>

<p>recbole实现了earlystopping早停策略，可以设置控制收敛的步骤数。</p>
<p>eval_setting，可以设置不同的数据切分方式，具体可见<a href="https://recbole.io/cn/evaluation.html">评测 | 伯乐 (recbole.io)</a>，大致如下：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/lenovo/image-20211205163312199.png" alt="image-20211205163312199"></p>
<h2 id="run！跑起来吧，baseline！"><a href="#run！跑起来吧，baseline！" class="headerlink" title="run！跑起来吧，baseline！"></a>run！跑起来吧，baseline！</h2><p>我们cd到RecBole目录下，此时数据集已经准备好，test.yaml文件也已经写好，可以开始跑实验了，用以下指令：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">python run_recbole.py --model=SASRec --dataset=book ----config_files=test.yaml</span><br></pre></td></tr></table></figure>

<p>如果看不懂参数的话，点进run_recbole.py看一下就明白了！</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>RecBole是个集成度很高，也比较方便用于复现一些基本推荐模型的开源库（在此致敬中国人民大学AI BOX小组！），需要用recbole跑自己的baseline时，只需要三步：</p>
<ol>
<li>构造满足原子文件的数据集</li>
<li>写好.yaml格式的配置文件</li>
<li>run！</li>
</ol>
<h2 id="记录一些遇到的坑"><a href="#记录一些遇到的坑" class="headerlink" title="记录一些遇到的坑"></a>记录一些遇到的坑</h2><h3 id="1-neg-sampling-不能使用-‘CE’-loss"><a href="#1-neg-sampling-不能使用-‘CE’-loss" class="headerlink" title="1. neg_sampling 不能使用 ‘CE’ loss"></a>1. neg_sampling 不能使用 ‘CE’ loss</h3><p>在配置文件中加上一行：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">neg_sampling: (注意这里冒号后面有个空格)</span><br></pre></td></tr></table></figure>



<h3 id="2-使用不上gpu"><a href="#2-使用不上gpu" class="headerlink" title="2.使用不上gpu"></a>2.使用不上gpu</h3><p><strong>问题描述：</strong>程序可以跑起来，但是nvtop看不到它在gpu上运行。并且无论如何修改配置文件都没有用。非常奇怪的问题。检查torch.cuda.is_available()的时候发现，居然输出False，所以原因是用不了CUDA。</p>
<p><strong>问题原因</strong>：用不了CUDA。</p>
<p>用conda list检查安装的库时发现，默认安装的torch是cpu的</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/lenovo/image-20211205140943532.png" alt="image-20211205140943532"></p>
<blockquote>
<p>左边是默认安装的，想了一宿也没想明白为什么默认安装cpu版本</p>
</blockquote>
<p>卸载cpu版本再安装cuda版本有点麻烦，所以我直接新建了一个环境，并且自带torch=1.10，然后再根据依赖安装，使用命令：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pip install -r requirement.txt</span><br></pre></td></tr></table></figure>

<p>这里不用担心torch会被覆盖，因为torch版本大于1.17就会自动跳过了。</p>
<p>然后总可以跑了吧，运行下面指令（用RecBole需要这样输）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">python run_recbole.py --model=BERT4Rec --dataset=book ----config_files=bert4rec.yaml</span><br></pre></td></tr></table></figure>

<p>按理说此时torch是建环境时我安装的cuda版本，一定不是cpu版本，然而每次运行仍然是device=cpu。实在让人崩溃！</p>
<p><strong>解决方案</strong>：</p>
<p>最后debug多轮，寻找到的解决方案是：</p>
<ul>
<li>在run_recbole.py里import torch，并且打印torch.cuda.is_available()</li>
</ul>
<p>这样做合理的<strong>可能的原因</strong>：</p>
<p>可能项目某个地方import torch，import进来的torch是cpu版本的，所以提前import可以解决。</p>
]]></content>
      <categories>
        <category>代码阅读</category>
      </categories>
      <tags>
        <tag>RecBole</tag>
      </tags>
  </entry>
  <entry>
    <title>论文笔记：《Self-Attentive Sequential Recommendation》</title>
    <url>/2021/11/03/SASRec/</url>
    <content><![CDATA[<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117230533153.png" alt="image-20211117230533153"></p>
<hr>
<p>原paper：<a href="https://ieeexplore.ieee.org/document/8594844">https://ieeexplore.ieee.org/document/8594844</a></p>
<p>源码解读：<a href="https://github.com/Guadzilla/Paper_notebook/tree/main/SASRec">https://github.com/Guadzilla/Paper_notebook/tree/main/SASRec</a></p>
<hr>
<p>中译：自注意序列推荐</p>
<p>总结：比较早使用self-attention的序列推荐模型</p>
<hr>
<span id="more"></span>

<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li>  question作者想解决什么问题？  </li>
</ul>
<p>序列动态是许多当代推荐系统的一个重要特征，它试图根据用户最近执行的操作来捕捉用户活动的“上下文“。RNN模型可以在稠密数据集上捕捉长期语义。（马尔科夫链）MC模型可以在稀疏数据集上仅根据最近几次action做出预测。本文想平衡这两个目标：在稀疏和稠密数据集上做到捕捉长期语义、依赖较少的action做预测。</p>
<ul>
<li>  method作者通过什么理论/模型来解决这个问题？</li>
</ul>
<p>本文提出了一个基于self-attention的序列模型（SASRec），在每个时间步寻找与用户历史最相关的物品作为next item的预测。</p>
<ul>
<li>  answer作者给出的答案是什么？</li>
</ul>
<p>在稀疏和稠密数据集上，与MC/CNN/RNN方法相比都取得了SOTA效果。</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li>  why作者为什么研究这个课题？    </li>
</ul>
<p>MC方法模型简单，但因为它的强假设（当前预测仅取决于最近n次）使得它在稀疏数据上表现好，但是不能捕捉更复杂的动态转换。RNN方法需要稠密数据，并且计算复杂。最近出现新的序列模型Transformer，它是基于self-attention的，效率高并且可以捕获句子中单词的句法和语义模式。受self-attention方法启发，应用到序列推荐上。</p>
<ul>
<li>  how当前研究到了哪一阶段？ </li>
</ul>
<p>第一个将transformer里的self-attention应用到了序列推荐上。</p>
<ul>
<li>  what作者基于什么样的假设（看不懂最后去查）？</li>
</ul>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><ul>
<li>优点 <ul>
<li>  SASRec模型建模了整个序列，自适应地考虑items来预测</li>
<li>  在dense和sparse的数据集上效果都很好</li>
<li>  比CNN/RNN方法快了一个数量级</li>
</ul>
</li>
<li>  缺点</li>
<li>展望<ul>
<li>  引进更多上下文信息，比如等待时间、行为类型、位置、设备等。</li>
<li>  探索处理超长序列（如clicks）的方法</li>
</ul>
</li>
</ul>
<h2 id="Dataset-amp-Metric"><a href="#Dataset-amp-Metric" class="headerlink" title="Dataset &amp; Metric"></a>Dataset &amp; Metric</h2><ul>
<li>数据来源 （都开源）<ul>
<li>  Amazon</li>
<li>  Steam 作者爬的，开源了</li>
<li>  Movielens</li>
</ul>
</li>
<li>重要指标 <ul>
<li>  Hit@10</li>
<li>  NDCG@10</li>
</ul>
</li>
</ul>
<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p>这部分主要参考了知乎[[1]](<a href="https://zhuanlan.zhihu.com/p/277660092?utm_source=qq">推荐算法炼丹笔记：序列化推荐算法SASRec - 知乎 (zhihu.com)</a>)</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211115114043810.png" alt="image-20211115114043810"></p>
<p><strong>1.Embedding层</strong></p>
<p><img src="https://pic4.zhimg.com/80/v2-6127cd6bfcdc00f007ba287f11c1f55f_720w.jpg" alt="img"></p>
<p><strong>A. Positional Embedding</strong></p>
<p><img src="https://pic1.zhimg.com/80/v2-558fcc53330d91271fc2850a3998e704_720w.jpg" alt="img"></p>
<p><strong>2.Self-Attention Block</strong></p>
<p><strong>A.Self-Attention Layer</strong></p>
<p><img src="https://pic4.zhimg.com/80/v2-4ad7a98ce285113021eade4349199c5f_720w.jpg" alt="img"></p>
<p><strong>C.Point-Wise Feed-Forward Network</strong>: 尽管self-attention能将之前item的emebdding使用自适应的权重进行集成，但仍然是一个先线性模型,为了加入非线性能力, 我们使用两层的DDN,</p>
<p><img src="https://pic2.zhimg.com/80/v2-bdfa1cac41b3f4aa676e81d54a72671d_720w.jpg" alt="img"></p>
<p><strong>3.Stacking Self-Attention Blocks</strong></p>
<p>在第一个self-attention block之后,学习item的迁移可以学习更加复杂的item迁移,所以我们对self-attention block进行stacking,第b(b&gt;1)的block可以用下面的形式进行定义：</p>
<p><img src="https://pic3.zhimg.com/80/v2-cdc40ee5705587460d39e19649625942_720w.jpg" alt="img"></p>
<p><strong>4.Prediction Layer</strong></p>
<p><img src="https://pic2.zhimg.com/80/v2-873157dd4336dcbbd818227c7ced3f25_720w.jpg" alt="img"></p>
<p>使用同质(homogeneous)商品embedding的一个潜在问题是，它们的内部产品不能代表不对称的商品转换。然而，我们的模型没有这个问题，因为它学习了一个非线性变换。例如，前馈网络可以很容易地实现同项嵌入的不对称性,<strong>经验上使用共享的商品embedding也可以大大提升模型的效果;</strong></p>
<p><strong>显示的用户建模</strong>：为了提供个性化的推荐,现有的方法常用两种方法,(1).学习显示的用户embedding表示用户的喜好;(2).考虑之前的行为并且引入隐式的用户embedding。此处使用并没有带来提升。</p>
<p><strong>5.网络训练</strong></p>
<p><img src="https://pic1.zhimg.com/80/v2-684099a2a86837c0b3ad701ea2169710_720w.jpg" alt="img"></p>
<p><strong>6.方案复杂度分析</strong></p>
<p><strong>a. 空间复杂度</strong></p>
<p>模型中学习的参数来自于self-attention.ffn以及layer normalization的参数,总的参数为:</p>
<p><img src="https://pic4.zhimg.com/80/v2-3d4d8db1c48964728a0c6830ecc4a71b_720w.jpg" alt="img"></p>
<p><strong>b. 时间复杂度</strong></p>
<p>我们模型的计算复杂度主要在于self-attention layer和FFN网络,</p>
<p><img src="https://pic1.zhimg.com/80/v2-1cd0b2b09e9bc3fba57281ab76f2d478_720w.jpg" alt="img"></p>
<p>里面最耗时间的还是self-attention layer, 不过这个可以进行并行化。</p>
<h2 id="Experiment-amp-Table"><a href="#Experiment-amp-Table" class="headerlink" title="Experiment &amp; Table"></a>Experiment &amp; Table</h2><p>该次实验主要为了验证下面的四个问题：</p>
<ol>
<li>是否SASRec比现有最好的模型(CNN/RNN)要好？</li>
<li>在SASRec框架中不同的成份的影响怎么样？</li>
<li>SASRec的训练效率和可扩展性怎么样？</li>
<li>attention的权重是否可以学习得到关于位置和商品属性的有意义的模式?</li>
</ol>
<p><strong>1. 推荐效果比较</strong></p>
<p><img src="https://pic4.zhimg.com/80/v2-e789c62c7c2e998f0713341ebc43155f_720w.jpg" alt="img"></p>
<ul>
<li>SASRec在稀疏的和dense的数据集合熵比所有的baseline都要好, 获得了6.9%的Hit Rate提升以及9.6%的NDCG提升；</li>
</ul>
<p><strong>2. SASRec框架中不同成份的影响</strong></p>
<p><img src="https://pic2.zhimg.com/80/v2-155ea54d12922a3d1aafcece005b5731_720w.jpg" alt="img"></p>
<ul>
<li>删除PE: 删除位置embedding ,在稀疏的数据集上,删除PE效果变好,但是在稠密的数据集上,删除PE的效果变差了。</li>
<li>不共享IE(Item Embedding): 使用共享的item embedding比不使用要好很多;</li>
<li>删除RC(Residual Connection):不实用残差连接,性能会变差非常多;</li>
<li>删除Dropout: dropout可以帮助模型,尤其是在稀疏的数据集上,Dropout的作用更加明显;</li>
<li>blocks的个数：没有block的时候,效果最差,在dense数据集上,相比稀疏数据多一些block的效果好一些;</li>
<li>Multi-head:在我们数据集上,single-head效果最好.</li>
</ul>
<p><strong>3. SASRec的训练效率和可扩展性</strong></p>
<p><img src="https://pic4.zhimg.com/80/v2-b4a0692c6cf9b0a335dae79eba2ed723_720w.jpg" alt="img"></p>
<p><img src="https://pic1.zhimg.com/80/v2-1a7f5f0f47c7ada0e2ccd22b23078584_720w.jpg" alt="img"></p>
<ul>
<li>SASRec是最快的;</li>
<li>序列长度可以扩展至500左右.</li>
</ul>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://zhuanlan.zhihu.com/p/277660092?utm_source=qq">[1]推荐算法炼丹笔记：序列化推荐算法SASRec</a></p>
]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>序列推荐</tag>
        <tag>SASRec</tag>
        <tag>自注意力</tag>
      </tags>
  </entry>
  <entry>
    <title>论文笔记：《Streaming Session-based Recommendation》</title>
    <url>/2022/03/07/SSRM/</url>
    <content><![CDATA[<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220307185213298.png" alt="image-20220307185213298"></p>
<hr>
<p>原paper：<a href="https://dl.acm.org/doi/10.1145/3292500.3330839">Streaming Session-based Recommendation | Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</a></p>
<p>源码解读：未开源</p>
<hr>
<p>中译：流会话推荐</p>
<p>总结：第一篇结合了流推荐和会话推荐的论文（准备开坑）。主要解决两个问题，MF attention + GRU  解决用户行为的不确定性；存储技术+主动采样策略 解决了更贴近实时场景的“高速、海量、连续的流数据”的需求。个人认为可以进一步做的地方：session encoder部分，用新模型；储存技术；采样技术。</p>
<hr>
<span id="more"></span>

<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li><p>question作者想解决什么问题？ </p>
<p>1）用户行为的不确定性。</p>
<p>2）会话推荐在实际场景中会以会话流的形式出现，即连续不断的、海量的、高速的会话数据，但现有会话推荐模型都是离线模型，没有模型可以解决这个问题。</p>
</li>
<li><p>method作者通过什么理论/模型来解决这个问题？</p>
<p>作者提出SSRM(Streaming Session-based Recommendation Machine)模型，其中</p>
<p>1）为了解决用户行为的不确定性，作者利用历史交互，提出基于矩阵分解的注意力模型。</p>
<p>2）为了解决流会话数据“海量”、“高速”的挑战，作者基于存储提出使用主动采样策略的流模型。</p>
</li>
<li><p>answer作者给出的答案是什么？</p>
<p>在LastFM和Gowalla数据集上证明了SOTA。</p>
</li>
</ul>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>第一篇提出流会话推荐模型的论文。</p>
<ul>
<li><p>why作者为什么研究这个课题？    </p>
<p>“流会话”(Streaming session)的设定更贴近实际。</p>
</li>
<li><p>how当前研究到了哪一阶段？</p>
<p>第一篇提出流会话推荐模型的论文。</p>
</li>
<li><p>what作者基于什么样的假设（看不懂最后去查）？</p>
<p>1）用户的历史交互信息是可获得的</p>
<p>2）背景是流会话的设定，即会话数据海量、连续不断、迅速地迭代。</p>
</li>
</ul>
<h2 id="Dataset-amp-Metric"><a href="#Dataset-amp-Metric" class="headerlink" title="Dataset &amp; Metric"></a>Dataset &amp; Metric</h2><ul>
<li>数据来源 </li>
</ul>
<p>LastFM：<a href="http://mtg.upf.edu/static/datasets/last.fm/lastfm-dataset-1K.tar.gz">http://mtg.upf.edu/static/datasets/last.fm/lastfm-dataset-1K.tar.gz</a></p>
<p>Gowalla：<a href="https://snap.stanford.edu/data/loc-gowalla.html">https://snap.stanford.edu/data/loc-gowalla.html</a></p>
<ul>
<li>数据划分</li>
</ul>
<p>给数据集 $D$ 中的会话按时间排序，分成前60%作为训练集，和后40%作为候选集。为了模拟线上的流数据输入，将候选集再划分成5个等长切片作为测试机。第一个测试机和10%的训练集作为验证集。实验中，若要预测第 $i$ 个测试集的序列行为，那么 $i$ 之前的测试集切片都用作在线训练。</p>
<ul>
<li>重要指标 </li>
</ul>
<p>MRR@20、Recall@20</p>
<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><ul>
<li><h3 id="SSRM模型框架"><a href="#SSRM模型框架" class="headerlink" title="SSRM模型框架"></a>SSRM模型框架</h3></li>
</ul>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220307191209899.png" alt="image-20220307191209899"></p>
<p>SSRM的主要工作是，建立一个基于注意力的会话推荐系统（离线模型），再将其拓展到流会话的设定。</p>
<ul>
<li><h3 id="离线模型：基于历史行为的注意力编码器"><a href="#离线模型：基于历史行为的注意力编码器" class="headerlink" title="离线模型：基于历史行为的注意力编码器"></a>离线模型：基于历史行为的注意力编码器</h3></li>
</ul>
<p>离线模型 = 建模序列 + 矩阵分解</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220307193225545.png" alt="image-20220307193225545"></p>
<p>其中，建模“序列” = 基础会话编码器 + 基于矩阵分解的注意力会话编码器。这两部分如下1.2.：</p>
<ol>
<li><strong>基础会话编码器(Basic Session Encoder)</strong></li>
</ol>
<p>主要为了建模当前会话的表示。采用基本的GRU模型（重置门、更新门、候选状态），将最后一个隐藏状态作为当前会话 $i$ 的基本表示： $c_i = h_t$ 。</p>
<ol start="2">
<li><strong>基于矩阵分解的注意力会话编码器(MF-based Attentive Session Encoder)</strong></li>
</ol>
<p>MF分解得到每个用户的隐含表示 $p_u \in \R ^ {1 \times D} $ 和每个物品的隐含表示 $q_t \in \R ^ {1 \times D} $ 。MF既用来单独作为一个模块，还通过引入MF（利用了历史交互信息）来加强上面的基础模型。</p>
<p>$p_u $ 和 $q_t$ 的内积  $\hat y_{u,t}^R = &lt;p_u,q_t&gt;=p_u \cdot q_t$ 用来表示用户 $u$ 有多“喜欢”物品 $i$ 。这里的 $q_t$ 和上面的 $h_t$ 是物品 $t$ 的两种表示，作用、含义不同： $h_t$ ，即GRU在物品 $t$ 处的隐含表示，是总结了序列行为(1,2,…,t)的物品表示；而 $q_t$ 作为实际的物品表示，可用于和用户隐含表示做内积，表明用户是否喜欢这个物品。</p>
<p>为了降低随机性、捕捉用户 $u$ 的主要意图，这里使用了注意力机制编码会话表示 $c_i$：<br>$$<br>c_i = [\ \sum^t_{j=1}\alpha_{u,j}\cdot h_j \ ; h_t \ ]<br>$$<br>其中权重 $\alpha_u = softmax(\hat y_u^R)$ ，即对用户 $u$ 对序列中物品的“喜欢”程度的打分进行softmax，得到和为1的权重。求到加权的表示后，再与GRU得到的会话表示 $h_t$ 拼接，作为最终的会话表示（这里 $h_t$ 用到了两次，一次作为最后一个物品表示，一次作为会话表示）。</p>
<ol start="3">
<li><strong>混合注意力推荐系统</strong></li>
</ol>
<p>SSRM再进一步将注意力编码器的输出和MF的输出结合，使得模型不仅考虑当前会话的序列行为，还考虑了用户的长期兴趣和共现行为。</p>
<p>会话编码器最后输出，每个会话对每个物品的打分： $\hat y_{i,t}^S = q_t B c_i^T $  ，表示第 $i$ 个会话对物品 $t$ 的i打分，其中 $q_t \in \R ^ {1 \times D} $ ， $c_i \in \R ^ {1 \times H} $ ，变换矩阵 $B \in \R ^ {D \times H} $ 。</p>
<p>由MF得到的 $p_u$ 和 $q_t$  的内积  $\hat y_{u,t}^R =p_u \cdot q_t$ 。</p>
<p>两者使用一个可调参数 $w$ 加权求和  $\hat y_{i,t} = w \hat y_{i,t}^R + (1-w)\hat y_{i,t}^S $ 。</p>
<p>最后，把物品排序任务当作分类任务，使用CE loss损失函数训练模型。 $r_u$ 是物品的真实标签分布， $\hat y_u$ 是预测的概率分布。<br>$$<br>L(r_u,\hat y_u) = - \sum^n_{i=1} r_{u,i} \cdot log(\hat y_{u,i})<br>$$</p>
<ul>
<li><h3 id="在线训练：基于存储的、使用主动采样策略的流模型"><a href="#在线训练：基于存储的、使用主动采样策略的流模型" class="headerlink" title="在线训练：基于存储的、使用主动采样策略的流模型"></a>在线训练：基于存储的、使用主动采样策略的流模型</h3></li>
</ul>
<p>建立完基于注意力的会话推荐系统（离线模型）后，再将其拓展到流会话的设定。建立存储的目的是准确地概括总结历史行为。</p>
<p><strong>1.传统模型的更新方法</strong></p>
<p>随机采样技术。令 $C$ 为保存会话序列的存储库，令 $t$ 为下一个时刻即将到达的数据实例。当 $t&gt;|C|$ 时，存储库会以 $\frac{|C|}{t} $ 的概率存储这个数据实例，同时随机替换掉存储库 $C$ 里的数据实例。这样得到的存储库可以证明相当于当前数据集的随机采样结果，同时也证明可以保留模型的长期记忆[1]。但是 $\frac{|C|}{t} $ 是随时间递减的，这样模型就会倾向于忽略最近的数据，因为越近的数据被选入存储库的概率越低。而实际场景中是存在用户兴趣漂移的现象的，换句话说，使用这种随机采样技术难以实现“捕捉最新产生的数据中的行为模式”。</p>
<p><strong>2.主动采样策略</strong></p>
<p>虽然更新时同时使用整个存储库和所有新到达数据能产生更好结果，但是由于高速的流数据和有限的计算资源，这种方式通常会导致可利用的数据非常少（这个问题也被称为系统过载）。所以需要一个明智的样本选择策略。</p>
<p>本文提出的主动采样策略的思想和一些主动学习（Active learning）类似，即“选择对系统贡献最大的样本的最小集合”，供用户评估。具体来说，为了使模型在有限时间窗口内尽可能地多学习，采样策略每次应该选择 $C^{new}\cup C$ 中<strong>信息量最大</strong>的会话实例。</p>
<p>计算会话的信息量，使用会话中每个物品得分的均值，其中物品得分 $r_{u,k}$ 表示用户 $u$ 在当前模型下对物品 $k$ 的预测能力，使用用户隐含表示和物品隐含表示做内积得到 $r_{u,k}=p_u q_k$。$r_{u,k}$ 值越小， $r_{s_i}$ 值越小，说明模型越难以预测该会话中的物品，说明该会话信息量越大，越能修正模型，越该对它进行采样。<br>$$<br>r_{s_i}=\frac{\sum^t_{k=1}r_{u,k}}{t}<br>$$<br>以 $r_{s_i}$ 值对这些会话降序排列，再根据排名计算每个会话的权重因子 $w_{s_i}$，最后得到每个会话的采样概率 $p(s_i)$ ：<br>$$<br>w_{s_i}=exp(\frac{rank_{s_i}}{C\cup C^{new}})<br>;\<br>p(s_i)=\frac{w_{s_i}}{\sum_{s_i \in C \cup C^{new}}w_{s_i}}<br>$$<br>其中，信息量越大的会话排名越靠后，权重因子越高，采样概率也越大。</p>
<h2 id="Experiment-amp-Table"><a href="#Experiment-amp-Table" class="headerlink" title="Experiment &amp; Table"></a>Experiment &amp; Table</h2><ul>
<li><h3 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述"></a>算法描述</h3></li>
</ul>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220308193608578.png" alt="image-20220308193608578"></p>
<ul>
<li><h3 id="和SBR的经典方法比较"><a href="#和SBR的经典方法比较" class="headerlink" title="和SBR的经典方法比较"></a>和SBR的经典方法比较</h3></li>
</ul>
<p>SOTA。LastFM的提升率比Gowalla高的主要原因是，Gowalla更系数，每个用户的点击很少，并且会话数据只有寥寥几个，从而导致训练不充分。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220308193302601.png" alt="image-20220308193302601"></p>
<ul>
<li><h3 id="验证矩阵分解注意力机制的有效性（MF-Attention）"><a href="#验证矩阵分解注意力机制的有效性（MF-Attention）" class="headerlink" title="验证矩阵分解注意力机制的有效性（MF-Attention）"></a>验证矩阵分解注意力机制的有效性（MF-Attention）</h3></li>
</ul>
<p>Baseline：只有基础会话编码器（Basic session encoder），好像就是GRU4Rec。三个模型采用相同的streaming技术，以消除线上更新方法带来的影响。SSRM效果最好。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220308193902198.png" alt="image-20220308193902198"></p>
<ul>
<li><h3 id="不同流策略的影响"><a href="#不同流策略的影响" class="headerlink" title="不同流策略的影响"></a>不同流策略的影响</h3></li>
</ul>
<p>S1：没有存储库，且仅用新来的数据更新模型。S2：有存储库，采用从 $C$ 中随机采样训练的传统方法。S3：S2的基础上，从 $C\cup C^{new}$ 中采样。S1、S3优于S2，说明相比于历史long-term memory，模型更能从用户最近行为中受益。而SSRM模型优于其它所有，说明本文提出的主动采样策略是有效的。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220308215603572.png" alt="image-20220308215603572"></p>
<ul>
<li><h3 id="不同过载设定的影响"><a href="#不同过载设定的影响" class="headerlink" title="不同过载设定的影响"></a>不同过载设定的影响</h3></li>
</ul>
<p>$W$ ：固定时间窗口，即一次能从 $C\cup C^{new}$ 中采样的样本数。 $W$ 越小说明工作负载越重，只训练到有限的序列。横轴看，窗口越大，过载越轻，模型表现越好；反之窗口越大，过载越重，模型表现越差。纵向看不同测试集，因为这些测试集是按时间顺序分的，越往后新用户、新物品越多。但是可以发现，从test2到test5，它们之间的gap越来越小。SSRM能够快速减小这种gap，证明其处理新数据的能力。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220308215631714.png" alt="image-20220308215631714"></p>
<ul>
<li><h3 id="存储库大小的影响"><a href="#存储库大小的影响" class="headerlink" title="存储库大小的影响"></a>存储库大小的影响</h3></li>
</ul>
<p> $|C|$ 的大小决定了保留多少历史信息，它保留的观测行为越多，就越可能从历史行为中采样，就会有更多的样本距离current behavior越久远，故模型就会用更少的当前会话信息来更新。换句话说，对于过去没有出现的用户和项目，模型会学习得更少，这将导致性能相对较差。Figure 6 里也能得出这个结论，最近的行为更重要。但是因为存在long-memory problem，如果只用当前session来训练模型的话，结果会变低，总的来说，历史行为和当前行为两者得结合起来。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220308194605932.png" alt="image-20220308194605932"></p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>[1]Ernesto Diaz-Aviles, Lucas Drumond, Lars Schmidt-Thieme, and Wolfgang Nejdl. 2012. Real-time top-n recommendation in social streams. RecSys (2012)</p>
]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>流会话推荐</tag>
        <tag>SSRM</tag>
      </tags>
  </entry>
  <entry>
    <title>SASRec代码笔记</title>
    <url>/2021/11/06/SASRec%E4%BB%A3%E7%A0%81/</url>
    <content><![CDATA[<hr>
<p>完整的代码注释：<a href="https://github.com/Guadzilla/Paper_notebook/tree/main/SASRec">https://github.com/Guadzilla/Paper_notebook/tree/main/SASRec</a></p>
<p>论文笔记：<a href="https://guadzilla.github.io/2021/11/03/SASRec/">https://guadzilla.github.io/2021/11/03/SASRec/</a></p>
<hr>
<h2 id="collections-defaultdict-list"><a href="#collections-defaultdict-list" class="headerlink" title="collections.defaultdict(list)"></a>collections.defaultdict(list)</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">collections</span>.<span class="title">defaultdict</span>(<span class="params">default_factory=<span class="literal">None</span>, /[, ...]</span>)</span></span><br></pre></td></tr></table></figure>

<p>返回一个新的类似字典的对象。<code>defaultdict</code> 是内置 <code>dict </code>类的子类。 它重载了一个方法并添加了一个可写的实例变量。</p>
<p>本对象包含一个名为 <code>default_factory</code> 的属性，构造时，第一个参数用于为该属性提供初始值，默认为 None。所有其他参数（包括关键字参数）都相当于传递给 dict 的构造函数。</p>
<p>使用<code> defulydict(list)</code>实例化对象时， <code>default_factory=list</code>，可以很轻松地<strong>将（键-值对组成的）序列转换为（键-列表组成的）字典</strong>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s = [(<span class="string">&#x27;yellow&#x27;</span>, <span class="number">1</span>), (<span class="string">&#x27;blue&#x27;</span>, <span class="number">2</span>), (<span class="string">&#x27;yellow&#x27;</span>, <span class="number">3</span>), (<span class="string">&#x27;blue&#x27;</span>, <span class="number">4</span>), (<span class="string">&#x27;red&#x27;</span>, <span class="number">1</span>)]</span><br><span class="line">d = defaultdict(<span class="built_in">list</span>)</span><br><span class="line"><span class="keyword">for</span> k, v <span class="keyword">in</span> s:</span><br><span class="line">    d[k].append(v)</span><br><span class="line"></span><br><span class="line"><span class="built_in">sorted</span>(d.items())</span><br><span class="line"><span class="comment"># 输出：[(&#x27;blue&#x27;, [2, 4]), (&#x27;red&#x27;, [1]), (&#x27;yellow&#x27;, [1, 3])]</span></span><br></pre></td></tr></table></figure>

<p>当字典中没有的键第一次出现时，python自动为其返回一个空列表，list.append()会将值添加进新列表；再次遇到相同的键时，list.append()将其它值再添加进该列表。</p>
<span id="more"></span>

<h2 id="Python自定义多线程"><a href="#Python自定义多线程" class="headerlink" title="Python自定义多线程"></a>Python自定义多线程</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">random_neq</span>(<span class="params">l, r, s</span>):</span></span><br><span class="line">    t = np.random.randint(l, r)</span><br><span class="line">    <span class="keyword">while</span> t <span class="keyword">in</span> s:</span><br><span class="line">        t = np.random.randint(l, r)</span><br><span class="line">    <span class="keyword">return</span> t</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample_function</span>(<span class="params">user_train, usernum, itemnum, batch_size, maxlen, result_queue, SEED</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sample</span>():</span></span><br><span class="line"></span><br><span class="line">        user = np.random.randint(<span class="number">1</span>, usernum + <span class="number">1</span>)    <span class="comment"># 随机采样user id，注意是从1开始的</span></span><br><span class="line">        <span class="keyword">while</span> <span class="built_in">len</span>(user_train[user]) &lt;= <span class="number">1</span>: user = np.random.randint(<span class="number">1</span>, usernum + <span class="number">1</span>)  <span class="comment"># 长度小于1的训练集不要</span></span><br><span class="line"></span><br><span class="line">        seq = np.zeros([maxlen], dtype=np.int32)    <span class="comment"># seq序列，长度固定为maxlen，用0在前面padding补上长度，例：[0,0,...,0,23,15,2,6]</span></span><br><span class="line">        pos = np.zeros([maxlen], dtype=np.int32)</span><br><span class="line">        neg = np.zeros([maxlen], dtype=np.int32)</span><br><span class="line">        nxt = user_train[user][-<span class="number">1</span>]  <span class="comment"># user_train的最后一个item取为nxt</span></span><br><span class="line">        idx = maxlen - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        ts = <span class="built_in">set</span>(user_train[user])  <span class="comment"># ts为序列的item集合</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">reversed</span>(user_train[user][:-<span class="number">1</span>]):   <span class="comment"># 从后往前遍历user_train,idx为当前要填充的下标</span></span><br><span class="line">            seq[idx] = i</span><br><span class="line">            pos[idx] = nxt</span><br><span class="line">            <span class="keyword">if</span> nxt != <span class="number">0</span>: neg[idx] = random_neq(<span class="number">1</span>, itemnum + <span class="number">1</span>, ts)  <span class="comment"># 生成的负样本不能取该序列item集合里的item</span></span><br><span class="line">            nxt = i</span><br><span class="line">            idx -= <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> idx == -<span class="number">1</span>: <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> (user, seq, pos, neg)    <span class="comment"># 返回一次采样，(用户id,训练序列，label序列，负样本序列)</span></span><br><span class="line"></span><br><span class="line">    np.random.seed(SEED)</span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:     <span class="comment"># 采样一个batch_size大小的数据样本，打包成一个batch，放到线程队列里</span></span><br><span class="line">        one_batch = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(batch_size):</span><br><span class="line">            one_batch.append(sample())</span><br><span class="line"></span><br><span class="line">        result_queue.put(<span class="built_in">zip</span>(*one_batch))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WarpSampler</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, User, usernum, itemnum, batch_size=<span class="number">64</span>, maxlen=<span class="number">10</span>, n_workers=<span class="number">1</span></span>):</span></span><br><span class="line">        self.result_queue = Queue(maxsize=n_workers * <span class="number">10</span>)   <span class="comment"># 长度为10的线程队列</span></span><br><span class="line">        self.processors = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_workers):</span><br><span class="line">            self.processors.append(     <span class="comment"># Process()进程的类, target：要调用的对象即sampler_function，args：调用该对象要接受的参数</span></span><br><span class="line">                Process(target=sample_function, args=(User,</span><br><span class="line">                                                      usernum,</span><br><span class="line">                                                      itemnum,</span><br><span class="line">                                                      batch_size,</span><br><span class="line">                                                      maxlen,</span><br><span class="line">                                                      self.result_queue,</span><br><span class="line">                                                      np.random.randint(<span class="number">2e9</span>)</span><br><span class="line">                                                      )))</span><br><span class="line">            self.processors[-<span class="number">1</span>].daemon = <span class="literal">True</span></span><br><span class="line">            self.processors[-<span class="number">1</span>].start()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">next_batch</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.result_queue.get()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> self.processors:</span><br><span class="line">            p.terminate()</span><br><span class="line">            p.join()</span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line"><span class="comment"># sampler是WarpSampler对象的实例，每次调用sampler.next_batch(),就返回一个batch的样本。</span></span><br><span class="line"><span class="comment"># 进一步解释：每次调用sampler.next_batch()就call其线程队列里的一个线程，每个线程用于返回一个batch的数据。</span></span><br><span class="line">sampler = WarpSampler(user_train, usernum, itemnum, batch_size=args.batch_size, maxlen=args.maxlen, n_workers=<span class="number">3</span>)</span><br></pre></td></tr></table></figure>



<h2 id="torch-tril"><a href="#torch-tril" class="headerlink" title="torch.tril()"></a>torch.tril()</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.tril(<span class="built_in">input</span>, diagonal=<span class="number">0</span>, *, out=<span class="literal">None</span>) → Tensor</span><br><span class="line"><span class="comment"># 功能：返回下三角矩阵其余部分用out填充（默认为0）</span></span><br><span class="line"><span class="comment"># input：输入矩阵，二维tensor</span></span><br><span class="line"><span class="comment"># diagonal：表示对角线位置，diagonal=0为主对角线，diagonal=-1为主对角线往下1格，diagonal=1为主对角线往上1格</span></span><br><span class="line"><span class="comment"># out：表示填充，默认用out=None即0填充</span></span><br></pre></td></tr></table></figure>

<p>例：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.randn(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">tensor([[-<span class="number">1.0813</span>, -<span class="number">0.8619</span>,  <span class="number">0.7105</span>],</span><br><span class="line">        [ <span class="number">0.0935</span>,  <span class="number">0.1380</span>,  <span class="number">2.2112</span>],</span><br><span class="line">        [-<span class="number">0.3409</span>, -<span class="number">0.9828</span>,  <span class="number">0.0289</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tril(a)</span><br><span class="line">tensor([[-<span class="number">1.0813</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>],</span><br><span class="line">        [ <span class="number">0.0935</span>,  <span class="number">0.1380</span>,  <span class="number">0.0000</span>],</span><br><span class="line">        [-<span class="number">0.3409</span>, -<span class="number">0.9828</span>,  <span class="number">0.0289</span>]])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = torch.randn(<span class="number">4</span>, <span class="number">6</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b</span><br><span class="line">tensor([[ <span class="number">1.2219</span>,  <span class="number">0.5653</span>, -<span class="number">0.2521</span>, -<span class="number">0.2345</span>,  <span class="number">1.2544</span>,  <span class="number">0.3461</span>],</span><br><span class="line">        [ <span class="number">0.4785</span>, -<span class="number">0.4477</span>,  <span class="number">0.6049</span>,  <span class="number">0.6368</span>,  <span class="number">0.8775</span>,  <span class="number">0.7145</span>],</span><br><span class="line">        [ <span class="number">1.1502</span>,  <span class="number">3.2716</span>, -<span class="number">1.1243</span>, -<span class="number">0.5413</span>,  <span class="number">0.3615</span>,  <span class="number">0.6864</span>],</span><br><span class="line">        [-<span class="number">0.0614</span>, -<span class="number">0.7344</span>, -<span class="number">1.3164</span>, -<span class="number">0.7648</span>, -<span class="number">1.4024</span>,  <span class="number">0.0978</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tril(b, diagonal=<span class="number">1</span>)</span><br><span class="line">tensor([[ <span class="number">1.2219</span>,  <span class="number">0.5653</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>],</span><br><span class="line">        [ <span class="number">0.4785</span>, -<span class="number">0.4477</span>,  <span class="number">0.6049</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>],</span><br><span class="line">        [ <span class="number">1.1502</span>,  <span class="number">3.2716</span>, -<span class="number">1.1243</span>, -<span class="number">0.5413</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>],</span><br><span class="line">        [-<span class="number">0.0614</span>, -<span class="number">0.7344</span>, -<span class="number">1.3164</span>, -<span class="number">0.7648</span>, -<span class="number">1.4024</span>,  <span class="number">0.0000</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tril(b, diagonal=-<span class="number">1</span>)</span><br><span class="line">tensor([[ <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>],</span><br><span class="line">        [ <span class="number">0.4785</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>],</span><br><span class="line">        [ <span class="number">1.1502</span>,  <span class="number">3.2716</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>],</span><br><span class="line">        [-<span class="number">0.0614</span>, -<span class="number">0.7344</span>, -<span class="number">1.3164</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>]])</span><br></pre></td></tr></table></figure>



<h2 id="Python中的-波浪线运算符"><a href="#Python中的-波浪线运算符" class="headerlink" title="Python中的 ~ 波浪线运算符"></a>Python中的 ~ 波浪线运算符</h2><p>~，用法只有一个那就是按位取反</p>
<p><a href="https://blog.csdn.net/lanchunhui/article/details/51746477"> Python 波浪线与补码_https://space.bilibili.com/59807853-CSDN博客_python 波浪线</a></p>
<h2 id="torch-nn-MultiAttention"><a href="#torch-nn-MultiAttention" class="headerlink" title="torch.nn.MultiAttention"></a>torch.nn.MultiAttention</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.nn.MultiheadAttention(embed_dim, num_heads, dropout=<span class="number">0.0</span>, bias=<span class="literal">True</span>, add_bias_kv=<span class="literal">False</span>, add_zero_attn=<span class="literal">False</span>, kdim=<span class="literal">None</span>, vdim=<span class="literal">None</span>, batch_first=<span class="literal">False</span>, device=<span class="literal">None</span>, dtype=<span class="literal">None</span>):</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>对应公式：<br>$$<br>Multihead(Q,K,V) = Concat(head_1,…,head_h)W^O    \<br>where \quad head_i= Attention(QW^Q_i,KW^K_i,VW^V_i)<br>$$</p>
<p>计算公式：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">forward(query, key, value, key_padding_mask=<span class="literal">None</span>, need_weights=<span class="literal">True</span>, attn_mask=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>

<p>QKV比较常规，需要注意的是</p>
<ol>
<li>key_padding_mask参数，大小为（N，S），指定key中的哪些元素不做attention计算，即看作padding。注意，为True的位置不计算attention（是padding的地方不计算）</li>
<li>attn_mask参数，</li>
</ol>
<h2 id="torch-nn-BCEWithLogitsLoss"><a href="#torch-nn-BCEWithLogitsLoss" class="headerlink" title="torch.nn.BCEWithLogitsLoss()"></a>torch.nn.BCEWithLogitsLoss()</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">forward(self, input: Tensor, target: Tensor) -&gt; Tensor</span><br></pre></td></tr></table></figure>

<p>参数说明：</p>
<ul>
<li>input: Tensor of arbitrary shape as unnormalized scores (often referred to as logits).</li>
<li>target: Tensor of the same shape as input with values between 0 and 1</li>
</ul>
<p>input：$x$        output：$y$</p>
<p>$$<br>ℓ(x,y)=L={l_1,…,l_N}^T<br>$$<br>$$<br>l_n=−w_n[y_n·log\sigma(x_n)+(1−y_n)·log(1−\sigma(x_n))]<br>$$</p>
<p>当 $y=1$ 时，$l_n=−log\sigma(x_n)$  ；当 $y=0$ 时，$l_n=−log(1-\sigma(x_n))$     。</p>
<p>论文里使用了一个全1的矩阵pos_labels，和一个全0的矩阵neg_labels。正例标签值都为1（正确的item，ground truth应该是概率为1），负例标签值都为0（错误的item，ground truth应该是概率为0）。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pos_labels, neg_labels = torch.ones(pos_logits.shape, device=args.device), \</span><br><span class="line">torch.zeros(neg_logits.shape, device=args.device)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># print(&quot;\neye ball check raw_logits:&quot;); print(pos_logits); print(neg_logits)</span></span><br><span class="line"><span class="comment"># check pos_logits &gt; 0, neg_logits &lt; 0</span></span><br><span class="line">adam_optimizer.zero_grad()</span><br><span class="line">indices = np.where(pos != <span class="number">0</span>)    <span class="comment"># 返回一个二维数组array， array[0]=[横坐标], array[1]=[纵坐标]</span></span><br><span class="line">loss = bce_criterion(pos_logits[indices], pos_labels[indices])  <span class="comment"># 使正例的得分尽量</span></span><br><span class="line">loss += bce_criterion(neg_logits[indices], neg_labels[indices])</span><br></pre></td></tr></table></figure>



<h2 id="torch-argsort"><a href="#torch-argsort" class="headerlink" title="torch.argsort()"></a>torch.argsort()</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.argsort(<span class="built_in">input</span>, dim=-<span class="number">1</span>, descending=<span class="literal">False</span>) → LongTensor</span><br></pre></td></tr></table></figure>

<p>沿着指定dim从小到大（默认）排序元素，然后返回这些元素原来的下标。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt;t = torch.randint(<span class="number">1</span>,<span class="number">10</span>,(<span class="number">1</span>,<span class="number">5</span>))</span><br><span class="line">&gt;&gt;&gt;t</span><br><span class="line">tensor([[<span class="number">7</span>, <span class="number">9</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">3</span>]])</span><br><span class="line">&gt;&gt;&gt;t.argsort()</span><br><span class="line">tensor([[<span class="number">4</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">1</span>]])</span><br><span class="line">&gt;&gt;&gt;t.argsort().argsort()</span><br><span class="line">tensor([[<span class="number">3</span>, <span class="number">4</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 两次argsort()可以返回每个元素的rank排名</span></span><br><span class="line"><span class="comment"># 解释：</span></span><br><span class="line"><span class="comment"># 把商品0,1,2,3,4按顺序摆好，他们的得分分别为[7,9,5,6,3]</span></span><br><span class="line"><span class="comment"># 对所有商品的得分从小到大排序（argsort()操作）</span></span><br><span class="line"><span class="comment"># 得到积分排名是[3,5,6,7,9]，积分排名对应的商品id是[4,2,3,0,1]（第一次argsort()的结果），每个商品id对应的下标就是他们的得分名次</span></span><br><span class="line"><span class="comment"># 例如商品4得分最高排在第一位，商品1得分最低排最后一位</span></span><br><span class="line"><span class="comment"># 然后我们想得到0,1,2,3,4顺序下的结果</span></span><br><span class="line"><span class="comment"># 所以对商品id排序，使得商品摆放顺序由[4,2,3,0,1]变为[0,1,2,3,4]，这里也是argsort()操作，因为0~4天然有顺序关系</span></span><br><span class="line"><span class="comment"># [4,2,3,0,1]变为[0,1,2,3,4]的同时，排名情况[0,1,2,3,4]也变成了[3,4,1,2,0]（第二次argsort()的结果）</span></span><br><span class="line"><span class="comment"># 即求得每个商品在原来顺序下的得分名次</span></span><br></pre></td></tr></table></figure>

<p><a href="https://www.cnblogs.com/traditional/p/13702904.html">numpy中的argmax、argmin、argwhere、argsort、argpartition函数 - 古明地盆 - 博客园 (cnblogs.com)</a></p>
<h2 id="评价指标Hit-Ratio、NDCG-1"><a href="#评价指标Hit-Ratio、NDCG-1" class="headerlink" title="评价指标Hit Ratio、NDCG[1]"></a>评价指标Hit Ratio、NDCG<a href="https://dl.acm.org/doi/10.1145/2806416.2806504">[1]</a></h2><h3 id="Hit-Ratio"><a href="#Hit-Ratio" class="headerlink" title="Hit Ratio"></a>Hit Ratio</h3><p>Evaluation Metrics. Given a user, each algorithm produces a ranked list of items. To assess the ranked list with the ground-truth item set (GT), we adopt Hit Ratio (HR), which has been commonly used in top-N evaluation . If a test item appears in the recommended list, it is deemed a hit. HR is calculated as:<br>$$<br>HR@K=\frac{Number\ of \  Hits@K}{|GT|}<br>$$</p>
<h3 id="NDCG"><a href="#NDCG" class="headerlink" title="NDCG"></a>NDCG</h3><p>As the HR is recall-based metric, it does not reflect the accuracy of getting top ranks correct, which is crucial in many real-world applications. To address this, we also adopt Normalized Discounted Cumulative Gain (NDCG), which assigns higher importance to results at top ranks, scoring successively lower ranks with marginal fractional utility:<br>$$<br>NDCG@K=Z_K\sum^K_{i=1}\frac{2^{r_i}-1}{log_2{(i+1)}}<br>$$<br>where ZK is the normalizer to ensure the perfect ranking has a value of 1; ri is the graded relevance of item at position i. We use the simple binary relevance for our work: ri = 1 if the item is in the test set, and 0 otherwise. For both metrics, larger values indicate better performance. In the evaluation, we calculate both metrics for each user in the test set, and report the average score.</p>
<h3 id="代码实现："><a href="#代码实现：" class="headerlink" title="代码实现："></a>代码实现：</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># evaluate on test set</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span>(<span class="params">model, dataset, args</span>):</span></span><br><span class="line">    [train, valid, test, usernum, itemnum] = copy.deepcopy(dataset)  <span class="comment"># deepcopy一份用于valid和test</span></span><br><span class="line"></span><br><span class="line">    NDCG = <span class="number">0.0</span></span><br><span class="line">    HT = <span class="number">0.0</span></span><br><span class="line">    valid_user = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> usernum &gt; <span class="number">10000</span>:  <span class="comment"># 用户数量大于10000就随机采10000</span></span><br><span class="line">        users = random.sample(<span class="built_in">range</span>(<span class="number">1</span>, usernum + <span class="number">1</span>), <span class="number">10000</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        users = <span class="built_in">range</span>(<span class="number">1</span>, usernum + <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">for</span> u <span class="keyword">in</span> users:</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(train[u]) &lt; <span class="number">1</span> <span class="keyword">or</span> <span class="built_in">len</span>(test[u]) &lt; <span class="number">1</span>: <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">        seq = np.zeros([args.maxlen], dtype=np.int32)</span><br><span class="line">        idx = args.maxlen - <span class="number">1</span></span><br><span class="line">        <span class="comment"># 假设原始序列为[1,2,3,4,5,6,7]    6用于valid；7用于test</span></span><br><span class="line">        seq[idx] = valid[u][<span class="number">0</span>]  <span class="comment"># seq: [0,0,0,...,0,0,0,6]</span></span><br><span class="line">        idx -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">reversed</span>(train[u]):  <span class="comment"># seq: [0,0,0,...,0,1,2,3,4,5,6]  只剩test里的[7]用于预测</span></span><br><span class="line">            seq[idx] = i</span><br><span class="line">            idx -= <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> idx == -<span class="number">1</span>: <span class="keyword">break</span></span><br><span class="line">        rated = <span class="built_in">set</span>(train[u])  <span class="comment"># 序列物品集合</span></span><br><span class="line">        rated.add(<span class="number">0</span>)</span><br><span class="line">        item_idx = [test[u][<span class="number">0</span>]]  <span class="comment"># 取出ground truth label</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):  <span class="comment"># item_idx: [label,random,random,...,random] 1+100个随机物品，看得分在top10的情况</span></span><br><span class="line">            t = np.random.randint(<span class="number">1</span>, itemnum + <span class="number">1</span>)</span><br><span class="line">            <span class="keyword">while</span> t <span class="keyword">in</span> rated: t = np.random.randint(<span class="number">1</span>, itemnum + <span class="number">1</span>)</span><br><span class="line">            item_idx.append(t)</span><br><span class="line"></span><br><span class="line">        predictions = -model.predict(*[np.array(l) <span class="keyword">for</span> l <span class="keyword">in</span> [[u], [seq], item_idx]])</span><br><span class="line">        predictions = predictions[<span class="number">0</span>]  <span class="comment"># (1,101) -&gt; 101 (squeeze)</span></span><br><span class="line"></span><br><span class="line">        rank = predictions.argsort().argsort()[<span class="number">0</span>].item()  <span class="comment"># 做两次argsort()，可以得到每个位置的rank排名</span></span><br><span class="line"></span><br><span class="line">        valid_user += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> rank &lt; <span class="number">10</span>:  <span class="comment"># TOP10才记录，这里真实rank = rank + 1 ，因为argsort()索引包含0</span></span><br><span class="line">            NDCG += <span class="number">1</span> / np.log2(rank + <span class="number">2</span>)</span><br><span class="line">            HT += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> valid_user % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;.&#x27;</span>, end=<span class="string">&quot;&quot;</span>)</span><br><span class="line">            <span class="comment"># sys.stdout.flush()</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> NDCG / valid_user, HT / valid_user</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2 id="参考文献："><a href="#参考文献：" class="headerlink" title="参考文献："></a>参考文献：</h2><p><a href="https://dl.acm.org/doi/10.1145/2806416.2806504">[1]He X, Chen T, Kan M Y, et al. Trirank: Review-aware explainable recommendation by modeling aspects</a></p>
]]></content>
      <categories>
        <category>代码阅读</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>SASRec</tag>
      </tags>
  </entry>
  <entry>
    <title>论文笔记：《Time Interval Aware Self-Attention for Sequential Recommendation》</title>
    <url>/2021/11/18/TiSASRec/</url>
    <content><![CDATA[<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117164643993.png"></p>
<hr>
<p>原paper：<a href="https://dl.acm.org/doi/10.1145/3336191.3371786">https://dl.acm.org/doi/10.1145/3336191.3371786</a></p>
<p>源码解读：<a href="https://github.com/Guadzilla/Paper_notebook/tree/main/TiSASRec">https://github.com/Guadzilla/Paper_notebook/tree/main/TiSASRec</a></p>
<hr>
<p>中译：时间间隔感知的自注意力序列推荐</p>
<p>总结：是SASRec工作的延续，在self-attention的基础上加了绝对位置信息和相对时间间隔信息（加在Q和K里）取得了更好的performamce。发现Beauty数据集序列模式不明显。</p>
<span id="more"></span>

<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li>  <strong>question作者想解决什么问题？</strong></li>
</ul>
<p>MC模型和RNN模型都只将用户交互作为有序序列（一种强假设），却没有考虑交互与交互之间的时间间隔。</p>
<ul>
<li>  <strong>method作者通过什么理论/模型来解决这个问题？</strong></li>
</ul>
<p>在序列模型的结构中显式建模交互的时间戳（timestamps），并且探索不同时间间隔对next item推荐的影响。提出TiSASRec模型，模型建模了item在序列中的绝对位置以及交互之间的时间间隔。</p>
<ul>
<li>  <strong>answer作者给出的答案是什么？</strong></li>
</ul>
<p>展示了不同设定下TiSASRec的特点，比较了不同位置编码下自注意力模块的表现。在dense和sparse数据集都取得了SOTA。</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li>  <strong>why作者为什么研究这个课题？</strong></li>
</ul>
<p>Temporal recommendation（实时推荐）主要建模“绝对时间”来捕获用户与物品的实时动态，即挖掘实时模式、依据时间建模。Sequential recommendation（序列推荐）主要依据交互的顺序挖掘序列模式。序列推荐只用timestamps来决定item顺序，其实假设了所有交互之间是等间隔的。但一天之内产生的序列和一个月内产生的序列显然对next item的影响区别很大。</p>
<ul>
<li>  <strong>how当前研究到了哪一阶段</strong></li>
</ul>
<p>目前的序列推荐只挖掘序列模式，即假设交互之间是等间隔的，不合理。有模型使用自注意力+相对位置编码[1]，受到启发。</p>
<ul>
<li>  <strong>what作者基于什么样的假设（看不懂最后去查）</strong></li>
</ul>
<p>交互序列应该被建模为包含时间间隔的序列。</p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><ul>
<li><p><strong>优点</strong></p>
<ul>
<li>  结合了<strong>绝对位置编码</strong>和<strong>相对时间间隔</strong>编码的优点。</li>
<li>  证明了使用相对时间间隔的有效性。</li>
</ul>
</li>
<li><p>  <strong>缺点</strong></p>
</li>
<li><p>  <strong>展望</strong></p>
</li>
</ul>
<h2 id="Dataset-amp-Metric"><a href="#Dataset-amp-Metric" class="headerlink" title="Dataset &amp; Metric"></a>Dataset &amp; Metric</h2><ul>
<li><p><strong>数据来源</strong></p>
<ul>
<li>  MovieLens-1m</li>
<li>  Amazon CDs&amp;Vinyl/ Movies&amp;TV/ Beauty/ Games</li>
<li>  Steam</li>
</ul>
</li>
<li><p><strong>重要指标</strong></p>
<ul>
<li>  Hit@10、NDCG@10</li>
</ul>
</li>
</ul>
<h2 id="Method-amp-Table"><a href="#Method-amp-Table" class="headerlink" title="Method &amp; Table"></a>Method &amp; Table</h2><ul>
<li>模型架构</li>
</ul>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211116223143274.png"></p>
<ul>
<li>参数说明</li>
</ul>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211116223530787.png" alt="image-20211116223530787"></p>
<p><strong>1.个性化的时间间隔（time interval）</strong></p>
<p>规定了序列$S$的maxlen（n），长度小于n的序列用一个特殊标记的padding item来padding。时间序列$T$用第一个item的timestamps来padding（到这里还只是时间戳）。</p>
<p>对每个用户制定个性化的时间间隔，个性化指的其实就是下面介绍的缩放操作。对用户$u$来说，时间戳序列$t=(t_1,t_2,…,t_n)$，用任意两个物品的时间戳之差表示物品之间的时间间隔，作为任意两个物品之间的关系（relation）$r_{ij}$，于是得到时间间隔集合$R^u$。规定一个缩放系数$r^u_{min}=min(R^u)$，即序列里的最小时间间隔，再对所有时间间隔缩放$r^u_{ij}=\lfloor\frac{|r_i-r_j|}{r^u_{min}}\rfloor$，得到时间间隔矩阵$M^u\in N^{n\times n}$。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211116225152019.png" alt="image-20211116225152019"></p>
<p>另外，论文还规定了每个$r^u_{ij}$的阈值，对大于阈值的做了一个clip操作得到$M^u_{clipped}$。</p>
<p><strong>2.Embedding层</strong></p>
<ul>
<li>item的表示：padding item用$\vec0$表示，其它每个item用d维向量表示，构成$M^I\in R^{|I|\times d}$的item embedding矩阵，则前n个item的表示为$E^I\in R^{n\times d}$。</li>
</ul>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117222616679.png" alt="image-20211117222616679"></p>
<ul>
<li>position 的表示：即位置编码，用两个K、V矩阵$M^P_K\in R^{n\times d}$和$M^P_V\in R^{n\times d}$，表示每个位置（序列最大长度为n）的Key和Value向量，分别为$E^P_K\in R^{n\times d}$和$E^P_V\in R^{n\times d}$。</li>
</ul>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117081419839.png" alt="image-20211117081419839"></p>
<ul>
<li>relative time interval的表示：和positional embedding相似，用两个K、V矩阵$E^P_K\in R^{k\times d}$和$E^P_V\in R^{k\times d}$，表示每个位置（序列最大长度为n）的Key和Value向量，其中k表示一共有k种相对时间间隔。于是clipped后的$M^u_{clipped}$，把对应的$r_{ij}$替换成对应的K、V向量，就得到了$E^R_K\in R^{n\times n\times d}$和$E^R_V\in R^{n\times n\times d}$。</li>
</ul>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117081413089.png" alt="image-20211117081413089"></p>
<p><strong>3.时间间隔感知的自注意力机制</strong></p>
<p>仅有item和对应的时间戳也不能把序列确定下来，还要加入item在序列中的位置。</p>
<ul>
<li><strong>时间间隔感知的自注意力层Time Interval-Aware Self-attention Layer</strong></li>
</ul>
<p>传统的自注意力层为QKV模式，可以定义成$Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V$。用item embedding乘以$W^Q、W^K、W^V$投影到其对应的$Querry、Key、Value$空间上。</p>
<p>这里本质上也是这么做的，但对K和V做了一点改变。</p>
<p>作者首先将$E^I=(m_{s_1},m_{s_2},…,m_{s_n})$表示的item序列变换新序列$Z=(z_1,z_2,…,z_n)$，$z_i\in R^d$。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117222727853.png" alt="image-20211117222727853"></p>
<p>其中$(m_{s_j}W^V+r_{ij}^v+p_j^v)$**对应QKV模式里的$Value$**，其中$m_{s_j}W^Q$是将item embedding投影到$Value$空间，不过在此基础上还加上了realation embedding（相对时间间隔）和position embedding（位置编码）的value表示。</p>
<p>找到$Value$以后，公式就可以写成：$z_i=\sum^n_{j=1}\alpha_{ij}\ Value_j$。</p>
<p>系数$\alpha_{ij}$是其实就是$softmax(\frac{QK^T}{\sqrt{d_k}})$部分。$softmax()$在论文中体现在$\alpha_{ij}=\frac{exp\ e_{ij}}{\sum^n_{k=1}exp\ e_{ik}}$，那么可以猜测$\frac{QK^T}{\sqrt{d_k}}$就对应论文中的$e_{ij}$了。事实正如此，$e_{ij}$被定义为：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117090739720.png" alt="image-20211117090739720"></p>
<p>其中$m_{s_j}W^Q$是将item embedding投影到$Querry$空间，**对应QKV模式里的$Querry$**。</p>
<p>$(m_{s_j}W^K+r^k_{ij}+p^k_j)$，**对应QKV模式里的$Key$**，其中$m_{s_j}W^K$是将item embedding投影到$Key$空间，不过在此基础上还加上了realation embedding（相对时间间隔）和position embedding（位置编码）的key表示，另外除以的$\sqrt{d}$是缩放系数。</p>
<ul>
<li><strong>因果关系Causality</strong></li>
</ul>
<p>序列本身就有因果关系，因为我们在预测第t+1个物品时，只知道前t个物品的信息。但是在做self-attention时，每个物品都能感知到所有物品（因为Q对所有K做了查询），破坏了因果关系。所以我们必须规定，在做self-attention时，规定每个$Q_i$只能查询$K_j$，其中$j&lt;i$，即每个Q只能查询在其之前（previous）的K，满足了因果关系，代码里可以用mask实现。</p>
<ul>
<li><strong>前馈层Point-wise Feef-Forward Network</strong></li>
</ul>
<p>FFN为模型加入非线性性。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117093429227.png" alt="image-20211117093429227"></p>
<p>Residual connection和dropout正则化。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117093655881.png" alt="image-20211117093655881"></p>
<p>Layer Norm正则化。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117093809122.png" alt="image-20211117093809122"></p>
<p><strong>4.预测层</strong></p>
<p>常规的点积计算每个物品的得分。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117094112160.png" alt="image-20211117094112160"></p>
<p><strong>5.模型训练</strong></p>
<p>取物品序列$\widetilde{S_{|S^u|}}=(S^u_1,S^u_2,…,S^u_{|S^u|})$和对应的时间序列$\widetilde{T_{|T^u|}}=(T^u_1,T^u_2,…,T^u_{|T^u|})$的前$|S^u|-1$项，即$\widetilde{S_{|S^u|-1}}=(S^=u_1,S^u_2,…,S^u_{|S^u|-1})$和$\widetilde{T_{|T^u|-1}}=(T^u_1,T^u_2,…,T^u_{|T^u-1|})$。通过裁剪和补长各自化成成相同长度n的两个序列$s=(s_1,s_2,…,s_n)$，和$t=(t_1,t_2,…,t_n)$。给定这两个序列，再规定对应的输出序列$o=(o_1,o_2,…,o_n)$，其中$o_i$定义为：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117095415283.png" alt="image-20211117095415283"></p>
<p>简而言之，padding项的输出为&lt;pad&gt;；$s$最后一项之前的输出（预测）为下一项；$s$最后一项的输出（预测）为$S^u_{|S^u|}$，注意$S^u_{|S^u|}$不在$s$里，因为一开始就把最后一项拿出来了。</p>
<p>loss采用进行负采样的binary cross entropy，加入了F正则项：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117100703091.png" alt="image-20211117100703091"></p>
<p>padding项也计算了loss，但是没有意义，所以实际计算时把padding项的loss mask掉。</p>
<h2 id="Experiment-amp-Table"><a href="#Experiment-amp-Table" class="headerlink" title="Experiment &amp; Table"></a>Experiment &amp; Table</h2><p><strong>1.模型表现</strong></p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117101056812.png" alt="image-20211117101056812"></p>
<ul>
<li>TiSASRec在6个数据集上达到了SOTA</li>
</ul>
<p><strong>2.Ablation Study</strong></p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117103132511.png" alt="image-20211117103132511"></p>
<ul>
<li>TiSASRec-R去掉了在K和V里去掉了position embedding（绝对位置）</li>
</ul>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117103534252.png" alt="image-20211117103534252"></p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117103553274.png" alt="image-20211117103553274"></p>
<ul>
<li>SASRec去掉了relative time interval（relation，相对时间间隔）</li>
</ul>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117114708312.png" alt="image-20211117114708312"></p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117114729528.png" alt="image-20211117114729528"></p>
<ul>
<li>结果表明保留绝对位置和相对时间间隔时model performance最好</li>
</ul>
<p><strong>3.超参数实验</strong></p>
<p><strong>A.隐向量维度d</strong></p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117104027269.png" alt="image-20211117104027269"></p>
<ul>
<li>不同模型在不同数据集（<em>除了Games和Steam，why？</em>）上选择d={10，20，30，40，50}</li>
<li>基本上所给模型在所给数据集上都是d越大越好</li>
<li>在Beauty数据集比较特殊，MARank、Caser、TransRec的表现随着d增大在变差</li>
<li>所以最后选d=50</li>
</ul>
<p><strong>B. 序列最大长度n</strong></p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117104705615.png" alt="image-20211117104705615"></p>
<ul>
<li>n越大效果越好，并且在这两个数据集上SASRec表现比TiSASRec差且更快收敛。</li>
<li>所以最后选n=50</li>
<li><em>疑问：只选了MovieLens和Amazon CD&amp;Vinyl做实验，why？SASRec论文里选MovieLens-1m做实验的时候maxlen选的可是200，且performance比TiSASRec选50时好….这篇论文maxlen选的最大才50，why？</em></li>
</ul>
<p><strong>C.最大时间间隔k</strong></p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117105425551.png" alt="image-20211117105425551"></p>
<ul>
<li>k越大意味着要训练的参数越多</li>
<li>TiSASRec整体上更稳定，TiSASRec-R当k取合适时表现最好，但当k更大时表现变差。</li>
<li><em>疑问：ml-1m上比较稳定且permformance在提升，到最大值2048。但CD&amp;Vinyl上最好表现是k=256，但论文最后选的k=512</em></li>
</ul>
<p><strong>4.个性化时间间隔实验</strong></p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117110513024.png" alt="image-20211117110513024"></p>
<ul>
<li>Method（1）直接用时间戳作为特征，Method（2）使用没缩放的时间间隔，Method（3）使用个性化（根据每个用户最小时间间隔缩放后的）的时间间隔，即论文方法。</li>
<li>注意前两个方法没有使用时间戳裁剪 timestamps clip</li>
<li>Method（3）的performence最好</li>
</ul>
<p><strong>5.可视化</strong></p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20211117111955529.png" alt="image-20211117111955529"></p>
<ul>
<li>Figure 7 表明预测时使用时间间隔产生的推荐不一样，而且好像更准确</li>
<li>Figure 8 是不同时间间隔的权重可视化<ul>
<li>小时间间隔的权重更大，说明更短期交互的物品对预测结果影响更大</li>
<li>(a)MovieLens是dense数据集，(b) CDs&amp;Vinyl是sparse数据集。左边绿的区域更大，说明dense数据集上预测需要更大范围的物品。</li>
<li>Amazon Beauty数据集没有明显的黄绿区域，说明这个数据集没有明显的序列模式，这也说明了为什么有些序列模型在该数据集上效果不是很好。</li>
</ul>
</li>
</ul>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://aclanthology.org/N18-2074/">[1]Self-Attention with Relative Position Representations</a>)</p>
]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>序列推荐</tag>
        <tag>自注意力</tag>
        <tag>TiSASRec</tag>
      </tags>
  </entry>
  <entry>
    <title>TiSASRec代码笔记</title>
    <url>/2021/11/22/TiSASRec%E4%BB%A3%E7%A0%81/</url>
    <content><![CDATA[<hr>
<p>完整的代码注释：<a href="https://github.com/Guadzilla/Paper_notebook/tree/main/TiSASRec">https://github.com/Guadzilla/Paper_notebook/tree/main/TiSASRec</a></p>
<p>论文笔记：<a href="https://guadzilla.github.io/2021/11/18/TiSASRec/">https://guadzilla.github.io/2021/11/18/TiSASRec/</a></p>
<hr>
<h2 id="squeeze-unsqueeze-repeat-expand"><a href="#squeeze-unsqueeze-repeat-expand" class="headerlink" title="squeeze, unsqueeze, repeat ,expand"></a>squeeze, unsqueeze, repeat ,expand</h2><p><strong>torch.squeeze(input,dim,*,out) —&gt;Tensor</strong></p>
<blockquote>
<p><em>squeeze：挤压，捏</em></p>
</blockquote>
<p>与unsqueeze操作相反，在指定dim处加入一维，如果dim未指定，则所有为1的维度去掉。</p>
<p><strong>torch.unsqueeze(input,dim) —&gt; Tensor</strong></p>
<blockquote>
<p><em>unsqueeze：挤压的反义词，膨胀</em></p>
</blockquote>
<p>与squeeze操作相反，返回一个新张量，在原来张量的指定dim处加入一维。</p>
<span id="more"></span>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>],		<span class="comment"># x.shape=(2,4) ,有三处可以插入维度 _,1,_,4,_</span></span><br><span class="line">                  [<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>]]) 	</span><br><span class="line"></span><br><span class="line">torch.unsqueeze(x, <span class="number">0</span>).shape			<span class="comment"># (_,2,_,4,_),在第0维度（最左边）插入1维 = (1,2,4)</span></span><br><span class="line"><span class="comment"># torch.Size([1, 2, 4])</span></span><br><span class="line"></span><br><span class="line">torch.unsqueeze(x, <span class="number">1</span>).shape			<span class="comment"># (_,2,_,4,_),在第1维度（中间的）插入1维 = (2,1,4)</span></span><br><span class="line"><span class="comment"># torch.Size([2, 1, 4])</span></span><br><span class="line"></span><br><span class="line">torch.unsqueeze(x, <span class="number">2</span>).shape			<span class="comment"># (_,2,_,4,_),在第2维度（最右边）插入1维 = (2,1,4)</span></span><br><span class="line"><span class="comment"># torch.Size([2, 4, 1])</span></span><br><span class="line"></span><br><span class="line">y = torch.unsqueeze(x, -<span class="number">1</span>).unsqueeze(-<span class="number">1</span>)		<span class="comment"># 在最后填两个为1的维度</span></span><br><span class="line">y.shape</span><br><span class="line"><span class="comment"># torch.Size([2, 4, 1, 1])		</span></span><br><span class="line">y.squeeze().shape					<span class="comment"># squeeze不指定dim，会去掉所有size=1的维度</span></span><br><span class="line"><span class="comment"># torch.Size([2, 4])</span></span><br></pre></td></tr></table></figure>

<p>*<em>torch.repeat(<em>size)</em></em></p>
<p>沿着指定的维度重复这个张量。类似numpy.tile()，地板铺（把tensor当成一块地板，按形状铺）。</p>
<p>*<em>torch.expand(<em>sizes)</em></em></p>
<p>将单个维度<strong>拓展</strong>成更大维度，和repeat不一样。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">x</span><br><span class="line"><span class="comment"># tensor([1, 2, 3])</span></span><br><span class="line">x.repeat(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line"><span class="comment"># tensor([[1, 2, 3, 1, 2, 3, 1, 2, 3],		# x作为地板，被重复铺了(2,3)次</span></span><br><span class="line"><span class="comment">#         [1, 2, 3, 1, 2, 3, 1, 2, 3]])</span></span><br><span class="line">x.expand(<span class="number">2</span>,<span class="number">3</span>)		</span><br><span class="line"><span class="comment"># tensor([[1, 2, 3],						# x被拓展成(2,3)</span></span><br><span class="line"><span class="comment">#         [1, 2, 3]])</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>实际代码：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">time_mask = time_mask.unsqueeze(-<span class="number">1</span>).repeat(self.head_num, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"><span class="comment"># 1.unsqueeze()：time_mask.shape=(batch_size,maxlen) ——&gt; (batch_size,maxlen,1)，最后一个维度填1</span></span><br><span class="line"><span class="comment"># 2.repeat():(batch_size,maxlen,1) ——&gt; (self.head_num*batch_size,maxlen,1),第一个维度乘倍数</span></span><br><span class="line">time_mask = time_mask.expand(-<span class="number">1</span>, -<span class="number">1</span>, attn_weights.shape[-<span class="number">1</span>])	<span class="comment"># 这里attn_weights.shape[-1]=maxlen</span></span><br><span class="line"><span class="comment"># 3.(self.head_num*batch_size,maxlen,1) ——&gt;(self.head_num*batch_size,maxlen,maxlen)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">attn_mask = attn_mask.unsqueeze(<span class="number">0</span>).expand(attn_weights.shape[<span class="number">0</span>], -<span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line"><span class="comment"># (maxlen,maxlen) ——&gt; (1,maxlen,maxlen) ——&gt; (batch_size,maxlen,maxlen)</span></span><br></pre></td></tr></table></figure>



<h2 id="手动多头注意力"><a href="#手动多头注意力" class="headerlink" title="手动多头注意力"></a>手动多头注意力</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TimeAwareMultiHeadAttention</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="comment"># required homebrewed mha layer for Ti/SASRec experiments</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, hidden_size, head_num, dropout_rate, dev</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(TimeAwareMultiHeadAttention, self).__init__()</span><br><span class="line">        self.Q_w = torch.nn.Linear(hidden_size, hidden_size)</span><br><span class="line">        self.K_w = torch.nn.Linear(hidden_size, hidden_size)</span><br><span class="line">        self.V_w = torch.nn.Linear(hidden_size, hidden_size)</span><br><span class="line"></span><br><span class="line">        self.dropout = torch.nn.Dropout(p=dropout_rate)</span><br><span class="line">        self.softmax = torch.nn.Softmax(dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.head_num = head_num</span><br><span class="line">        self.head_size = hidden_size // head_num</span><br><span class="line">        self.dropout_rate = dropout_rate</span><br><span class="line">        self.dev = dev</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, queries, keys, time_mask, attn_mask, time_matrix_K, time_matrix_V, abs_pos_K, abs_pos_V</span>):</span></span><br><span class="line">        <span class="comment"># time_mask: padding item的mask,     attn_mask: 为了causality的mask,下三角</span></span><br><span class="line">        Q, K, V = self.Q_w(queries), self.K_w(keys), self.V_w(keys)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># head dim * batch dim for parallelization (h*N, T, C/h)</span></span><br><span class="line">        <span class="comment"># 即(batch_size, maxlen, hidden_units) ----&gt; (batch_size*3, maxlen, hidden_units/3)</span></span><br><span class="line">        <span class="comment">#   (batch_size, maxlen, maxlen, hidden_units) ----&gt; (batch_size*3, maxlen, maxlen, hidden_units/3)</span></span><br><span class="line">        Q_ = torch.cat(torch.split(Q, self.head_size, dim=<span class="number">2</span>), dim=<span class="number">0</span>)</span><br><span class="line">        K_ = torch.cat(torch.split(K, self.head_size, dim=<span class="number">2</span>), dim=<span class="number">0</span>)</span><br><span class="line">        V_ = torch.cat(torch.split(V, self.head_size, dim=<span class="number">2</span>), dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        time_matrix_K_ = torch.cat(torch.split(time_matrix_K, self.head_size, dim=<span class="number">3</span>), dim=<span class="number">0</span>)</span><br><span class="line">        time_matrix_V_ = torch.cat(torch.split(time_matrix_V, self.head_size, dim=<span class="number">3</span>), dim=<span class="number">0</span>)</span><br><span class="line">        abs_pos_K_ = torch.cat(torch.split(abs_pos_K, self.head_size, dim=<span class="number">2</span>), dim=<span class="number">0</span>)</span><br><span class="line">        abs_pos_V_ = torch.cat(torch.split(abs_pos_V, self.head_size, dim=<span class="number">2</span>), dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># batched channel wise matmul to gen attention weights  ---公式（8）</span></span><br><span class="line">        attn_weights = Q_.matmul(torch.transpose(K_, <span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">        attn_weights += Q_.matmul(torch.transpose(abs_pos_K_, <span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">        attn_weights += time_matrix_K_.matmul(Q_.unsqueeze(-<span class="number">1</span>)).squeeze(-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># seq length adaptive scaling   ---公式（8）</span></span><br><span class="line">        attn_weights = attn_weights / (K_.shape[-<span class="number">1</span>] ** <span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># key masking, -2^32 lead to leaking, inf lead to nan</span></span><br><span class="line">        <span class="comment"># 0 * inf = nan, then reduce_sum([nan,...]) = nan</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># time_mask = time_mask.unsqueeze(-1).expand(attn_weights.shape[0], -1, attn_weights.shape[-1])</span></span><br><span class="line">        <span class="comment"># 会报错，必须按下面的1.2.3.</span></span><br><span class="line">        time_mask = time_mask.unsqueeze(-<span class="number">1</span>).repeat(self.head_num, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 1.unsqueeze()：time_mask.shape=(batch_size,maxlen) ——&gt; (batch_size,maxlen,1),最后一个维度填1</span></span><br><span class="line">        <span class="comment"># 2.repeat():(batch_size,maxlen,1) ——&gt; (self.head_num*batch_size,maxlen,1),第一个维度乘倍数</span></span><br><span class="line">        time_mask = time_mask.expand(-<span class="number">1</span>, -<span class="number">1</span>, attn_weights.shape[-<span class="number">1</span>])</span><br><span class="line">        <span class="comment"># 3.(self.head_num*batch_size,maxlen,1) ——&gt;(self.head_num*batch_size,maxlen,maxlen)</span></span><br><span class="line">        <span class="comment"># tips：attn_weights= (B,maxlen,maxlen),每个batch中size=(maxlen,maxlen)，每行表示某个item对其它所有item的atten矩阵</span></span><br><span class="line">        <span class="comment">#      time_mask是对padding的item做mask,本来是(B,maxlen,1),每个batch中size=(maxlen,1)</span></span><br><span class="line">        <span class="comment">#      expand成(B,maxlen,maxlen)才能把attn里padding的物品，即对应行都mask掉</span></span><br><span class="line"></span><br><span class="line">        attn_mask = attn_mask.unsqueeze(<span class="number">0</span>).expand(attn_weights.shape[<span class="number">0</span>], -<span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># (maxlen,maxlen) ——&gt; (1,maxlen,maxlen) ——&gt; (batch_size,maxlen,maxlen)</span></span><br><span class="line">        <span class="comment"># padding取负无穷是因为底下要用softmax，以e为底的负无穷接近0</span></span><br><span class="line">        paddings = torch.ones(attn_weights.shape) *  (-<span class="number">2</span>**<span class="number">32</span>+<span class="number">1</span>) <span class="comment"># -1e23 # float(&#x27;-inf&#x27;),</span></span><br><span class="line">        paddings = paddings.to(self.dev)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 这两步一起为了mask掉不用的attention计算，第一步是mask掉padding的items，第二是为了因果关系mask掉afterwards的items</span></span><br><span class="line">        attn_weights = torch.where(time_mask, paddings, attn_weights) <span class="comment"># True:pick padding</span></span><br><span class="line">        attn_weights = torch.where(attn_mask, paddings, attn_weights) <span class="comment"># enforcing causality</span></span><br><span class="line"></span><br><span class="line">        attn_weights = self.softmax(attn_weights)   <span class="comment"># ---公式（7）</span></span><br><span class="line">        attn_weights = self.dropout(attn_weights)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># ---公式（6），把alpha放进去乘了</span></span><br><span class="line">        outputs = attn_weights.matmul(V_)</span><br><span class="line">        outputs += attn_weights.matmul(abs_pos_V_)</span><br><span class="line">        outputs += attn_weights.unsqueeze(<span class="number">2</span>).matmul(time_matrix_V_).reshape(outputs.shape)<span class="comment">#.squeeze(2)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># (num_head * N, T, C / num_head) -&gt; (N, T, C)</span></span><br><span class="line">        outputs = torch.cat(torch.split(outputs, Q.shape[<span class="number">0</span>], dim=<span class="number">0</span>), dim=<span class="number">2</span>) <span class="comment"># div batch_size</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>代码阅读</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>TiSASRec</tag>
      </tags>
  </entry>
  <entry>
    <title>（待更新）推荐系统：经典算法——协同过滤（Collebrative Filtering）</title>
    <url>/2021/11/23/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4/</url>
    <content><![CDATA[<h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p>经典Movielens数据集</p>
<blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">All ratings are contained in the file &quot;ratings.dat&quot; and are in the</span><br><span class="line">following format:</span><br><span class="line"></span><br><span class="line">UserID::MovieID::Rating::Timestamp</span><br><span class="line"></span><br><span class="line">- UserIDs range between 1 and 6040 </span><br><span class="line">- MovieIDs range between 1 and 3952</span><br><span class="line">- Ratings are made on a 5-star scale (whole-star ratings only)</span><br><span class="line">- Timestamp is represented in seconds since the epoch as returned by time(2)</span><br><span class="line">- Each user has at least 20 ratings</span><br></pre></td></tr></table></figure>
</blockquote>
<h2 id="实验设计"><a href="#实验设计" class="headerlink" title="实验设计"></a>实验设计</h2><p>采用K-fold交叉验证，将用户行为数据均匀分成K份，其中一份作为测试集，K-1份作为训练集。协同过滤算法只考虑物品/用户的共现关系，所以用户序列都用集合表示。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 划分数据集</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">SplitData</span>(<span class="params">data, M, k, seed</span>):</span></span><br><span class="line">    test = []</span><br><span class="line">    train = []</span><br><span class="line">    random.seed(seed)</span><br><span class="line">    <span class="keyword">for</span> user, item <span class="keyword">in</span> data:</span><br><span class="line">        <span class="keyword">if</span> random.randint(<span class="number">0</span>, M) == k:</span><br><span class="line">            test.append([user, item])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            train.append([user, item])</span><br><span class="line">    train_ = defaultdict(<span class="built_in">set</span>)</span><br><span class="line">    test_ = defaultdict(<span class="built_in">set</span>)</span><br><span class="line">    <span class="keyword">for</span> user, item <span class="keyword">in</span> train:</span><br><span class="line">        train_[user].add(item)</span><br><span class="line">    <span class="keyword">for</span> user, item <span class="keyword">in</span> test:</span><br><span class="line">        test_[user].add(item)</span><br><span class="line">    <span class="keyword">return</span> train_, test_</span><br></pre></td></tr></table></figure>

<h2 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h2><p>召回率Recall，准确率Precision，覆盖率Coverage，新颖度Popularity。</p>
<span id="more"></span>

<p>召回率Recall：正确推荐的商品占所有应该推荐的商品的比例，即应该推荐的推荐了多少。公式描述：对用户u推荐N个物品（$R(u)$），令用户在测试集上喜欢的物品集合为$T(u)$，则<br>$$<br>Recall=\frac{\sum_u|R(u) \cap T(u)|}{\sum_u |T(u)|}<br>$$<br>准确率Precision：正确推荐的商品占推荐的商品列表的比例，即有多少推荐对了。公式描述：<br>$$<br>Precision=\frac{\sum_u|R(u) \cap T(u)|}{\sum_u |R(u)|}<br>$$<br>覆盖率Coverage：推荐的商品占所有商品的比例，即推荐的商品覆盖了多少所有商品。反映发掘长尾的能力。<br>$$<br>Coverage = \frac{\bigcup_u R(u)}{|I|} \ \  , \ \bigcup:并集<br>$$<br>新颖度Popularity：刻画推荐物品的平均流行度，平均流行度（Popularity）越高，新颖度越低。$Popularity(x)$定义为$x$在所有用户序列中出现的次数，出现次数越多，流行度越高。<br>$$<br>Popularity= \sum _u \sum _ { i \in R(u) } \log (Popularity(i)+1)<br>$$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 评价指标:召回率、准确率</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Metric</span>(<span class="params">train, test, N, all_recommend_list</span>):</span>  <span class="comment"># N:推荐N个物品</span></span><br><span class="line">    hit = <span class="number">0</span></span><br><span class="line">    recall_all = <span class="number">0</span>      <span class="comment"># recall 的分母</span></span><br><span class="line">    precision_all = <span class="number">0</span>   <span class="comment"># precision 的分母</span></span><br><span class="line">    <span class="keyword">for</span> user <span class="keyword">in</span> train.keys():</span><br><span class="line">        tu = test[user]</span><br><span class="line">        rank = all_recommend_list[user][<span class="number">0</span>:N]</span><br><span class="line">        <span class="keyword">for</span> item, pui <span class="keyword">in</span> rank:</span><br><span class="line">            <span class="keyword">if</span> item <span class="keyword">in</span> tu:</span><br><span class="line">                hit += <span class="number">1</span></span><br><span class="line">        recall_all += <span class="built_in">len</span>(tu)</span><br><span class="line">        precision_all += N</span><br><span class="line">    recall = hit / (recall_all * <span class="number">1.0</span>)</span><br><span class="line">    precision = hit / (precision_all * <span class="number">1.0</span>)</span><br><span class="line">    <span class="keyword">return</span> recall, precision</span><br><span class="line"></span><br><span class="line"><span class="comment"># 评价指标：覆盖率</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Coverage</span>(<span class="params">train, test, N, all_recommend_list</span>):</span>  <span class="comment"># N:推荐N个物品</span></span><br><span class="line">    recommend_items = <span class="built_in">set</span>()</span><br><span class="line">    all_items = <span class="built_in">set</span>()</span><br><span class="line">    <span class="keyword">for</span> user <span class="keyword">in</span> train.keys():</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> train[user]:</span><br><span class="line">            all_items.add(item)</span><br><span class="line">        <span class="keyword">for</span> item, pui <span class="keyword">in</span> rank:</span><br><span class="line">            recommend_items.add(item)</span><br><span class="line">    coverage = <span class="built_in">len</span>(recommend_items) / (<span class="built_in">len</span>(all_items) * <span class="number">1.0</span>)</span><br><span class="line">    <span class="keyword">return</span> coverage</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 评价指标：新颖度</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Popularity</span>(<span class="params">train, test, N, recommend_res</span>):</span>	<span class="comment"># N:推荐N个物品</span></span><br><span class="line">    item_popularity = <span class="built_in">dict</span>()</span><br><span class="line">    <span class="keyword">for</span> user, items <span class="keyword">in</span> train.items():</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">            <span class="keyword">if</span> item <span class="keyword">not</span> <span class="keyword">in</span> item_popularity:</span><br><span class="line">                item_popularity[item] = <span class="number">0</span></span><br><span class="line">            item_popularity[item] += <span class="number">1</span></span><br><span class="line">    popularity = <span class="number">0</span></span><br><span class="line">    n = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> user <span class="keyword">in</span> train.keys():</span><br><span class="line">        rank = recommend_res[user][<span class="number">0</span>:N]</span><br><span class="line">        <span class="keyword">for</span> item, pui <span class="keyword">in</span> rank:</span><br><span class="line">            popularity += math.log(<span class="number">1</span> + item_popularity[item])</span><br><span class="line">            n += <span class="number">1</span></span><br><span class="line">    popularity /= n * <span class="number">1.0</span></span><br><span class="line">    <span class="keyword">return</span> popularity</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>推荐系统实战</category>
      </categories>
  </entry>
  <entry>
    <title>模板</title>
    <url>/2021/10/17/%E6%A8%A1%E6%9D%BF/</url>
    <content><![CDATA[<hr>
<p>原paper：</p>
<p>源码解读：</p>
<hr>
<p>中译：</p>
<p>总结：</p>
<hr>
<span id="more"></span>

<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li>question作者想解决什么问题？ </li>
</ul>
<ul>
<li>method作者通过什么理论/模型来解决这个问题？</li>
</ul>
<ul>
<li>answer作者给出的答案是什么？</li>
</ul>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li>why作者为什么研究这个课题？    </li>
</ul>
<ul>
<li>how当前研究到了哪一阶段？</li>
</ul>
<ul>
<li>what作者基于什么样的假设（看不懂最后去查）？</li>
</ul>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><ul>
<li>优点 </li>
</ul>
<ul>
<li>缺点</li>
</ul>
<ul>
<li>展望</li>
</ul>
<h2 id="Dataset-amp-Metric"><a href="#Dataset-amp-Metric" class="headerlink" title="Dataset &amp; Metric"></a>Dataset &amp; Metric</h2><ul>
<li>数据来源 </li>
</ul>
<ul>
<li>重要指标 </li>
</ul>
<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><ul>
<li></li>
<li></li>
</ul>
<h2 id="Experiment-amp-Table"><a href="#Experiment-amp-Table" class="headerlink" title="Experiment &amp; Table"></a>Experiment &amp; Table</h2><ul>
<li></li>
<li></li>
</ul>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><h2 id="一些备注"><a href="#一些备注" class="headerlink" title="一些备注"></a>一些备注</h2><p>数学公式不能正确显示的情况：<br>1.<br>$$<br>\mathit{L}=\mathit{L}<em>{cr} + \lambda_1\mathit{L}</em>{in}+\lambda_2||\Theta||^2_2<br>$$</p>
<p>在每个下划线 “ _ ” 前后各加一个空格就好了</p>
<p>$$<br>\mathit{L}=\mathit{L} _ {cr} + \lambda_1\mathit{L} _ {in}+\lambda_2||\Theta||^2 _ 2<br>$$</p>
<p>2.</p>
<p><img src="https://gitee.com/Guadzilla/img-hosting/raw/master/image-20211118185724771.png" alt="image-20211118185724771"></p>
]]></content>
  </entry>
  <entry>
    <title>推荐系统基础1</title>
    <url>/2022/04/22/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%801/</url>
    <content><![CDATA[<hr>
<h1 id="任务1：推荐系统基础"><a href="#任务1：推荐系统基础" class="headerlink" title="任务1：推荐系统基础"></a>任务1：推荐系统基础</h1><ul>
<li>阅读推荐系统在工业落地的链接：<ul>
<li><a href="https://mp.weixin.qq.com/s/WXcfdzz7vts9UYBVxWs3AA">推荐系统整体架构及算法流程详解</a></li>
<li><a href="https://tech.meituan.com/2017/03/24/travel-recsys.html">美团旅游推荐系统的演进</a></li>
<li><a href="https://www.alibabacloud.com/zh/product/airec">阿里智能推荐AIRec</a></li>
</ul>
</li>
<li>思考 &amp; 回答以下问题，并将回答记录到博客<ul>
<li>推荐系统与常见的结构化问题的区别是什么？</li>
<li>如何评价推荐系统「推荐」的准不准？</li>
<li>推荐系统一般分为召回 &amp; 排序，为什么这样划分？</li>
</ul>
</li>
</ul>
<p>代码地址： <a href="https://github.com/Guadzilla/Basics-of-Recsys">https://github.com/Guadzilla/Basics-of-Recsys</a></p>
<span id="more"></span>
<h2 id="推荐系统与常见的结构化问题的区别是什么？"><a href="#推荐系统与常见的结构化问题的区别是什么？" class="headerlink" title="推荐系统与常见的结构化问题的区别是什么？"></a>推荐系统与常见的结构化问题的区别是什么？</h2><p>结构化数据即表格数据（tabular data），绝大多数数据都是表格数据。</p>
<p>推荐系统有user信息，item信息，这些数据虽然都是结构化的，但是推荐系统涉及到user和item的交互，所以不仅仅是一条结构化数据预测一个分类or一个数值那么简单，还需要从交互中抽象出用户兴趣，例如将用户交互建模为序列，这就不是结构化问题了。</p>
<h2 id="如何评价推荐系统「推荐」的准不准？"><a href="#如何评价推荐系统「推荐」的准不准？" class="headerlink" title="如何评价推荐系统「推荐」的准不准？"></a>如何评价推荐系统「推荐」的准不准？</h2><p>常用的评价指标有：召回率Recall，准确率Precision，覆盖率Coverage，新颖度Popularity。</p>
<p>召回率Recall：正确推荐的商品占所有应该推荐的商品的比例，即应该推荐的推荐了多少。公式描述：对用户u推荐N个物品（$R(u)$），令用户在测试集上喜欢的物品集合为$T(u)$，则</p>
<script type="math/tex; mode=display">
Recall=\frac{\sum_u|R(u) \cap T(u)|}{\sum_u |T(u)|}</script><p>准确率Precision：正确推荐的商品占推荐的商品列表的比例，即有多少推荐对了。公式描述：</p>
<script type="math/tex; mode=display">
Precision=\frac{\sum_u|R(u) \cap T(u)|}{\sum_u |R(u)|}</script><p>覆盖率Coverage：推荐的商品占所有商品的比例，即推荐的商品覆盖了多少所有商品。反映发掘长尾的能力。</p>
<script type="math/tex; mode=display">
Coverage = \frac{\bigcup_u R(u)}{|I|} \ \  , \ \bigcup:并集</script><p>新颖度Popularity：刻画推荐物品的平均流行度，平均流行度（Popularity）越高，新颖度越低。$Popularity(x)$定义为$x$在所有用户序列中出现的次数，出现次数越多，流行度越高。</p>
<script type="math/tex; mode=display">
Popularity= \sum _u \sum _ { i \in R(u) } \log (Popularity(i)+1)</script><p>AUC曲线：AUC（Area Under Curve），ROC曲线下与坐标轴围成的面积。在讲AUC前需要理解混淆矩阵，召回率，精确率，ROC曲线等概念。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/v2-a253b01cf7f141b9ad11eefdf3cf58d3_1440w.jpg" alt="img"></p>
<p>根据混淆矩阵的定义，以另一种形式定义召回率和精确率：</p>
<script type="math/tex; mode=display">
Recall = \frac{TP}{TP+FN} \\
Precision = \frac{TP}{TP+FP}</script><p>ROC曲线的横坐标为假阳性率（False Positive Rate, FPR），$FPR=\frac{FP}{FP+TN}$，N是真实负样本的个数， FP是N个负样本中被分类器预测为正样本的个数。<strong>FPRate的意义是所有真实类别为0的样本中，预测类别为1的比例。</strong></p>
<p>纵坐标为真阳性率（True Positive Rate, TPR），$TPR=\frac{TP}{TP+FN}$，P是真实正样本的个数，TP是P个正样本中被分类器预测为正样本的个数。<strong>TPRate的意义是所有真实类别为1的样本中，预测类别为1的比例。</strong></p>
<p><img src="https://camo.githubusercontent.com/07a924ef2229334f903f1ba3e5cd17115a16159dcb1756cda93232b3cf998c0d/687474703a2f2f72796c756f2e6f73732d636e2d6368656e6764752e616c6979756e63732e636f6d2f4a6176616175632e706e67" alt="img"></p>
<p>AUC的计算方法同时考虑了分类器对于正例和负例的分类能力，在样本不平衡的情况下，依然能够对分类器作出合理的评价。对ROC更细致的解释：<a href="https://www.zhihu.com/question/39840928/answer/241440370">如何理解机器学习和统计中的AUC？ - 知乎 (zhihu.com)</a></p>
<p>下面是部分评价指标的代码实现。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 评价指标:召回率、准确率</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Metric</span>(<span class="params">train, test, N, all_recommend_list</span>):</span>  <span class="comment"># N:推荐N个物品</span></span><br><span class="line">    hit = <span class="number">0</span></span><br><span class="line">    recall_all = <span class="number">0</span>      <span class="comment"># recall 的分母</span></span><br><span class="line">    precision_all = <span class="number">0</span>   <span class="comment"># precision 的分母</span></span><br><span class="line">    <span class="keyword">for</span> user <span class="keyword">in</span> train.keys():</span><br><span class="line">        tu = test[user]</span><br><span class="line">        rank = all_recommend_list[user][<span class="number">0</span>:N]</span><br><span class="line">        <span class="keyword">for</span> item, pui <span class="keyword">in</span> rank:</span><br><span class="line">            <span class="keyword">if</span> item <span class="keyword">in</span> tu:</span><br><span class="line">                hit += <span class="number">1</span></span><br><span class="line">        recall_all += <span class="built_in">len</span>(tu)</span><br><span class="line">        precision_all += N</span><br><span class="line">    recall = hit / (recall_all * <span class="number">1.0</span>)</span><br><span class="line">    precision = hit / (precision_all * <span class="number">1.0</span>)</span><br><span class="line">    <span class="keyword">return</span> recall, precision</span><br><span class="line"></span><br><span class="line"><span class="comment"># 评价指标：覆盖率</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Coverage</span>(<span class="params">train, test, N, all_recommend_list</span>):</span>  <span class="comment"># N:推荐N个物品</span></span><br><span class="line">    recommend_items = <span class="built_in">set</span>()</span><br><span class="line">    all_items = <span class="built_in">set</span>()</span><br><span class="line">    <span class="keyword">for</span> user <span class="keyword">in</span> train.keys():</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> train[user]:</span><br><span class="line">            all_items.add(item)</span><br><span class="line">        <span class="keyword">for</span> item, pui <span class="keyword">in</span> rank:</span><br><span class="line">            recommend_items.add(item)</span><br><span class="line">    coverage = <span class="built_in">len</span>(recommend_items) / (<span class="built_in">len</span>(all_items) * <span class="number">1.0</span>)</span><br><span class="line">    <span class="keyword">return</span> coverage</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 评价指标：新颖度</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Popularity</span>(<span class="params">train, test, N, recommend_res</span>):</span>	<span class="comment"># N:推荐N个物品</span></span><br><span class="line">    item_popularity = <span class="built_in">dict</span>()</span><br><span class="line">    <span class="keyword">for</span> user, items <span class="keyword">in</span> train.items():</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">            <span class="keyword">if</span> item <span class="keyword">not</span> <span class="keyword">in</span> item_popularity:</span><br><span class="line">                item_popularity[item] = <span class="number">0</span></span><br><span class="line">            item_popularity[item] += <span class="number">1</span></span><br><span class="line">    popularity = <span class="number">0</span></span><br><span class="line">    n = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> user <span class="keyword">in</span> train.keys():</span><br><span class="line">        rank = recommend_res[user][<span class="number">0</span>:N]</span><br><span class="line">        <span class="keyword">for</span> item, pui <span class="keyword">in</span> rank:</span><br><span class="line">            popularity += math.log(<span class="number">1</span> + item_popularity[item])</span><br><span class="line">            n += <span class="number">1</span></span><br><span class="line">    popularity /= n * <span class="number">1.0</span></span><br><span class="line">    <span class="keyword">return</span> popularity</span><br></pre></td></tr></table></figure>
<h2 id="推荐系统一般分为召回-amp-精排，为什么这样划分？"><a href="#推荐系统一般分为召回-amp-精排，为什么这样划分？" class="headerlink" title="推荐系统一般分为召回 &amp; 精排，为什么这样划分？"></a>推荐系统一般分为召回 &amp; 精排，为什么这样划分？</h2><blockquote>
<p>物品集量级非常大，召回先选出一部分候选物品，再对这一部分候选物品做精排，计算开销相比于对所有物品做精排大大降低了。</p>
</blockquote>
<p>相关知识点：召回、排序（精排）。</p>
<h3 id="召回"><a href="#召回" class="headerlink" title="召回"></a>召回</h3><p>从海量的物品中，先筛选出一小部分物品作为推荐的候选集。</p>
<h4 id="召回的目的"><a href="#召回的目的" class="headerlink" title="召回的目的"></a>召回的目的</h4><p>当用户和物品量比较大时，如果直接精排（计算预测得分）复杂度会非常高。计算预测得分通常是计算用户和物品的向量内积，假设 user 和 item 的 embedding 维度都是 D ，用户数为 M ，物品数为 N ，那么计算这个得分的复杂度就是 $O(D^2) *O(MN)$。当 M 和 N 都是百万量级、亿量级时，计算开销会非常大。</p>
<p>如果可以先从海量的物品中，先筛选出一小部分用户最可能喜欢的物品（召回），例如先选出 N/100 的物品，那么复杂度就是 $\frac{O(D^2) *O(MN)}{100}$ ，降低为原来的一百分之一，计算效率更高了。实际场景中，做热销召回的量级可能是百级，这样一来从百万量级的物品数降低到百量级的物品数，计算开销大大降低！另一方面，大量内容中真正的精品只是少数，对所有内容都计算将非常的低效，会浪费大量资源和时间。</p>
<h4 id="召回的重要性"><a href="#召回的重要性" class="headerlink" title="召回的重要性"></a>召回的重要性</h4><p>虽然精排模型一直是优化的重点，但召回模型也非常的重要，因为如果召回的内容不对，怎么精排都是错误的。</p>
<h4 id="召回的方法"><a href="#召回的方法" class="headerlink" title="召回的方法"></a>召回的方法</h4><ol>
<li>热销召回：将一段时间内的热门内容召回。</li>
<li>协同召回：基于用户与用户行为的相似性推荐，可以很好的突破一定的限制，发现用户潜在的兴趣偏好。</li>
<li>标签召回：根据每个用户的行为，构建标签，并根据标签召回内容。</li>
<li>时间召回：将一段时间内最新的内容召回，在新闻视频等有时效性的领域常用。是常见的几种召回方法。</li>
</ol>
<h4 id="多路召回"><a href="#多路召回" class="headerlink" title="多路召回"></a>多路召回</h4><p>一开始我们可能有成千上万的物品，首先要由召回（也叫触发，recall）来挖掘出原则上任何用户有可能感兴趣的东西。这个环节是入口。有时候，单独的召回可能难以做到照顾所有方面，这个时候就需要多路召回。所谓的“多路召回”策略，就是指采用不同的策略、特征或简单模型，分别召回一部分候选集，然后把候选集混合在一起供后续排序模型使用。下图只是一个多路召回的例子，也就是说可以使用多种不同的策略来获取用户排序的候选商品集合，<strong>而具体使用哪些召回策略其实是与业务强相关的</strong>，针对不同的任务就会有对于该业务真实场景下需要考虑的召回规则。例如视频推荐，召回规则可以是“热门视频”、“导演召回”、“演员召回”、“最近上映“、”流行趋势“、”类型召回“等等。</p>
<p><img src="https://camo.githubusercontent.com/5194f61aac70bfec14ede3fa6b27aed0670f2cd59b5e9ad688f1f526a4c5f658/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303230303731373230313431313336372e706e67237069635f63656e746572" alt="img" style="zoom:67%;" /></p>
<h4 id="Embedding召回"><a href="#Embedding召回" class="headerlink" title="Embedding召回"></a>Embedding召回</h4><h3 id="精排"><a href="#精排" class="headerlink" title="精排"></a>精排</h3><p>排序负责将多个召回策略的结果进行个性化排序。</p>
<h4 id="精排的重要性"><a href="#精排的重要性" class="headerlink" title="精排的重要性"></a>精排的重要性</h4><p>精排是最纯粹的排序，也是最纯粹的机器学习模块。它的目标只有一个，就是<strong>根据手头所有的信息输出最准</strong>的预测。精排一直是优化的重点。召回的物品中，筛选出用户最感兴趣的物品，进一步做出个性化排序，才最终达到推荐的目的。</p>
<h4 id="精排模型"><a href="#精排模型" class="headerlink" title="精排模型"></a>精排模型</h4><p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/icTMNdGHpfJYqcAFSwiaWKjeqTweM9aJrNKqZVvMn2GZvoDTnPHjYMVywvGicII8P9d4nMjib5Jia8kGlDbicibTGSPlQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片" style="zoom:67%;" /></p>
]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>推荐系统基础</tag>
      </tags>
  </entry>
  <entry>
    <title>推荐系统基础2</title>
    <url>/2022/04/22/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%802/</url>
    <content><![CDATA[<hr>
<h1 id="任务2：Movielens介绍"><a href="#任务2：Movielens介绍" class="headerlink" title="任务2：Movielens介绍"></a>任务2：Movielens介绍</h1><ul>
<li>下载并读取Movielens 1M数据集（用户、电影、评分）</li>
<li>统计如下指标：<ul>
<li>总共包含多少用户？</li>
<li>总共包含多个电影？</li>
<li>平均每个用户对多少个电影进行了评分？</li>
<li>每部电影 &amp; 每个用户的平均评分是？</li>
</ul>
</li>
<li>如果你来进行划分数据集为训练和验证，你会如何划分？</li>
</ul>
<p>代码地址： <a href="https://github.com/Guadzilla/Basics-of-Recsys">https://github.com/Guadzilla/Basics-of-Recsys</a></p>
<span id="more"></span>
<h2 id="统计指标"><a href="#统计指标" class="headerlink" title="统计指标"></a>统计指标</h2><ul>
<li>总共包含 6040 个用户</li>
<li>总共包含 3883 部电影</li>
<li>平均每个用户对 165.6 部电影进行评分</li>
</ul>
<p>其余细节见 notebook 。</p>
<h2 id="划分数据集"><a href="#划分数据集" class="headerlink" title="划分数据集"></a>划分数据集</h2><p>参考项亮《推荐系统实践》，将用户行为数据集按照均匀分布随机分成 K 份，挑选一份作为测试集，剩下的 K-1 份作为训练集，进行 K 次实验，然后将 K 次评测指标的平均值作为最终评测指标 （即 K-fold 交叉验证）。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">SplitData</span>(<span class="params">data, K, i, seed</span>):</span></span><br><span class="line">    test = []</span><br><span class="line">    train = []</span><br><span class="line">    random.seed(seed)</span><br><span class="line">    <span class="keyword">for</span> user, item <span class="keyword">in</span> data:</span><br><span class="line">        <span class="keyword">if</span> random.randint(<span class="number">0</span>,K)==i:</span><br><span class="line">            test.append([user,item])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            train.append([user,item])</span><br><span class="line">    <span class="keyword">return</span> train,test</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>推荐系统基础</tag>
      </tags>
  </entry>
  <entry>
    <title>推荐系统基础3</title>
    <url>/2022/04/22/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%803/</url>
    <content><![CDATA[<hr>
<h1 id="任务3：协同过滤基础"><a href="#任务3：协同过滤基础" class="headerlink" title="任务3：协同过滤基础"></a>任务3：协同过滤基础</h1><ul>
<li><a href="https://github.com/datawhalechina/fun-rec/blob/master/docs/第一章 推荐系统基础/1.1 基础推荐算法/1.1.2 协同过滤.md">阅读协同过滤教程</a></li>
<li>编写代码计算两个用户的相似度</li>
<li>编写代码计算两个物品的相似度</li>
</ul>
<p>代码地址： <a href="https://github.com/Guadzilla/Basics-of-Recsys">https://github.com/Guadzilla/Basics-of-Recsys</a></p>
<span id="more"></span>
<h2 id="协同过滤算法"><a href="#协同过滤算法" class="headerlink" title="协同过滤算法"></a>协同过滤算法</h2><p>协同过滤（Collaborative Filtering）推荐算法是最经典、最常用的推荐算法。</p>
<p>所谓协同过滤， 基本思想是<strong>根据用户之前的喜好</strong>以及<strong>其他兴趣相近的用户的选择</strong>来给用户推荐物品(基于对用户历史行为数据的挖掘发现用户的喜好偏向， 并预测用户可能喜好的产品进行推荐)，<strong>一般是仅仅基于用户的行为数据（评价、购买、下载等）, 而不依赖于项的任何附加信息（物品自身特征）或者用户的任何附加信息（年龄， 性别等）</strong>。目前应用比较广泛的协同过滤算法是基于邻域的方法， 而这种方法主要有下面两种算法：</p>
<ul>
<li><strong>基于用户的协同过滤算法(UserCF)</strong>: 给用户推荐和他兴趣相似的其他用户喜欢的产品</li>
<li><strong>基于物品的协同过滤算法(ItemCF)</strong>: 给用户推荐和他之前喜欢的物品相似的物品</li>
</ul>
<p>不管是UserCF还是ItemCF算法， 非常重要的步骤之一就是计算用户和用户或者物品和物品之间的<strong>相似度</strong>， 所以下面先整理常用的相似性度量方法， 然后再对每个算法的具体细节进行展开。</p>
<h2 id="相似性度量方法"><a href="#相似性度量方法" class="headerlink" title="相似性度量方法"></a>相似性度量方法</h2><h4 id="杰卡德相似系数"><a href="#杰卡德相似系数" class="headerlink" title="杰卡德相似系数"></a><strong>杰卡德相似系数</strong></h4><p>杰卡德(Jaccard)相似系数是衡量两个<strong>集合的相似度</strong>一种指标。两个用户$u$和$v$交互商品交集的数量占这两个用户交互商品并集的数量的比例，称为两个集合的杰卡德相似系数，用符号$sim_{uv}$表示，其中$N(u),N(v)$分别表示用户$u$和用户$v$交互商品的集合。 <script type="math/tex">sim_{uv}=\frac{|N(u) \cap N(v)|}{\sqrt{|N(u)| \cup|N(v)|}}</script> 由于杰卡德相似系数一般无法反映具体用户的评分喜好信息， 所以常用来评估用户<strong>是否</strong>会对某商品进行打分， 而不是预估用户会对某商品打多少分。</p>
<h4 id="余弦相似度"><a href="#余弦相似度" class="headerlink" title="余弦相似度"></a><strong>余弦相似度</strong></h4><p>余弦相似度(cosine similarity) 衡量了两个向量的夹角，夹角越小越相似。首先从集合的角度描述余弦相似度，相比于Jaccard公式来说就是分母有差异，不是两个用户交互商品的并集的数量，而是两个用户分别交互的商品数量的乘积，公式如下：</p>
<script type="math/tex; mode=display">
sim_{uv}=\frac{|N(u) \cap N(v)|}{\sqrt{|N(u)|\cdot|N(v)|}}</script><p>从向量的角度进行描述，令矩阵$A$为用户-商品交互矩阵(因为是TopN推荐并不需要用户对物品的评分，只需要知道用户对商品是否有交互就行)，即矩阵的每一行表示一个用户对所有商品的交互情况，有交互的商品值为1没有交互的商品值为0，矩阵的列表示所有商品。若用户和商品数量分别为$m,n$的话，交互矩阵$A$就是一个$m$行$n$列的矩阵。此时用户的相似度可以表示为(其中$u\cdot v$指的是向量点积)： </p>
<script type="math/tex; mode=display">
sim_{uv} = cos(u,v) =\frac{u\cdot v}{|u|\cdot |v|}</script><p>上述用户-商品交互矩阵在现实情况下是非常的稀疏了，为了避免存储这么大的稀疏矩阵，在计算用户相似度的时候一般会采用集合的方式进行计算。理论上向量之间的相似度计算公式都可以用来计算用户之间的相似度，但是会根据实际的情况选择不同的用户相似度度量方法。</p>
<p>这个在具体实现的时候， 可以使用<code>cosine_similarity</code>进行实现：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics.pairwise <span class="keyword">import</span> cosine_similarity</span><br><span class="line">i = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">j = [<span class="number">1</span>, <span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0</span>]</span><br><span class="line">cosine_similarity([i, j])</span><br></pre></td></tr></table></figure>
<h4 id="皮尔逊相关系数"><a href="#皮尔逊相关系数" class="headerlink" title="皮尔逊相关系数"></a>皮尔逊相关系数</h4><p>皮尔逊相关系数的公式与余弦相似度的计算公式非常的类似，首先对于上述的余弦相似度的计算公式写成求和的形式 ：</p>
<script type="math/tex; mode=display">
sim_{uv} = \frac{\sum_i r_{ui}·r_{vi}}{\sqrt{\sum_i r_{ui}^2}\sqrt{\sum_i r_{vi}^2}}</script><p>其中$r_{ui},r_{vi}$分别表示用户$u$和用户$v$对商品$i$是否有交互(或者具体的评分值)。</p>
<p>如下是皮尔逊相关系数计算公式：</p>
<script type="math/tex; mode=display">
sim(u,v)=\frac{\sum_{i\in I}(r_{ui}-\bar r_u)(r_{vi}-\bar r_v)}{\sqrt{\sum_{i\in I }(r_{ui}-\bar r_u)^2}\sqrt{\sum_{i\in I }(r_{vi}-\bar r_v)^2}}</script><p>其中$r_{ui},r_{vi}$分别表示用户$u$和用户$v$对商品$i$是否有交互(或者具体的评分值)，$\bar r_u, \bar r_v$分别表示用户$u$和用户$v$交互的所有商品交互数量或者具体评分的平均值。所以相比余弦相似度，皮尔逊相关系数通过使用用户的<strong>平均分对各独立评分进行修正</strong>，减小了用户评分偏置的影响。具体实现， 我们也是可以调包， 这个计算方式很多， 下面是其中的一种：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> pearsonr</span><br><span class="line"></span><br><span class="line">i = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">j = [<span class="number">1</span>, <span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0</span>]</span><br><span class="line">pearsonr(i, j)</span><br></pre></td></tr></table></figure>
<p><a href="https://scipy.github.io/devdocs/reference/generated/scipy.stats.pearsonr.html?highlight=pearson#scipy.stats.pearsonr">皮尔逊相关系数 scipy官方文档</a></p>
<h2 id="编写代码计算两个用户、物品的相似度"><a href="#编写代码计算两个用户、物品的相似度" class="headerlink" title="编写代码计算两个用户、物品的相似度"></a>编写代码计算两个用户、物品的相似度</h2><p>详见notebook</p>
]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>推荐系统基础</tag>
      </tags>
  </entry>
  <entry>
    <title>推荐系统基础4</title>
    <url>/2022/04/23/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%804/</url>
    <content><![CDATA[<hr>
<h1 id="任务4：协同过滤进阶"><a href="#任务4：协同过滤进阶" class="headerlink" title="任务4：协同过滤进阶"></a>任务4：协同过滤进阶</h1><ul>
<li>编写User-CF代码，通过用户相似度得到电影推荐</li>
<li>编写Item-CF代码，通过物品相似度得到电影推荐</li>
<li>进阶：如果不使用矩阵乘法，你能使用倒排索引实现上述计算吗？</li>
</ul>
<p>代码地址： <a href="https://github.com/Guadzilla/Basics-of-Recsys">https://github.com/Guadzilla/Basics-of-Recsys</a></p>
<span id="more"></span>
<h2 id="基于用户的协同过滤"><a href="#基于用户的协同过滤" class="headerlink" title="基于用户的协同过滤"></a>基于用户的协同过滤</h2><h3 id="UserCF原理介绍"><a href="#UserCF原理介绍" class="headerlink" title="UserCF原理介绍"></a>UserCF原理介绍</h3><p>基于用户的协同过滤算法(UserCF)的假设是：<strong>相似用户的兴趣也相似</strong>。所以，当一个用户A需要个性化推荐的时候， 我们可以先找到和他有相似兴趣的其他用户， 然后把那些用户喜欢的， 而用户A没有听说过的物品推荐给A。</p>
<p><img src="https://camo.githubusercontent.com/4cf6c62c1f1e533dffdd89db9bc045b560c95444a4a6e61b7b2d49cf28703f06/687474703a2f2f72796c756f2e6f73732d636e2d6368656e6764752e616c6979756e63732e636f6d2fe59bbee78987696d6167652d32303231303632393233323534303238392e706e67" alt="image-20210629232540289"  /></p>
<p><strong>UserCF算法主要包括两个步骤：</strong></p>
<ol>
<li>找到和目标用户兴趣相似的用户集合</li>
<li>找到这个集合中的用户喜欢的， 且目标用户没有听说过的物品推荐给目标用户。</li>
</ol>
<p>上面的两个步骤中， 第一个步骤里面， 我们会基于前面给出的相似性度量的方法找出与目标用户兴趣相似的用户， 而第二个步骤里面， 如何基于相似用户喜欢的物品来对目标用户进行推荐呢？ 这个要依赖于目标用户对相似用户喜欢的物品的一个喜好程度， 那么如何衡量这个程度大小呢？ 为了更好理解上面的两个步骤， 下面拿一个具体的例子把两个步骤具体化。</p>
<p><strong>以下图为例，此例将会用于本文各种算法中</strong></p>
<p><img src="https://camo.githubusercontent.com/38b1abf510c90bc3a8850a09ba19e39ea11b1f83b916f0432cc29fdaed665e5c/687474703a2f2f72796c756f2e6f73732d636e2d6368656e6764752e616c6979756e63732e636f6d2f254535253942254245254537253839253837696d6167652d32303231303632393233323632323735382e706e67" alt="image-20210629232622758"></p>
<p>给用户推荐物品的过程可以<strong>形象化为一个猜测用户对商品进行打分的任务</strong>，上面表格里面是5个用户对于5件物品的一个打分情况，可以理解为用户对物品的喜欢程度</p>
<p>应用UserCF算法的两个步骤：</p>
<ol>
<li>首先根据前面的这些打分情况(或者说已有的用户向量）计算一下Alice和用户1， 2， 3， 4的相似程度， 找出与Alice最相似的n个用户</li>
<li>根据这n个用户对物品5的评分情况和与Alice的相似程度会猜测出Alice对物品5的评分， 如果评分比较高的话， 就把物品5推荐给用户Alice， 否则不推荐。</li>
</ol>
<p>关于第一个步骤， 上面已经给出了计算两个用户相似性的方法， 这里不再过多赘述， 这里主要解决第二个问题， 如何产生最终结果的预测。</p>
<p><strong>最终结果的预测</strong></p>
<p>根据上面的几种方法， 我们可以计算出向量之间的相似程度， 也就是可以计算出Alice和其他用户的相近程度， 这时候我们就可以选出与Alice最相近的前n个用户， 基于他们对物品5的评价猜测出Alice的打分值， 那么是怎么计算的呢？</p>
<p>这里常用的方式之一是<strong>利用用户相似度和相似用户的评价加权平均获得用户的评价预测</strong>， 用下面式子表示：</p>
<script type="math/tex; mode=display">
R_{\mathrm{u}, \mathrm{p}}=\frac{\sum_{\mathrm{s} \in S}\left(w_{\mathrm{u}, \mathrm{s}} \cdot R_{\mathrm{s}, \mathrm{p}}\right)}{\sum_{\mathrm{s} \in S} w_{\mathrm{u}, \mathrm{s}}}</script><p>这个式子里面， 权重$w_{u,s}$是用户$u$和用户$s$的相似度， $R_{s,p}$是用户$s$对物品$p$的评分。</p>
<p>还有一种方式如下：</p>
<script type="math/tex; mode=display">
P_{i, j}=\bar{R}_{i}+\frac{\sum_{k=1}^{n}\left(S_{i, k}\left(R_{k, j}-\bar{R}_{k}\right)\right)}{\sum_{k=1}^{n} S_{i, k}}</script><p>这种方式考虑的更加全面， 依然是用户相似度作为权值， 但后面不单纯是其他用户对物品的评分， 而是<strong>该物品的评分与此用户的所有评分的差值进行加权平均， 这时候考虑到了有的用户内心的评分标准不一的情况</strong>， 即有的用户喜欢打高分， 有的用户喜欢打低分的情况。</p>
<p>所以这一种计算方式更为推荐。下面的计算将使用这个方式。这里的$S_{i,k}$与上面的$w_{u,s}$的意思是类似的，表示的是用户i和用户k之间的相似度。</p>
<p>在获得用户$u$对不同物品的评价预测后， 最终的推荐列表根据预测评分进行排序得到。 至此，基于用户的协同过滤算法的推荐过程完成。</p>
<p>根据上面的问题， 下面手算一下：</p>
<p>目标: 猜测Alice对物品5的得分：</p>
<ol>
<li><strong>计算Alice与其他用户的相似度（这里使用皮尔逊相关系数）</strong>:</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义数据集， 也就是那个表格， 注意这里我们采用字典存放数据， 因为实际情况中数据是非常稀疏的， 很少有情况是现在这样</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadData</span>():</span></span><br><span class="line">    ratings=&#123;<span class="string">&#x27;Alice&#x27;</span>: &#123;<span class="string">&#x27;item1&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;item2&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;item3&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;item4&#x27;</span>: <span class="number">4</span>&#125;,</span><br><span class="line">           <span class="string">&#x27;user1&#x27;</span>: &#123;<span class="string">&#x27;item1&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;item2&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;item3&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;item4&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;item5&#x27;</span>: <span class="number">3</span>&#125;,</span><br><span class="line">           <span class="string">&#x27;user2&#x27;</span>: &#123;<span class="string">&#x27;item1&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;item2&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;item3&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;item4&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;item5&#x27;</span>: <span class="number">5</span>&#125;,</span><br><span class="line">           <span class="string">&#x27;user3&#x27;</span>: &#123;<span class="string">&#x27;item1&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;item2&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;item3&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;item4&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;item5&#x27;</span>: <span class="number">4</span>&#125;,</span><br><span class="line">           <span class="string">&#x27;user4&#x27;</span>: &#123;<span class="string">&#x27;item1&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;item2&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;item3&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;item4&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;item5&#x27;</span>: <span class="number">1</span>&#125;</span><br><span class="line">          &#125;</span><br><span class="line">    <span class="keyword">return</span> ratings</span><br><span class="line">ratings = loadData()</span><br><span class="line">ratings = pd.DataFrame(ratings).T</span><br><span class="line">ratings</span><br></pre></td></tr></table></figure>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220422153425763.png" alt="image-20220422153425763"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 取出用户向量</span></span><br><span class="line">Alice = ratings.loc[<span class="string">&#x27;Alice&#x27;</span>,:<span class="string">&#x27;item4&#x27;</span>]</span><br><span class="line">user1 = ratings.loc[<span class="string">&#x27;user1&#x27;</span>,:<span class="string">&#x27;item4&#x27;</span>]</span><br><span class="line">user2 = ratings.loc[<span class="string">&#x27;user2&#x27;</span>,:<span class="string">&#x27;item4&#x27;</span>]</span><br><span class="line">user3 = ratings.loc[<span class="string">&#x27;user3&#x27;</span>,:<span class="string">&#x27;item4&#x27;</span>]</span><br><span class="line">user4 = ratings.loc[<span class="string">&#x27;user4&#x27;</span>,:<span class="string">&#x27;item4&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义皮尔逊相似度</span></span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> pearsonr</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pearsonrSim</span>(<span class="params">x,y</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    皮尔森相似度</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> pearsonr(x,y)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算Alice和其它用户的相似度</span></span><br><span class="line">Alice_user1_similarity = pearsonrSim(Alice,user1)</span><br><span class="line">Alice_user2_similarity = pearsonrSim(Alice,user2)</span><br><span class="line">Alice_user3_similarity = pearsonrSim(Alice,user3)</span><br><span class="line">Alice_user4_similarity = pearsonrSim(Alice,user4)</span><br><span class="line">Alice_user1_similarity,Alice_user2_similarity,Alice_user3_similarity,Alice_user4_similarity</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出相似度</span></span><br><span class="line">(<span class="number">0.8528028654224415</span>, <span class="number">0.7071067811865475</span>, <span class="number">0.0</span>, -<span class="number">0.7921180343813393</span>)</span><br></pre></td></tr></table></figure>
<p>从这里看出, Alice用户1和用户2,用户3,用户4的相似度是0.85, 0.7, 0, -0.79。 所以如果n=2， 找到与Alice最相近的两个用户是用户1， 和Alice的相似度是0.85， 用户2， 和Alice相似度是0.7。</p>
<ol>
<li><strong>根据相似度用户计算Alice对物品5的最终得分</strong> 用户1对物品5的评分是3， 用户2对物品5的打分是5， 那么根据上面的计算公式， 可以计算出Alice对物品5的最终得分是 </li>
</ol>
<script type="math/tex; mode=display">
P_{Alice, 物品5}=\bar{R}_{Alice}+\frac{\sum_{k=1}^{2}\left(S_{Alice,user k}\left(R_{userk, 物品5}-\bar{R}_{userk}\right)\right)}{\sum_{k=1}^{2} S_{Alice, userk}}=4+\frac{0.85*(3-2.4)+0.7*(5-3.8)}{0.85+0.7}=4.87</script><p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220422154709719.png" alt="image-20220422154709719"></p>
<ol>
<li><strong>根据用户评分对用户进行推荐</strong> 这时候， 我们就得到了Alice对物品5的得分是4.87， 根据Alice的打分对物品排个序从大到小：<script type="math/tex">物品1>物品5>物品3=物品4>物品2</script> 这时候，如果要向Alice推荐2款产品的话， 我们就可以推荐物品1和物品5给Alice</li>
</ol>
<p>至此， 基于用户的协同过滤算法原理介绍完毕。</p>
<h3 id="UserCF代码实现"><a href="#UserCF代码实现" class="headerlink" title="UserCF代码实现"></a>UserCF代码实现</h3><p>这里简单的通过编程实现上面的案例，为后面的大作业做一个热身， 梳理一下上面的过程其实就是三步： 计算用户相似性矩阵、得到前n个相似用户、计算最终得分。</p>
<p>所以我们下面的程序也是分为这三步：</p>
<ol>
<li><strong>首先， 先把数据表给建立起来</strong> 这里采用字典的方式， 之所以没有用pandas， 是因为上面举得这个例子其实是个个例， 在真实情况中， 我们知道， 用户对物品的打分情况并不会这么完整， 会存在大量的空值， 所以矩阵会很稀疏， 这时候用DataFrame， 会有大量的NaN。故这里用字典的形式存储。 用两个字典， 第一个字典是物品-用户的评分映射， 键是物品1-5， 用A-E来表示， 每一个值又是一个字典， 表示的是每个用户对该物品的打分。 第二个字典是用户-物品的评分映射， 键是上面的五个用户， 用1-5表示， 值是该用户对每个物品的打分。</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义数据集， 也就是那个表格， 注意这里我们采用字典存放数据， 因为实际情况中数据是非常稀疏的， 很少有情况是现在这样</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadData</span>():</span></span><br><span class="line">    items=&#123;<span class="string">&#x27;A&#x27;</span>: &#123;<span class="number">1</span>: <span class="number">5</span>, <span class="number">2</span>: <span class="number">3</span>, <span class="number">3</span>: <span class="number">4</span>, <span class="number">4</span>: <span class="number">3</span>, <span class="number">5</span>: <span class="number">1</span>&#125;,</span><br><span class="line">           <span class="string">&#x27;B&#x27;</span>: &#123;<span class="number">1</span>: <span class="number">3</span>, <span class="number">2</span>: <span class="number">1</span>, <span class="number">3</span>: <span class="number">3</span>, <span class="number">4</span>: <span class="number">3</span>, <span class="number">5</span>: <span class="number">5</span>&#125;,</span><br><span class="line">           <span class="string">&#x27;C&#x27;</span>: &#123;<span class="number">1</span>: <span class="number">4</span>, <span class="number">2</span>: <span class="number">2</span>, <span class="number">3</span>: <span class="number">4</span>, <span class="number">4</span>: <span class="number">1</span>, <span class="number">5</span>: <span class="number">5</span>&#125;,</span><br><span class="line">           <span class="string">&#x27;D&#x27;</span>: &#123;<span class="number">1</span>: <span class="number">4</span>, <span class="number">2</span>: <span class="number">3</span>, <span class="number">3</span>: <span class="number">3</span>, <span class="number">4</span>: <span class="number">5</span>, <span class="number">5</span>: <span class="number">2</span>&#125;,</span><br><span class="line">           <span class="string">&#x27;E&#x27;</span>: &#123;<span class="number">2</span>: <span class="number">3</span>, <span class="number">3</span>: <span class="number">5</span>, <span class="number">4</span>: <span class="number">4</span>, <span class="number">5</span>: <span class="number">1</span>&#125;</span><br><span class="line">          &#125;</span><br><span class="line">    users=&#123;<span class="number">1</span>: &#123;<span class="string">&#x27;A&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;B&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;C&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;D&#x27;</span>: <span class="number">4</span>&#125;,</span><br><span class="line">           <span class="number">2</span>: &#123;<span class="string">&#x27;A&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;B&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;C&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;D&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;E&#x27;</span>: <span class="number">3</span>&#125;,</span><br><span class="line">           <span class="number">3</span>: &#123;<span class="string">&#x27;A&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;B&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;C&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;D&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;E&#x27;</span>: <span class="number">5</span>&#125;,</span><br><span class="line">           <span class="number">4</span>: &#123;<span class="string">&#x27;A&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;B&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;C&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;D&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;E&#x27;</span>: <span class="number">4</span>&#125;,</span><br><span class="line">           <span class="number">5</span>: &#123;<span class="string">&#x27;A&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;B&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;C&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;D&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;E&#x27;</span>: <span class="number">1</span>&#125;</span><br><span class="line">          &#125;</span><br><span class="line">    <span class="keyword">return</span> items,users</span><br><span class="line"></span><br><span class="line">items, users = loadData()</span><br><span class="line">item_df = pd.DataFrame(items).T</span><br><span class="line">user_df = pd.DataFrame(users).T</span><br></pre></td></tr></table></figure>
<ol>
<li><strong>计算用户相似性矩阵</strong> 这个是一个共现矩阵, 5*5，行代表每个用户， 列代表每个用户， 值代表用户和用户的相关性，这里的思路是这样， 因为要求用户和用户两两的相关性， 所以需要用双层循环遍历用户-物品评分数据， 当不是同一个用户的时候， 我们要去遍历物品-用户评分数据， 在里面去找这两个用户同时对该物品评过分的数据放入到这两个用户向量中。 因为正常情况下会存在很多的NAN， 即可能用户并没有对某个物品进行评分过， 这样的不能当做用户向量的一部分， 没法计算相似性。 还是看代码吧， 感觉不太好描述：</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;计算用户相似性矩阵&quot;&quot;&quot;</span></span><br><span class="line">similarity_matrix = pd.DataFrame(-<span class="number">1</span> * np.ones((<span class="built_in">len</span>(users), <span class="built_in">len</span>(users))), index=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], columns=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> userx <span class="keyword">in</span> users:</span><br><span class="line">    <span class="keyword">for</span> usery <span class="keyword">in</span> users:</span><br><span class="line">        userxVec=[]</span><br><span class="line">        useryVec=[]</span><br><span class="line">        <span class="keyword">if</span> userx == usery:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            userx_history = users[userx].keys()</span><br><span class="line">            usery_history = users[usery].keys()</span><br><span class="line">            intersection = <span class="built_in">set</span>(userx_history).intersection(usery_history) <span class="comment"># 用户x和用户y行为历史的交集，否则有nan无法计算相似性</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> intersection:</span><br><span class="line">                userxVec.append(users[userx][i])</span><br><span class="line">                useryVec.append(users[usery][i])</span><br><span class="line">            similarity_matrix[userx][usery]=np.corrcoef(np.array(userxVec),np.array(useryVec))[<span class="number">0</span>][<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<p>得到如下user相似性矩阵：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220422161751278.png" alt="image-20220422161751278"></p>
<p>注意相似度矩阵的初始值为-1，因为皮尔逊相关系数的取值为[-1,1]。</p>
<ol>
<li><strong>计算前n个相似的用户</strong></li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;计算前n个相似的用户&quot;&quot;&quot;</span></span><br><span class="line">n = <span class="number">2</span></span><br><span class="line">similar_users = <span class="built_in">dict</span>()</span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> users:</span><br><span class="line">    similar_users[user] = similarity_matrix[user].sort_values(ascending=<span class="literal">False</span>)[:n].index.tolist()</span><br><span class="line">    </span><br><span class="line">similar_users</span><br><span class="line">&#123;<span class="number">1</span>: [<span class="number">2</span>, <span class="number">3</span>], <span class="number">2</span>: [<span class="number">1</span>, <span class="number">4</span>], <span class="number">3</span>: [<span class="number">1</span>, <span class="number">2</span>], <span class="number">4</span>: [<span class="number">2</span>, <span class="number">1</span>], <span class="number">5</span>: [<span class="number">3</span>, <span class="number">4</span>]&#125;</span><br></pre></td></tr></table></figure>
<p>经计算，与用户1最相似的2个用户分别是 用户2 和 用户3 。</p>
<ol>
<li><strong>计算最终得分</strong></li>
</ol>
<p>这里就是上面的那个公式：</p>
<script type="math/tex; mode=display">
P_{Alice, 物品5}=\bar{R}_{Alice}+\frac{\sum_{k=1}^{2}\left(S_{Alice,user k}\left(R_{userk, 物品5}-\bar{R}_{userk}\right)\right)}{\sum_{k=1}^{2} S_{Alice, userk}}</script><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;计算最后得分,用户1对物品E的预测评分&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算所有用户平均评分</span></span><br><span class="line">user_mean_rating = <span class="built_in">dict</span>()</span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> users:</span><br><span class="line">    user_mean = np.mean([value <span class="keyword">for</span> value <span class="keyword">in</span> users[user].values()])</span><br><span class="line">    user_mean_rating[user] = user_mean</span><br><span class="line"><span class="comment"># 计算预测得分</span></span><br><span class="line">weighted_scores = <span class="number">0.</span></span><br><span class="line">corr_values_sum = <span class="number">0.</span></span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> similar_users[<span class="number">1</span>]:</span><br><span class="line">    weighted_scores += similarity_matrix[<span class="number">1</span>][user]</span><br><span class="line">    corr_values_sum += similarity_matrix[<span class="number">1</span>][user] * (users[user][<span class="string">&#x27;E&#x27;</span>] - user_mean_rating[user])</span><br><span class="line">predict = user_mean_rating[<span class="number">1</span>] + corr_values_sum/weighted_scores</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;用户1对物品E的预测评分为 <span class="subst">&#123;predict:<span class="number">.2</span>f&#125;</span> &#x27;</span>)</span><br><span class="line"></span><br><span class="line">用户<span class="number">1</span>对物品E的预测评分为 <span class="number">4.87</span></span><br></pre></td></tr></table></figure>
<p>计算结果如下：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220422164842510.png" alt="image-20220422164842510" style="zoom:67%;" /></p>
<h3 id="UserCF的缺点"><a href="#UserCF的缺点" class="headerlink" title="UserCF的缺点"></a>UserCF的缺点</h3><p>UserCF算法存在两个重大问题：</p>
<ol>
<li>数据稀疏性。 一个大型的电子商务推荐系统一般有非常多的物品，用户可能买的其中不到1%的物品，不同用户之间买的物品重叠性较低，导致算法无法找到一个用户的邻居，即偏好相似的用户。<strong>这导致UserCF不适用于那些正反馈获取较困难的应用场景</strong>(如酒店预订， 大件商品购买等低频应用)</li>
<li>算法扩展性。 基于用户的协同过滤需要维护用户相似度矩阵以便快速的找出Topn相似用户， 该矩阵的存储开销非常大，存储空间随着用户数量的增加而增加，<strong>不适合用户数据量大的情况使用</strong>。</li>
</ol>
<p>由于UserCF技术上的两点缺陷， 导致很多电商平台并没有采用这种算法， 而是采用了ItemCF算法实现最初的推荐系统。</p>
<h2 id="基于物品的协同过滤"><a href="#基于物品的协同过滤" class="headerlink" title="基于物品的协同过滤"></a>基于物品的协同过滤</h2><h3 id="ItemCF原理介绍"><a href="#ItemCF原理介绍" class="headerlink" title="ItemCF原理介绍"></a>ItemCF原理介绍</h3><p>基于物品的协同过滤(ItemCF)的基本思想是预先根据所有用户的历史偏好数据计算物品之间的相似性，然后把与用户喜欢的物品相类似的物品推荐给用户。比如物品a和c非常相似，因为喜欢a的用户同时也喜欢c，而用户A喜欢a，所以把c推荐给用户A。<strong>ItemCF算法并不利用物品的内容属性计算物品之间的相似度， 主要通过分析用户的行为记录计算物品之间的相似度， 该算法认为， 物品a和物品c具有很大的相似度是因为喜欢物品a的用户大都喜欢物品c</strong>。</p>
<p><strong>和UserCF类似，ItemCF算法主要包括两个步骤：</strong></p>
<ul>
<li>计算物品之间的相似度</li>
<li>根据物品的相似度和用户的历史行为给用户生成推荐列表（购买了该商品的用户也经常购买的其他商品）</li>
</ul>
<p>这里直接还是拿上面Alice的那个例子来看。</p>
<p><img src="https://camo.githubusercontent.com/38b1abf510c90bc3a8850a09ba19e39ea11b1f83b916f0432cc29fdaed665e5c/687474703a2f2f72796c756f2e6f73732d636e2d6368656e6764752e616c6979756e63732e636f6d2f254535253942254245254537253839253837696d6167652d32303231303632393233323632323735382e706e67" alt="image-20210629232622758"></p>
<p>如果想知道Alice对物品5打多少分， 基于物品的协同过滤算法会这么做：</p>
<ol>
<li>首先计算一下物品5和物品1， 2， 3， 4之间的相似性(它们也是向量的形式， 每一列的值就是它们的向量表示， 因为ItemCF认为如果物品a和物品c具有很大的相似度，那么是因为喜欢物品a的用户大都喜欢物品c， 所以就可以基于每个用户对该物品的打分或者说喜欢程度来向量化物品)</li>
<li>找出与物品5最相近的n个物品（取n=2）</li>
<li>根据Alice对最相近的n个物品的打分去计算对物品5的打分情况，加入评分偏置的预测公式如下：</li>
</ol>
<script type="math/tex; mode=display">
P_{Alice, 物品5}=\bar{R}_{物品5}+\frac{\sum_{k=1}^{2}\left(S_{物品5,物品 k}\left(R_{Alice, 物品k}-\bar{R}_{物品k}\right)\right)}{\sum_{k=1}^{2} S_{物品k, 物品5}}</script><p><strong>下面我们就可以具体计算一下，猜测Alice对物品5的打分：</strong></p>
<p>首先是步骤1：计算物品5和其它物品之间的相似度。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220422193745961.png" alt="image-20220422193745961"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 获取item向量</span></span><br><span class="line">item5 = ratings.loc[<span class="string">&#x27;user1&#x27;</span>:,<span class="string">&#x27;item5&#x27;</span>].values.tolist()</span><br><span class="line">item4 = ratings.loc[<span class="string">&#x27;user1&#x27;</span>:,<span class="string">&#x27;item4&#x27;</span>].values.tolist()</span><br><span class="line">item3 = ratings.loc[<span class="string">&#x27;user1&#x27;</span>:,<span class="string">&#x27;item3&#x27;</span>].values.tolist()</span><br><span class="line">item2 = ratings.loc[<span class="string">&#x27;user1&#x27;</span>:,<span class="string">&#x27;item2&#x27;</span>].values.tolist()</span><br><span class="line">item1 = ratings.loc[<span class="string">&#x27;user1&#x27;</span>:,<span class="string">&#x27;item1&#x27;</span>].values.tolist()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算item相似度</span></span><br><span class="line">item51_similarity = pearsonrSim(item5,item1)</span><br><span class="line">item52_similarity = pearsonrSim(item5,item2)</span><br><span class="line">item53_similarity = pearsonrSim(item5,item3)</span><br><span class="line">item54_similarity = pearsonrSim(item5,item4)</span><br><span class="line">[x.<span class="built_in">round</span>(<span class="number">2</span>) <span class="keyword">for</span> x <span class="keyword">in</span> (item51_similarity,item52_similarity,item53_similarity,item54_similarity)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出相似度</span></span><br><span class="line">[<span class="number">0.97</span>, -<span class="number">0.48</span>, -<span class="number">0.43</span>, <span class="number">0.58</span>]</span><br></pre></td></tr></table></figure>
<p>步骤2：对相似度进行排序，选择最靠前的n=2个物品：item1和item4</p>
<p>步骤3：下面根据公式计算Alice对物品5的打分</p>
<script type="math/tex; mode=display">
P_{Alice, 物品5}=\bar{R}_{物品5}+\frac{\sum_{k=1}^{2}\left(S_{物品5,物品 k}\left(R_{Alice, 物品k}-\bar{R}_{物品k}\right)\right)}{\sum_{k=1}^{2} S_{物品k, 物品5}}=\frac{13}{4}+\frac{0.97*(5-3.2)+0.58*(4-3.4)}{0.97+0.58}=4.6</script><p>这时候依然可以向Alice推荐物品5。</p>
<p>下面也是简单编程实现一下， 和上面的差不多：</p>
<h3 id="ItemCF代码实现"><a href="#ItemCF代码实现" class="headerlink" title="ItemCF代码实现"></a>ItemCF代码实现</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;计算物品的相似矩阵&quot;&quot;&quot;</span></span><br><span class="line">similarity_matrix = pd.DataFrame(-<span class="number">1</span> * np.ones((<span class="built_in">len</span>(items), <span class="built_in">len</span>(items))), index=[<span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;C&#x27;</span>, <span class="string">&#x27;D&#x27;</span>, <span class="string">&#x27;E&#x27;</span>], columns=[<span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;C&#x27;</span>, <span class="string">&#x27;D&#x27;</span>, <span class="string">&#x27;E&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 遍历每条物品-用户评分数据</span></span><br><span class="line"><span class="keyword">for</span> itemx <span class="keyword">in</span> items:</span><br><span class="line">    <span class="keyword">for</span> itemy <span class="keyword">in</span> items:</span><br><span class="line">        itemxVec = []</span><br><span class="line">        itemyVec = []</span><br><span class="line">        <span class="keyword">if</span> itemx == itemy:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            itemx_history = <span class="built_in">set</span>(items[itemx].keys())</span><br><span class="line">            itemy_history = <span class="built_in">set</span>(items[itemy].keys())</span><br><span class="line">            intersection = itemx_history.intersection(itemy_history)  <span class="comment"># 求交集，同时对两个物品都打分的用户，才有意义</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> intersection:</span><br><span class="line">                itemxVec.append(items[itemx][i])</span><br><span class="line">                itemyVec.append(items[itemy][i])</span><br><span class="line">            similarity_matrix[itemx][itemy] = pearsonrSim(itemxVec,itemyVec).<span class="built_in">round</span>(<span class="number">2</span>)</span><br><span class="line">            <span class="comment"># similarity_matrix[itemx][itemy] = np.corrcoef(np.array(itemxVec),np.array(itemyVec))[0][1] 两种计算方式等价</span></span><br></pre></td></tr></table></figure>
<p>得到物品相似度矩阵：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220422195802322.png" alt="image-20220422195802322"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;计算前n个相似的用户&quot;&quot;&quot;</span></span><br><span class="line">n = <span class="number">2</span></span><br><span class="line">similar_items = <span class="built_in">dict</span>()</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">    similar_items[item] = similarity_matrix[item].sort_values(ascending=<span class="literal">False</span>)[:n].index.tolist()</span><br><span class="line">    </span><br><span class="line">similar_items[<span class="string">&#x27;E&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 与E最相似的2个物品是A和D</span></span><br><span class="line">[<span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;D&#x27;</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;计算最后得分,用户1对物品E的预测评分&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算物品平均打分情况</span></span><br><span class="line">item_ratings_mean = <span class="built_in">dict</span>()</span><br><span class="line"><span class="keyword">for</span> item,rating <span class="keyword">in</span> items.items():</span><br><span class="line">    item_ratings_mean[item] = np.mean([value <span class="keyword">for</span> value <span class="keyword">in</span> rating.values()])</span><br><span class="line"></span><br><span class="line">weighted_scores = <span class="number">0.</span></span><br><span class="line">corr_values_sum = <span class="number">0.</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> similar_items[<span class="string">&#x27;E&#x27;</span>]:</span><br><span class="line">    weighted_scores += similarity_matrix[<span class="string">&#x27;E&#x27;</span>][item]</span><br><span class="line">    corr_values_sum += similarity_matrix[<span class="string">&#x27;E&#x27;</span>][item] * (users[<span class="number">1</span>][item] -  item_ratings_mean[item])</span><br><span class="line"></span><br><span class="line">predict = item_ratings_mean[<span class="string">&#x27;E&#x27;</span>] + corr_values_sum/weighted_scores</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;用户1对物品E的预测得分为 <span class="subst">&#123;predict:<span class="number">.2</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="9-协同过滤算法的问题分析"><a href="#9-协同过滤算法的问题分析" class="headerlink" title="9. 协同过滤算法的问题分析"></a>9. 协同过滤算法的问题分析</h3><p>协同过滤算法存在的问题之一就是<strong>泛化能力弱</strong>， 即协同过滤无法将两个物品相似的信息推广到其他物品的相似性上。 导致的问题是<strong>热门物品具有很强的头部效应， 容易跟大量物品产生相似， 而尾部物品由于特征向量稀疏， 导致很少被推荐</strong>。 比如下面这个例子：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220422203637025.png" alt="image-20220422203637025"></p>
<p>A, B, C, D是物品， 看右边的物品共现矩阵， 可以发现物品D与A、B、C的相似度比较大， 所以很有可能将D推荐给用过A、B、C的用户。 但是物品D与其他物品相似的原因是因为D是一件热门商品， 系统无法找出A、B、C之间相似性的原因是其特征太稀疏， 缺乏相似性计算的直接数据。 所以这就是协同过滤的天然缺陷：<strong>推荐系统头部效应明显， 处理稀疏向量的能力弱</strong>。</p>
<p>为了解决这个问题， 同时增加模型的泛化能力，2006年，<strong>矩阵分解技术(Matrix Factorization,MF</strong>)被提出， 该方法在协同过滤共现矩阵的基础上， 使用更稠密的隐向量表示用户和物品， 挖掘用户和物品的隐含兴趣和隐含特征， 在一定程度上弥补协同过滤模型处理稀疏矩阵能力不足的问题。</p>
<h2 id="进阶：如果不使用矩阵乘法，你能使用倒排索引实现上述计算吗？"><a href="#进阶：如果不使用矩阵乘法，你能使用倒排索引实现上述计算吗？" class="headerlink" title="进阶：如果不使用矩阵乘法，你能使用倒排索引实现上述计算吗？"></a>进阶：如果不使用矩阵乘法，你能使用倒排索引实现上述计算吗？</h2><p>矩阵乘法用在求两个向量的内积。当问题场景是仅预测用户是否会对物品评分，即共现矩阵只有 0 和 1 时，两个向量的内积可以用集合的形式表示。例如： <code>u = [1,0,0,1,0], v = [0,0,1,1,0]</code> ，矩阵乘法求得两个向量内积为 1 ，从集合的角度看，内积的计算结果其实也是用户 u 和 v 交互过的物品的交集元素个数。</p>
<p>所以集合角度的余弦相似度计算如下：</p>
<script type="math/tex; mode=display">
sim_{uv}=\frac{|N(u) \cap N(v)|}{\sqrt{|N(u)|\cdot|N(v)|}}</script><p>如果只建立 user 对 item 的索引，形式如： <code>&#123;uid: &#123;item1, item2,...&#125;, uid: &#123;item1, item2,...&#125;, ...&#125;</code>，计算两两用户的交互交集时，比较麻烦。建立倒排表形如：<code>&#123;item_id1: &#123;user_id1, user_id2, ... , user_idn&#125;, item_id2: ...&#125;</code> ，只需要对每个 item 遍历，就可以统计两两用户的交互交集。代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 建立item-&gt;users倒排表</span></span><br><span class="line"><span class="comment"># 倒排表的格式为: &#123;item_id1: &#123;user_id1, user_id2, ... , user_idn&#125;, item_id2: ...&#125; 也就是每个item对应有那些用户有过点击</span></span><br><span class="line"><span class="comment"># 建立倒排表的目的就是为了更方便的统计用户之间共同交互的商品数量</span></span><br><span class="line">item_users = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> uid, items <span class="keyword">in</span> tqdm(tra_users.items()): <span class="comment"># 遍历每一个用户的数据,其中包含了该用户所有交互的item</span></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> items: <span class="comment"># 遍历该用户的所有item, 给这些item对应的用户列表添加对应的uid</span></span><br><span class="line">        <span class="keyword">if</span> item <span class="keyword">not</span> <span class="keyword">in</span> item_users:</span><br><span class="line">            item_users[item] = <span class="built_in">set</span>()</span><br><span class="line">            item_users[item].add(uid)</span><br></pre></td></tr></table></figure>
<h2 id="课后思考"><a href="#课后思考" class="headerlink" title="课后思考"></a>课后思考</h2><p>1.<strong>什么时候使用UserCF，什么时候使用ItemCF？为什么？</strong></p>
<blockquote>
<ol>
<li>UserCF 由于是基于用户相似度进行推荐， 所以具备更强的社交特性， 这样的特点非常适于<strong>用户少， 物品多， 时效性较强的场合</strong>， 比如新闻推荐场景， 因为新闻本身兴趣点分散， 相比用户对不同新闻的兴趣偏好， 新闻的及时性，热点性往往更加重要， 所以正好适用于发现热点，跟踪热点的趋势。 另外还具有推荐新信息的能力， 更有可能发现惊喜, 因为看的是人与人的相似性, 推出来的结果可能更有惊喜，可以发现用户潜在但自己尚未察觉的兴趣爱好。</li>
<li>ItemCF 这个更适用于兴趣变化较为稳定的应用， 更接近于个性化的推荐， 适合<strong>物品少，用户多，用户兴趣固定持久， 物品更新速度不是太快的场合</strong>， 比如推荐艺术品， 音乐， 电影。</li>
</ol>
</blockquote>
<p>2.<strong>协同过滤在计算上有什么缺点？有什么比较好的思路可以解决（缓解）？</strong></p>
<blockquote>
<p>第一个问题就是<strong>泛化能力弱</strong>， 即协同过滤无法将两个物品相似的信息推广到其他物品的相似性上。 导致的问题是<strong>热门物品具有很强的头部效应， 容易跟大量物品产生相似， 而尾部物品由于特征向量稀疏， 导致很少被推荐</strong>。</p>
</blockquote>
<p><strong>3.上面介绍的相似度计算方法有什么优劣之处？</strong></p>
<blockquote>
<p>cosine相似度还是比较常用的， 一般效果也不会太差， 但是对于评分数据不规范的时候， 也就是说， 存在有的用户喜欢打高分， 有的用户喜欢打低分情况的时候，有的用户喜欢乱打分的情况， 这时候consine相似度算出来的结果可能就不是那么准确了。所以对于这种用户评分偏置的情况， 余弦相似度就不是那么好了， 可以考虑使用下面的皮尔逊相关系数。</p>
<script type="math/tex; mode=display">
P_{i, j}=\bar{R}_{i}+\frac{\sum_{k=1}^{n}\left(S_{i, k}\left(R_{k, j}-\bar{R}_{k}\right)\right)}{\sum_{k=1}^{n} S_{i, k}}</script></blockquote>
<p>4.<strong>协同过滤还存在其他什么缺陷？有什么比较好的思路可以解决（缓解）？</strong></p>
<blockquote>
<p>协同过滤的特点就是完全没有利用到物品本身或者是用户自身的属性， 仅仅利用了用户与物品的交互信息就可以实现推荐，比较简单高效， 但这也是它的一个短板所在， 由于无法有效的引入用户年龄， 性别，商品描述，商品分类，当前时间，地点等一系列用户特征、物品特征和上下文特征， 这就造成了有效信息的遗漏，不能充分利用其它特征数据。</p>
<p>为了解决这个问题， 在推荐模型中引用更多的特征，<strong>推荐系统慢慢的从以协同过滤为核心到了以逻辑回归模型为核心</strong>， 提出了能够综合不同类型特征的机器学习模型。</p>
<p>演化图左边的时间线梳理完毕：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220424123935703.png" alt="《深度学习推荐系统》- 王喆"></p>
</blockquote>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>使用皮尔逊相似度计算的预测结果非常差，可能因为计算皮尔逊相似度时出现大量nan（不知道为什么…），然后我的做法是用0填充，pearsonr = 0 代表无关。</p>
<p>皮尔逊相关系数计算公式：</p>
<script type="math/tex; mode=display">
P_{i, j}=\bar{R}_{i}+\frac{\sum_{k=1}^{n}\left(S_{i, k}\left(R_{k, j}-\bar{R}_{k}\right)\right)}{\sum_{k=1}^{n} S_{i, k}}</script><p>预测结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Pearsonr_ItemCF(K=<span class="number">10</span>,N=<span class="number">10</span>)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">recall: 0.18</span></span><br><span class="line"><span class="string">precision 0.61</span></span><br><span class="line"><span class="string">coverage 35.17</span></span><br><span class="line"><span class="string">Popularity 5.539</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">Cosine_Item_CF(K=<span class="number">10</span>,N=<span class="number">10</span>)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">recall: 9.2</span></span><br><span class="line"><span class="string">precision 30.48</span></span><br><span class="line"><span class="string">coverage 19.18</span></span><br><span class="line"><span class="string">Popularity 7.171</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>推荐系统基础</tag>
      </tags>
  </entry>
  <entry>
    <title>推荐系统基础6</title>
    <url>/2022/04/26/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%806/</url>
    <content><![CDATA[<hr>
<h1 id="任务6：Slope-One"><a href="#任务6：Slope-One" class="headerlink" title="任务6：Slope One"></a>任务6：Slope One</h1><ul>
<li>阅读<a href="https://blog.csdn.net/xidianliutingting/article/details/51916578">Slope One基础原理</a></li>
<li>编写Slope One用于电影推荐的流程</li>
<li>比较Slope One、SVD、协同过滤的精度，哪一个模型的RMSE评分更低？</li>
</ul>
<p>代码地址： <a href="https://github.com/Guadzilla/Basics-of-Recsys">https://github.com/Guadzilla/Basics-of-Recsys</a></p>
<span id="more"></span>
<h2 id="slope-one-算法"><a href="#slope-one-算法" class="headerlink" title="slope one 算法"></a>slope one 算法</h2><p>1.示例引入</p>
<p>我们可以这么认为，商品间受欢迎的差异从某种程度上是固定的，比如所有人都喜欢海底捞火锅，但对赛百味的喜爱程度一般。此时小明对海底捞火锅的评分为4，对赛百味的评分为2；而小吴对海底捞火锅的评分为5，对赛百味的评分为3。尽管两个人评分的习惯上不同，小明平均打的分都高，但是对两个物品来说，他们之间的评分差值是不变的，即 $5-3=4-2$ 。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220426192311847.png" alt="image-20220426192311847"></p>
<p>Slope one 的思路大抵如此，现在假设我们想预测 Alice 对物品 2 的评分，已有的是别的用户对物品 1 和对物品 2 的评分和 Alice 对物品 1 的评分。</p>
<p>从物品评分偏差的角度，我们可以求出物品 1 和物品 2 之间的评分偏差，即 $R_{1,2}=\frac{(3-1)+(4-3)+(3-3)+(1-5)}{4}=-0.25$，再用 Alice 对物品 1 的评分减去这个偏差，即 $p_{Alice,2}=r_{Alice,1}-R_{1,2}=5-(-0.25)=5.25$，把它作为 Alice 对物品 2 的预测评分。这就是Slope one 算法最简单的场景。</p>
<h2 id="slope-one-算法思想"><a href="#slope-one-算法思想" class="headerlink" title="slope one 算法思想"></a>slope one 算法思想</h2><p>Slope One 算法是由 Daniel Lemire 教授在 2005 年提出的一个 <strong>Item-Based</strong> 的协同过滤推荐算法。和其它类似算法相比, 它的最大优点在于算法很简单, 易于实现, 执行效率高, 同时推荐的准确性相对较高。<br>Slope One算法是基于不同物品之间的评分差的线性算法，预测用户对物品评分的个性化算法。主要分为三步：</p>
<p>Step1: 计算物品之间的评分差的均值，记为物品间的评分偏差(两物品同时被评分)；</p>
<p><img src="https://img-blog.csdn.net/20160715114006473" alt="这里写图片描述"></p>
<p>Step2:根据物品间的评分偏差和用户的历史评分，预测用户对未评分的物品的评分。</p>
<p><img src="https://img-blog.csdn.net/20160715114054480" alt="这里写图片描述"></p>
<p>Step3:将预测评分排序，取topN对应的物品推荐给用户。</p>
<p><strong>举例：</strong><br>假设有100个人对物品A和物品B打分了，R(AB)表示这100个人对A和B打分的平均偏差;有1000个人对物品B和物品C打分了， R(CB)表示这1000个人对C和B打分的平均偏差；</p>
<p><img src="https://img-blog.csdn.net/20160715114619049" alt="这里写图片描述"></p>
<h2 id="slope-one-的代码实现"><a href="#slope-one-的代码实现" class="headerlink" title="slope one 的代码实现"></a>slope one 的代码实现</h2><p>1.准备数据</p>
<p><img src="https://camo.githubusercontent.com/68d8995d1a9bacf4e58fa39359de71cbb99e3bf5abc5174bd1033e8b93fdae81/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303230303832373135303233373932312e706e67237069635f63656e746572" alt="在这里插入图片描述"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义数据集</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadData</span>():</span></span><br><span class="line">    rating_data=&#123;<span class="number">1</span>: &#123;<span class="string">&#x27;A&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;B&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;C&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;D&#x27;</span>: <span class="number">4</span>&#125;,</span><br><span class="line">                 <span class="number">2</span>: &#123;<span class="string">&#x27;A&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;B&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;C&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;D&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;E&#x27;</span>: <span class="number">3</span>&#125;,</span><br><span class="line">                 <span class="number">3</span>: &#123;<span class="string">&#x27;A&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;B&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;C&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;D&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;E&#x27;</span>: <span class="number">5</span>&#125;,</span><br><span class="line">                 <span class="number">4</span>: &#123;<span class="string">&#x27;A&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;B&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;C&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;D&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;E&#x27;</span>: <span class="number">4</span>&#125;,</span><br><span class="line">                 <span class="number">5</span>: &#123;<span class="string">&#x27;A&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;B&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;C&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;D&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;E&#x27;</span>: <span class="number">1</span>&#125;</span><br><span class="line">              &#125;</span><br><span class="line">    <span class="keyword">return</span> rating_data	</span><br><span class="line">users_rating = loadData()</span><br><span class="line">users_rating</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">&#123;1: &#123;&#x27;A&#x27;: 5, &#x27;B&#x27;: 3, &#x27;C&#x27;: 4, &#x27;D&#x27;: 4&#125;,</span></span><br><span class="line"><span class="string"> 2: &#123;&#x27;A&#x27;: 3, &#x27;B&#x27;: 1, &#x27;C&#x27;: 2, &#x27;D&#x27;: 3, &#x27;E&#x27;: 3&#125;,</span></span><br><span class="line"><span class="string"> 3: &#123;&#x27;A&#x27;: 4, &#x27;B&#x27;: 3, &#x27;C&#x27;: 4, &#x27;D&#x27;: 3, &#x27;E&#x27;: 5&#125;,</span></span><br><span class="line"><span class="string"> 4: &#123;&#x27;A&#x27;: 3, &#x27;B&#x27;: 3, &#x27;C&#x27;: 1, &#x27;D&#x27;: 5, &#x27;E&#x27;: 4&#125;,</span></span><br><span class="line"><span class="string"> 5: &#123;&#x27;A&#x27;: 1, &#x27;B&#x27;: 5, &#x27;C&#x27;: 5, &#x27;D&#x27;: 2, &#x27;E&#x27;: 1&#125;&#125;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p>2.建立倒排索引</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 建立倒排索引</span></span><br><span class="line">items_rating = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> user,ratings <span class="keyword">in</span> users_rating.items():</span><br><span class="line">    <span class="keyword">for</span> item,rating <span class="keyword">in</span> ratings.items():</span><br><span class="line">        <span class="keyword">if</span> item <span class="keyword">not</span> <span class="keyword">in</span> items_rating:</span><br><span class="line">            items_rating[item] = &#123;&#125;</span><br><span class="line">        <span class="keyword">if</span> user <span class="keyword">not</span> <span class="keyword">in</span> items_rating[item]:</span><br><span class="line">            items_rating[item][user] = <span class="number">0</span></span><br><span class="line">        items_rating[item][user] = rating</span><br><span class="line">items_rating</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">&#123;&#x27;A&#x27;: &#123;1: 5, 2: 3, 3: 4, 4: 3, 5: 1&#125;,</span></span><br><span class="line"><span class="string"> &#x27;B&#x27;: &#123;1: 3, 2: 1, 3: 3, 4: 3, 5: 5&#125;,</span></span><br><span class="line"><span class="string"> &#x27;C&#x27;: &#123;1: 4, 2: 2, 3: 4, 4: 1, 5: 5&#125;,</span></span><br><span class="line"><span class="string"> &#x27;D&#x27;: &#123;1: 4, 2: 3, 3: 3, 4: 5, 5: 2&#125;,</span></span><br><span class="line"><span class="string"> &#x27;E&#x27;: &#123;2: 3, 3: 5, 4: 4, 5: 1&#125;&#125;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p>3.计算物品间评分偏差矩阵</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 计算物品间评分偏差</span></span><br><span class="line">Ratings_diffs = &#123;&#125;	<span class="comment"># 评分偏差矩阵</span></span><br><span class="line">N_set = &#123;&#125;			<span class="comment"># 物品对间共同被评分次数</span></span><br><span class="line"><span class="keyword">for</span> itemx,itemx_history <span class="keyword">in</span> items_rating.items():</span><br><span class="line">    <span class="keyword">if</span> itemx <span class="keyword">not</span> <span class="keyword">in</span> Ratings_diffs:</span><br><span class="line">        Ratings_diffs[itemx] = &#123;&#125;</span><br><span class="line">        N_set[itemx] = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> itemy,itemy_history <span class="keyword">in</span> items_rating.items():</span><br><span class="line">        <span class="keyword">if</span> itemx != itemy:</span><br><span class="line">            Ratings_diffs[itemx][itemy] = <span class="number">0</span></span><br><span class="line">            N_set[itemx][itemy] = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> x_user <span class="keyword">in</span> itemx_history:</span><br><span class="line">                <span class="keyword">if</span> x_user <span class="keyword">in</span> itemy_history:</span><br><span class="line">                    Ratings_diffs[itemx][itemy] += items_rating[itemy][x_user] - items_rating[itemx][x_user]</span><br><span class="line">                    N_set[itemx][itemy] += <span class="number">1</span></span><br><span class="line"><span class="keyword">for</span> itemx,ys <span class="keyword">in</span> Ratings_diffs.items():</span><br><span class="line">    <span class="keyword">for</span> itemy,rating <span class="keyword">in</span> ys.items():</span><br><span class="line">        Ratings_diffs[itemx][itemy] /= N_set[itemx][itemy]</span><br><span class="line"></span><br><span class="line">Ratings_diffs,N_set</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">(&#123;&#x27;A&#x27;: &#123;&#x27;B&#x27;: -0.2, &#x27;C&#x27;: 0.0, &#x27;D&#x27;: 0.2, &#x27;E&#x27;: 0.5&#125;,</span></span><br><span class="line"><span class="string">  &#x27;B&#x27;: &#123;&#x27;A&#x27;: 0.2, &#x27;C&#x27;: 0.2, &#x27;D&#x27;: 0.4, &#x27;E&#x27;: 0.25&#125;,</span></span><br><span class="line"><span class="string">  &#x27;C&#x27;: &#123;&#x27;A&#x27;: 0.0, &#x27;B&#x27;: -0.2, &#x27;D&#x27;: 0.2, &#x27;E&#x27;: 0.25&#125;,</span></span><br><span class="line"><span class="string">  &#x27;D&#x27;: &#123;&#x27;A&#x27;: -0.2, &#x27;B&#x27;: -0.4, &#x27;C&#x27;: -0.2, &#x27;E&#x27;: 0.0&#125;,</span></span><br><span class="line"><span class="string">  &#x27;E&#x27;: &#123;&#x27;A&#x27;: -0.5, &#x27;B&#x27;: -0.25, &#x27;C&#x27;: -0.25, &#x27;D&#x27;: 0.0&#125;&#125;,</span></span><br><span class="line"><span class="string"> &#123;&#x27;A&#x27;: &#123;&#x27;B&#x27;: 5, &#x27;C&#x27;: 5, &#x27;D&#x27;: 5, &#x27;E&#x27;: 4&#125;,</span></span><br><span class="line"><span class="string">  &#x27;B&#x27;: &#123;&#x27;A&#x27;: 5, &#x27;C&#x27;: 5, &#x27;D&#x27;: 5, &#x27;E&#x27;: 4&#125;,</span></span><br><span class="line"><span class="string">  &#x27;C&#x27;: &#123;&#x27;A&#x27;: 5, &#x27;B&#x27;: 5, &#x27;D&#x27;: 5, &#x27;E&#x27;: 4&#125;,</span></span><br><span class="line"><span class="string">  &#x27;D&#x27;: &#123;&#x27;A&#x27;: 5, &#x27;B&#x27;: 5, &#x27;C&#x27;: 5, &#x27;E&#x27;: 4&#125;,</span></span><br><span class="line"><span class="string">  &#x27;E&#x27;: &#123;&#x27;A&#x27;: 4, &#x27;B&#x27;: 4, &#x27;C&#x27;: 4, &#x27;D&#x27;: 4&#125;&#125;)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p>4.预测评分</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 预测评分</span></span><br><span class="line"><span class="comment"># 首先找出Alice交互过的物品哪些与要预测的物品有过”共同被统一用户评分“的经历，即存在倒排索引Ratings_item[x][y]</span></span><br><span class="line">A_history = users_rating[<span class="number">1</span>]</span><br><span class="line">candidate_items = <span class="built_in">set</span>()</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> Ratings_diffs[<span class="string">&#x27;E&#x27;</span>]:</span><br><span class="line">    <span class="keyword">if</span> item <span class="keyword">in</span> A_history:</span><br><span class="line">        candidate_items.add(item)</span><br><span class="line">weighted_score = <span class="number">0</span></span><br><span class="line">weighted_sum = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> Ratings_diffs[<span class="string">&#x27;E&#x27;</span>]:</span><br><span class="line">    weighted_sum += N_set[<span class="string">&#x27;E&#x27;</span>][item]</span><br><span class="line">    weighted_score += Ratings_diffs[<span class="string">&#x27;E&#x27;</span>][item] * N_set[<span class="string">&#x27;E&#x27;</span>][item]</span><br><span class="line">predict = weighted_score/weighted_sum</span><br><span class="line">predict</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">-0.25</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<h2 id="slope-one使用场景"><a href="#slope-one使用场景" class="headerlink" title="slope one使用场景"></a>slope one使用场景</h2><p>该算法适用于物品更新不频繁，数量相对较稳定并且物品数目明显小于用户数的场景。依赖用户的用户行为日志和物品偏好的相关内容。<br>优点：<br>1.算法简单，易于实现，执行效率高；<br>2.可以发现用户潜在的兴趣爱好；<br>缺点：<br>依赖用户行为，存在冷启动问题和稀疏性问题。</p>
<h2 id="比较Slope-One、SVD、协同过滤的精度，哪一个模型的RMSE评分更低？"><a href="#比较Slope-One、SVD、协同过滤的精度，哪一个模型的RMSE评分更低？" class="headerlink" title="比较Slope One、SVD、协同过滤的精度，哪一个模型的RMSE评分更低？"></a>比较Slope One、SVD、协同过滤的精度，哪一个模型的RMSE评分更低？</h2><div class="table-container">
<table>
<thead>
<tr>
<th>Model</th>
<th>RMSE</th>
</tr>
</thead>
<tbody>
<tr>
<td>UserCF</td>
<td>3.80369</td>
</tr>
<tr>
<td>ItemCF</td>
<td>3.74319</td>
</tr>
<tr>
<td>SVD(MF)</td>
<td>3.75332</td>
</tr>
<tr>
<td>Slope One</td>
<td>3.91533</td>
</tr>
</tbody>
</table>
</div>
<h1 id="参考资料："><a href="#参考资料：" class="headerlink" title="参考资料："></a>参考资料：</h1><p><a href="https://blog.csdn.net/xidianliutingting/article/details/51916578">https://blog.csdn.net/xidianliutingting/article/details/51916578</a></p>
]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>推荐系统基础</tag>
      </tags>
  </entry>
  <entry>
    <title>推荐系统基础5</title>
    <url>/2022/04/25/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%805/</url>
    <content><![CDATA[<hr>
<h1 id="任务5：矩阵分解SVD"><a href="#任务5：矩阵分解SVD" class="headerlink" title="任务5：矩阵分解SVD"></a>任务5：矩阵分解SVD</h1><ul>
<li>阅读<a href="https://github.com/datawhalechina/fun-rec/blob/master/docs/%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80/1.1%20%E5%9F%BA%E7%A1%80%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/1.1.3%20%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3.md">矩阵分解基础教程</a>，<a href="https://alyssaq.github.io/2015/20150426-simple-movie-recommender-using-svd/">代码实现</a></li>
<li>编写SVD用于电影推荐的流程</li>
<li>比较SVD与协同过滤的精度，哪一个模型的RMSE评分更低？</li>
</ul>
<p>代码地址： <a href="https://github.com/Guadzilla/Basics-of-Recsys">https://github.com/Guadzilla/Basics-of-Recsys</a></p>
<span id="more"></span>
<h2 id="1-隐语义模型与矩阵分解"><a href="#1-隐语义模型与矩阵分解" class="headerlink" title="1. 隐语义模型与矩阵分解"></a>1. 隐语义模型与矩阵分解</h2><p>隐语义模型（Latent factor model，简称LFM），是基于矩阵分解（Matrix Factorization，简称MF）的推荐算法。</p>
<p>协同过滤算法的特点就是完全没有利用到物品本身或者是用户自身的属性， 仅仅利用了用户与物品的交互信息就可以实现推荐，是一个可解释性很强， 非常直观的模型， 但是也存在一些问题， 第一个就是处理稀疏矩阵的能力比较弱， 所以<strong>为了使得协同过滤更好处理稀疏矩阵问题， 增强泛化能力</strong>， 从协同过滤中衍生出矩阵分解模型或者叫隐语义模型，两者差不多说的一个意思， 就是在协同过滤共现矩阵的基础上， 使用更稠密的隐向量表示用户和物品， 挖掘用户和物品的隐含兴趣和隐含特征， 在一定程度上弥补协同过滤模型处理稀疏矩阵能力不足的问题。下图直观展现了协同过滤算法和隐语义模型（矩阵分解算法）的区别。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220425130039276.png" alt="image-20220425130039276"></p>
<h2 id="2-隐语义模型"><a href="#2-隐语义模型" class="headerlink" title="2. 隐语义模型"></a>2. 隐语义模型</h2><p>隐语义模型最早在文本领域被提出，用于找到文本的隐含语义。在2006年， 被用于推荐中， <strong>它的核心思想是通过隐含特征（latent factor）联系用户和物品（item）， 基于用户的行为找出潜在的主题和分类</strong>。</p>
<p>我们下面拿一个音乐评分的例子来具体看一下隐含特征的含义。</p>
<p>假设每个用户都有自己的听歌偏好， 比如A喜欢带有<strong>小清新的</strong>， <strong>吉他伴奏的</strong>， <strong>王菲</strong>的歌曲，如果一首歌正好<strong>是王菲唱的， 并且是吉他伴奏的小清新</strong>， 那么就可以将这首歌推荐给这个用户。 也就是说是<strong>小清新， 吉他伴奏， 王菲</strong>这些元素连接起了用户和歌曲。 当然每个用户对不同的元素偏好不同， 每首歌包含的元素也不一样， 所以我们就希望找到下面的两个矩阵：</p>
<ol>
<li><strong>潜在因子—— 用户矩阵Q</strong> 这个矩阵表示不同用户对于不同元素的偏好程度， 1代表很喜欢， 0代表不喜欢， 比如下面这样：</li>
</ol>
<p><img src="https://camo.githubusercontent.com/bc712ed4c34be52a07689afee52c5024248c7cc330484e6d305de2ac799dbe3c/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f323032303038323232323032353936382e706e673f782d6f73732d70726f636573733d696d6167652f77617465726d61726b2c747970655f5a6d46755a33706f5a57356e6147567064476b2c736861646f775f31302c746578745f6148523063484d364c7939696247396e4c6d4e7a5a473475626d56304c336431656d6876626d6478615746755a773d3d2c73697a655f312c636f6c6f725f4646464646462c745f3730237069635f63656e746572" alt="在这里插入图片描述"></p>
<ol>
<li><strong>潜在因子——音乐矩阵P</strong> 表示每种音乐含有各种元素的成分， 比如下表中， 音乐A是一个偏小清新的音乐， 含有小清新的Latent Factor的成分是0.9， 重口味的成分是0.1， 优雅成分0.2…..</li>
</ol>
<p><img src="https://camo.githubusercontent.com/89d841394de2508f511e1a05b87499ed3987b2abdb7a5bf8933ec5a36cf089b5/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303230303832323232303735313339342e706e673f782d6f73732d70726f636573733d696d6167652f77617465726d61726b2c747970655f5a6d46755a33706f5a57356e6147567064476b2c736861646f775f31302c746578745f6148523063484d364c7939696247396e4c6d4e7a5a473475626d56304c336431656d6876626d6478615746755a773d3d2c73697a655f312c636f6c6f725f4646464646462c745f3730237069635f63656e746572" alt="在这里插入图片描述"></p>
<p><strong>利用上面的这两个矩阵， 我们就能得出张三对音乐A的喜欢程度：</strong></p>
<blockquote>
<p>张三对<strong>小清新</strong>的偏好 <em> 音乐A含有<strong>小清新</strong>的成分 + 张三对<strong>重口味</strong>的偏好 </em> 音乐A含有<strong>重口味</strong>的成分 + 张三对<strong>优雅</strong>的偏好 <em> 音乐A含有<em>*优雅</em></em>的成分….,</p>
</blockquote>
<p>下面是对应的两个隐向量：</p>
<p><img src="https://camo.githubusercontent.com/950a5f6a6bbcbb19c476595e18bbab5a441b09ae2f5d3ac19e77063ceef97fec/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303230303832323232313632373231392e706e673f782d6f73732d70726f636573733d696d6167652f77617465726d61726b2c747970655f5a6d46755a33706f5a57356e6147567064476b2c736861646f775f31302c746578745f6148523063484d364c7939696247396e4c6d4e7a5a473475626d56304c336431656d6876626d6478615746755a773d3d2c73697a655f312c636f6c6f725f4646464646462c745f3730237069635f63656e746572" alt="在这里插入图片描述"></p>
<p>根据隐向量其实就可以得到张三对音乐A的打分，即： <script type="math/tex">0.6 * 0.9 + 0.8 * 0.1 + 0.1 * 0.2 + 0.1 * 0.4 + 0.7 * 0 = 0.69</script> 按照这个计算方式， 每个用户对每首歌其实都可以得到这样的分数， 最后就得到了我们的评分矩阵：</p>
<p><img src="https://camo.githubusercontent.com/16fde22e8adaf3cf91458b50417ce0fa22239d474263e0b7106b7c45d437083e/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303230303832323232323134313233312e706e673f782d6f73732d70726f636573733d696d6167652f77617465726d61726b2c747970655f5a6d46755a33706f5a57356e6147567064476b2c736861646f775f31302c746578745f6148523063484d364c7939696247396e4c6d4e7a5a473475626d56304c336431656d6876626d6478615746755a773d3d2c73697a655f312c636f6c6f725f4646464646462c745f3730237069635f63656e746572" alt="在这里插入图片描述"></p>
<p>这里的红色表示用户没有打分，我们通过隐向量计算得到的。</p>
<p>上面例子中的<strong>小清晰， 重口味， 优雅，伤感，五月天</strong>这些就可以看做是隐含特征， 而通过这个隐含特征就可以把用户和音乐联系起来。其实就是找到了每个用户、每个音乐的一个 5 维的隐向量表达形式（类似embedding）。</p>
<p>用户隐向量的每个维度代表该用户对这个隐含特征的相关性（兴趣），音乐隐向量的每个维度代表该音乐与这个隐含特征的相关性。</p>
<p>于是隐向量就可以反映出用户的兴趣和物品的风格，并能将相似的物品推荐给相似的用户等。 <strong>有没有感觉到是把协同过滤算法进行了一种延伸， 把用户的相似性和物品的相似性通过了一个叫做隐向量的方式进行表达</strong>。</p>
<p>但是， 真实的情况下我们其实是没有上面那两个矩阵的， 音乐那么多， 用户那么多， 我们没有办法去找一些隐特征去表示出这些东西， 另外一个问题就是即使能表示也不一定准， 对于每个用户或者每个物品的风格，我们每个人都有不同的看法。 所以事实上， 我们有的只有用户的评分矩阵， 也叫“共现矩阵”， 一般这种矩阵长这样：</p>
<p><img src="https://camo.githubusercontent.com/eda2821a1bbb617ec7f25e57e6664891cf538106d0ec85f64e8321de32458ecc/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303230303832323232333331333334392e706e673f782d6f73732d70726f636573733d696d6167652f77617465726d61726b2c747970655f5a6d46755a33706f5a57356e6147567064476b2c736861646f775f31302c746578745f6148523063484d364c7939696247396e4c6d4e7a5a473475626d56304c336431656d6876626d6478615746755a773d3d2c73697a655f312c636f6c6f725f4646464646462c745f3730237069635f63656e746572" alt="user-item评分矩阵"></p>
<p>上面说到过，如果直接用协同过滤算法去填充这个矩阵，很容易头部效应明显、泛化能力弱、长尾效应明显的问题。</p>
<p>如何才能将这个矩阵转化成我们想要的隐语义模型呢？矩阵分解算法实现了这一点。</p>
<h2 id="3-矩阵分解算法"><a href="#3-矩阵分解算法" class="headerlink" title="3. 矩阵分解算法"></a>3. 矩阵分解算法</h2><p>矩阵分解算法其实就是在<strong>想办法基于这个评分矩阵去找到上面例子中的那两个矩阵， 也就是用户兴趣和物品的隐向量表达。 具体做法是：将评分矩阵分解成Q和P两个矩阵乘积的形式， 这时候就可以基于这两个矩阵去预测某个用户对某个物品的评分了。 然后基于这个评分去进行推荐</strong>。这就是矩阵分解算法的原理。</p>
<h3 id="矩阵分解算法原理"><a href="#矩阵分解算法原理" class="headerlink" title="矩阵分解算法原理"></a>矩阵分解算法原理</h3><p>在矩阵分解的算法框架下， <strong>我们可以通过分解协同过滤的共现矩阵来得到用户和物品的隐向量</strong>， 就是上面的用户矩阵Q和物品矩阵P， 这也是“矩阵分解”名字的由来。</p>
<p><img src="https://camo.githubusercontent.com/e09db5bdf02c408d2231e52318d348d4aec26e864d22ab1a158136ee27de9da3/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303230303832333130313531333233332e706e673f782d6f73732d70726f636573733d696d6167652f77617465726d61726b2c747970655f5a6d46755a33706f5a57356e6147567064476b2c736861646f775f31302c746578745f6148523063484d364c7939696247396e4c6d4e7a5a473475626d56304c336431656d6876626d6478615746755a773d3d2c73697a655f312c636f6c6f725f4646464646462c745f3730237069635f63656e746572" alt="在这里插入图片描述"></p>
<p>矩阵分解算法将 $m\times n$ 维的共现矩阵 $R$ 分解成 $m \times k$ 维的用户矩阵 $U$ 和 $k \times n$ 维的物品矩阵 $V$ 相乘的形式。 其中 $m$ 是用户数量，  $n$ 是物品数量， $k$ 是隐向量维度， 也就是隐含特征个数， 只不过这里的隐含特征变得<strong>不可解释</strong>了， <strong>即我们不知道具体含义了</strong>， 要模型自己去学。$k$ 的大小决定了隐向量表达能力的强弱， $k$ 越大， 表达信息的能力就越强， 理解起来就是把隐含特征分类划分的越具体。</p>
<p>那么如果有了用户矩阵和物品矩阵的话， 我们就能计算用户 $u$ 对物品 $i$ 的评分， 只需要 </p>
<script type="math/tex; mode=display">
\operatorname{Preference}(u, i)=r_{u i}=p_{u}^{T} q_{i}=\sum_{f=1}^{F} p_{u, k} q_{k,i}</script><p>（熟悉推荐的同学应该对这个表达式非常熟悉，几乎所有预测得分都是这个计算得来的。）这里的 ${p_u}$ 就是用户 $u$ 的隐向量， 就类似与上面的张三向量， 注意这是列向量， $q_i$ 是物品 $i$ 的隐向量， 就类似于上面的音乐A向量， 这个也是列向量， 所以才用了 $p_{u}^{T} q_{i}$ 得到了一个标量， 也就是用户的最终评分， 计算过程其实和上面例子中一样。 这里的 $p_{u,k}$ 和 $q_{i,k}$ 是模型的参数， 也正是我们想办法要计算的， $p_{u,k}$度量的是用户 $u$ 对第 $k$ 个隐含特征的兴趣， 而 $q_{i,k}$ 度量了物品 $i$ 与第 $k$ 个隐含特征联系。</p>
<h3 id="矩阵分解算法的求解"><a href="#矩阵分解算法的求解" class="headerlink" title="矩阵分解算法的求解"></a>矩阵分解算法的求解</h3><p>对矩阵进行矩阵分解的主要方法有三种：特征值分解（Eigen Decomposition）、奇异值分解（Singular Value Decomposition，SVD）和梯度下降（Gradient Descent）。</p>
<p>其中，特征值分解只能作用于方阵，显然不适用于分解用户-物品矩阵。</p>
<p>传统的SVD分解的具体描述如下：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220425134607188.png" alt="image-20220425134607188"></p>
<p>可以说，奇异值分解似乎完美地解决了矩阵分解的问题，但其存在两点缺陷，使其不宜作为互联网场景下矩阵分解的主要方法。（1）SVD分解；（2）SVD分解的计算复杂度达到了 $O(mn^2)$ 的级别，这对于商品数量动辄上百万、用户数量往往上千万的互联网场景来说几乎是不可接受的。原理详解参考文章：<a href="https://blog.csdn.net/wuzhongqiang/article/details/108168238">奇异值分解(SVD)的原理详解及推导</a></p>
<p>由于上述两个原因，传统奇异值分解也不适用于解决大规模稀疏矩阵的矩阵分解问题。因此，梯度下降法成了进行矩阵分解的主要方法，这里对其进行具体的介绍。</p>
<h4 id="梯度下降法：Funk-SVD"><a href="#梯度下降法：Funk-SVD" class="headerlink" title="梯度下降法：Funk-SVD"></a>梯度下降法：Funk-SVD</h4><p>2006年的Netflix Prize之后， Simon Funk公布了一个矩阵分解算法叫做<strong>Funk-SVD</strong>, 后来被Netflix Prize的冠军Koren称为<strong>Latent Factor Model(LFM)</strong>。 Funk-SVD的思想很简单： <strong>把求解上面两个矩阵的参数问题转换成一个最优化问题， 可以通过训练集里面的观察值利用最小化来学习用户矩阵和物品矩阵</strong>。</p>
<p>我们上面已经知道， 如果有了用户矩阵和物品矩阵的话， 就能计算用户 $u$ 对物品 $i$ 的评分： <script type="math/tex">\operatorname{Preference}(u, i)=r_{u i}=p_{u}^{T} q_{i}</script> </p>
<p>而现在， 我们有真实的 $r_{u,i}$ , 但是没有 $p_{u}^{T} q_{i}$ ，那么我们可以初始化一个啊， 随机初始化一个用户矩阵 $U$ 和一个物品矩阵 $V$ ， 然后不就有 $p_{u}^{T} q_{i}$ 了？ 当然你说， 随机初始化的肯定不准啊， 但是， 有了 $p_{u}^{T} q_{i}$ 之后， 我们就可以计算一个预测值 $\hat{r}_{u i}$ , 即 <script type="math/tex">\hat{r}_{u i}=p_{u}^{T} q_{i}</script></p>
<p>一开始预测肯定是不准的， 那么这个预测的和真实值之间就会有一个误差： <script type="math/tex">e_{u i}=r_{u i}-\hat{r}_{u i}</script> </p>
<p>于是可以得到一个优化目标，即总的误差平方和： <script type="math/tex">\operatorname{SSE}=\sum_{u, i} e_{u i}^{2}=\sum_{u, i}\left(r_{u i}-\sum_{k=1}^{K} p_{u,k}· q_{k, i}\right)^{2}</script> </p>
<p>我们想办法进行训练，把 SSE 降到最小，那么我们的两个矩阵参数就可以算出来。</p>
<p>所以就把这个问题转成了最优化的的问题， 可以利用非常标准的梯度下降过程完成。</p>
<p>（1）确定目标函数。我们的目标函数就是：</p>
<script type="math/tex; mode=display">
\min _{\boldsymbol{q}^*, \boldsymbol{p}^*} \sum_{(u, i) \in K}\left(\boldsymbol{r}_{\mathrm{ui}}-p_{u}^{T} q_{i}\right)^{2}</script><p>这里的 $K$ 表示所有用户评分样本的集合。为了减少过拟合现象，加入正则化项后，目标函数如下：</p>
<script type="math/tex; mode=display">
\min _{\boldsymbol{q}^*, \boldsymbol{p}^*} \sum_{(u, i) \in K}\left(\boldsymbol{r}_{\mathrm{ui}}-p_{u}^{T} q_{i}\right)^{2} + \lambda ·(||q_i||^2+||p_u||^2)</script><p>（2）对目标函数求偏导，求取梯度下降的方向和幅度</p>
<p>先对 $p_u$ 和 $q_i$ 求偏导</p>
<script type="math/tex; mode=display">
\operatorname{Loss} =\sum_{(u, i) \in K}\left(\boldsymbol{r}_{\mathrm{ui}}-p_{u}^{T} q_{i}\right)^{2} + \lambda ·(||q_i||^2+||p_u||^2)\\
{\frac{\partial Loss}{\partial p_u} } = -2\ *\ (\boldsymbol{r}_{\mathrm{ui}}-p_{u}^{T} q_{i}) q_i\ +\ 2\lambda p_u \\
{\frac{\partial Loss}{\partial q_i} } = -2\ *\ (\boldsymbol{r}_{\mathrm{ui}}-p_{u}^{T} q_{i}) p_u \ +\ 2\lambda q_i \\</script><p>（3）利用第 2 步的求导结果，沿梯度的反方向更新参数：</p>
<script type="math/tex; mode=display">
q_i = q_i + \gamma ((\boldsymbol{r}_{\mathrm{ui}}-p_{u}^{T} q_{i}) q_i\ -\ \lambda p_u )\\
p_u = p_u + \gamma ((\boldsymbol{r}_{\mathrm{ui}}-p_{u}^{T} q_{i}) p_u \ -\ \lambda q_i)</script><p>其中 $\gamma$ 是学习率，原来的系数 2 可以省略，因为仅仅是放缩学习率。 </p>
<p>（4）当迭代次数超过上限次数 $n$ ，或者损失低于阈值 $\theta$ 时，结束训练，否则循环第三步。</p>
<p>在完成矩阵分解过程后，即可得到所有用户和物品的隐向量。在对某用户进行推荐时，可利用该用户的隐向量与所有物品的隐向量进行逐一的内积运算，得出该用户对所有物品的评分预测，再依次进行排序，得到最终的推荐列表。</p>
<p>在了解了矩阵分解的原理之后，就可以更清楚地解释为什么矩阵分解相较协同过滤有更强的泛化能力。在矩阵分解算法中，由于隐向量的存在，使任意的用户和物品之间都可以得到预测分值。而隐向量的生成过程其实是<strong>对共现矩阵进行全局拟合</strong>的过程，因此隐向量其实是利用全局信息生成的，有<strong>更强的泛化能力</strong>；而对协同过滤来说，如果两个用户没有相同的历史行为，两个物品没有相同的人购买，那么这两个用户和两个物品的相似度都将为 0（因为协同过滤只能利用用户和物品自己的信息进行相似度计算，这就使协同过滤不具备泛化利用全局信息的能力）。</p>
<h5 id="消除打分偏差"><a href="#消除打分偏差" class="headerlink" title="消除打分偏差"></a>消除打分偏差</h5><p>但在实际中， 单纯的 $\hat{r}_{u i}=p_{u}^{T} q_{i}$ 也是不够的， 还要考虑其他的一些因素， 比如一个评分系统， 有些固有的属性和用户物品无关， 而用户也有些属性和物品无关， 物品也有些属性和用户无关。 因此， Netfix Prize中提出了另一种LFM， 在原来的基础上加了偏置项， 来消除用户和物品打分的偏差， 即预测公式如下： <script type="math/tex">\hat{r}_{u i}=\mu+b_{u}+b_{i}+p_{u}^{T} \cdot q_{i}</script> 这个预测公式加入了3项偏置 $\mu,b_u,b_i$，作用如下：</p>
<ul>
<li>$\mu$：训练集中所有记录的评分的全局平均数。 在不同网站中， 因为网站定位和销售物品不同， 网站的整体评分分布也会显示差异。 比如有的网站中用户就喜欢打高分， 有的网站中用户就喜欢打低分。 而全局平均数可以表示网站本身对用户评分的影响。</li>
<li>$b_u$：用户偏差系数， 可以使用用户 $u$ 给出的所有评分的均值， 也可以当做训练参数。 这一项表示了用户的评分习惯中和物品没有关系的那种因素。 比如有些用户比较苛刻， 对什么东西要求很高， 那么他评分就会偏低， 而有些用户比较宽容， 对什么东西都觉得不错， 那么评分就偏高。</li>
<li>$b_i$：物品偏差系数， 可以使用物品 $i$ 收到的所有评分的均值， 也可以当做训练参数。 这一项表示了物品接受的评分中和用户没有关系的因素。 比如有些物品本身质量就很高， 因此获得的评分相对比较高， 有的物品本身质量很差， 因此获得的评分相对较低。</li>
</ul>
<p>加了用户和物品的打分偏差之后， 矩阵分解得到的隐向量更能反映不同用户对不同物品的“真实”态度差异， 也就更容易捕捉评价数据中有价值的信息， 从而避免推荐结果有偏。 注意此时的目标函数会发生变化： </p>
<script type="math/tex; mode=display">
\min _{\boldsymbol{q}^{*}, \boldsymbol{p}^{*}, \boldsymbol{b}^{*}} \sum_{(\mathrm{u}, \mathrm{i}) \in K}\left(\boldsymbol{r}_{\mathrm{ui}}-\mu-b_{\mathrm{u}}-b_{\mathrm{i}}-\boldsymbol{p}_{\mathrm{u}}^{\mathrm{T}} \boldsymbol{q}_{\mathrm{i}}\right)^{2}+\lambda\left(\left\|\boldsymbol{p}_{\mathrm{u}}\right\|^{2}+\left\|\boldsymbol{q}_{\mathrm{i}}\right\|^{2}+b_{\mathrm{u}}^{2}+b_{\mathrm{i}}^{2}\right)</script><p>此时如果把 $b_u$ 和 $b_i$ 当做训练参数的话， 那么它俩的梯度是：</p>
<script type="math/tex; mode=display">
\frac{\partial Loss}{\partial b_{u}}=-e_{u i}+\lambda b_{u} \\ \frac{\partial Loss}{\partial b_{i}}=-e_{u i}+\lambda b_{i}</script><p>更新公式为：</p>
<script type="math/tex; mode=display">
b_u = b_u + \gamma (e_{ui}-\ \lambda p_u )\\
p_u = p_u + \gamma (e_{ui}-\ \lambda q_i)</script><p>而对于 $p_{u}$ 和$q_{i}$， 导数没有变化， 更新公式也没有变化。</p>
<h3 id="矩阵分解算法的代码实现"><a href="#矩阵分解算法的代码实现" class="headerlink" title="矩阵分解算法的代码实现"></a>矩阵分解算法的代码实现</h3><p>我们这里用代码实现一下上面的算法来预测上一篇文章里面的那个预测 Alice 对物品 5 的评分， 看看矩阵分解到底是怎么进行预测或者是推荐的。 我把之前的例子拿过来：</p>
<p><img src="https://camo.githubusercontent.com/68d8995d1a9bacf4e58fa39359de71cbb99e3bf5abc5174bd1033e8b93fdae81/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303230303832373135303233373932312e706e67237069635f63656e746572" alt="在这里插入图片描述"></p>
<p>任务就是根据这个评分矩阵， 猜测Alice对物品5的打分。</p>
<p>在实现 MF 之前， 先来回忆一下 ItemCF 和 UserCF 对于这个问题的做法：</p>
<p>首先 ItemCF 的做法， 根据已有的用户打分计算物品之间的相似度， 得到物品的相似度矩阵， 根据这个相似度矩阵， 选择出前 K 个与物品 5 最相似的物品， 然后基于 Alice 对这 K 个物品的得分， 猜测 Alice 对物品 5 的得分， 有一个加权的计算公式。 </p>
<p>UserCF的做法，根据用户对其他物品的打分， 计算用户之间的相似度， 选择出与Alice最相近的K个用户， 然后基于那K个用户对物品5的打分计算出Alice对物品5的打分。 </p>
<p>但是，这两种方式有个问题， 就是如果矩阵非常稀疏的话， 当然这个例子是个特例， 一般矩阵都是非常稀疏的， 那么预测效果就不好， 因为两个相似用户对同一物品打分的概率以及 Alice 同时对两个相似物品打分的概率可能都比较小。 另外， 这两种方法显然没有考虑到全局的物品或者用户， 只是基于了最相似的例子， 很可能有偏。</p>
<p>那么 MF 在解决这个问题上是这么做的：</p>
<ol>
<li>首先， 它会先初始化用户矩阵 P 和物品矩阵 Q ，  P 的维度是<code>[users_num, K]</code>, Q的维度是<code>[item_nums, K]</code>， 这个 K 是隐向量的维度。 也就是把通过隐向量的方式把用户的兴趣和物品的特点关联了起来。 初始化这两个矩阵的方式很多， 但根据经验， 随机数需要和 $\frac{1}{\sqrt{K}}$ 成正比。 下面代码中会发现。</li>
<li>有了两个矩阵之后， 我就可以根据用户已经打分的数据去更新参数， 这就是训练模型的过程， 方法很简单， 就是遍历用户， 对于每个用户， 遍历它打分的物品， 这样就拿到了该用户和物品的隐向量， 然后两者相乘加上偏置就是预测的评分， 这时候与真实评分有个差距， 根据上面的梯度下降就可以进行参数的更新</li>
</ol>
<p>这样训练完之后， 我们就可以得到用户 Alice 和物品 5 的隐向量， 根据这个就可以预测 Alice 对物品 5 的打分。下面的代码的逻辑就是上面这两步， 这里使用带有偏置项和正则项的 MF 算法：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SVD</span>():</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, rating_data, F=<span class="number">5</span>, alpha=<span class="number">0.1</span>, lmbda=<span class="number">0.1</span>, max_iter=<span class="number">100</span></span>):</span></span><br><span class="line">        self.F = F           <span class="comment"># 这个表示隐向量的维度</span></span><br><span class="line">        self.P = <span class="built_in">dict</span>()          <span class="comment">#  用户矩阵P  大小是[users_num, F]</span></span><br><span class="line">        self.Q = <span class="built_in">dict</span>()     <span class="comment"># 物品矩阵Q  大小是[item_nums, F]</span></span><br><span class="line">        self.bu = <span class="built_in">dict</span>()   <span class="comment"># 用户偏差系数</span></span><br><span class="line">        self.bi = <span class="built_in">dict</span>()    <span class="comment"># 物品偏差系数</span></span><br><span class="line">        self.mu = <span class="number">1.0</span>        <span class="comment"># 全局偏差系数</span></span><br><span class="line">        self.alpha = alpha   <span class="comment"># 学习率</span></span><br><span class="line">        self.lmbda = lmbda    <span class="comment"># 正则项系数</span></span><br><span class="line">        self.max_iter = max_iter    <span class="comment"># 最大迭代次数</span></span><br><span class="line">        self.rating_data = rating_data <span class="comment"># 评分矩阵</span></span><br></pre></td></tr></table></figure>
<p>​        </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">    <span class="comment"># 初始化矩阵P和Q, 方法很多， 一般用随机数填充， 但随机数大小有讲究， 根据经验， 随机数需要和1/sqrt(F)成正比</span></span><br><span class="line">    cnt = <span class="number">0</span>    <span class="comment"># 统计总的打分数， 初始化mu用</span></span><br><span class="line">    <span class="keyword">for</span> user, items <span class="keyword">in</span> self.rating_data.items():</span><br><span class="line">        self.P[user] = [random.random() / math.sqrt(self.F)  <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, F)]</span><br><span class="line">        self.bu[user] = <span class="number">0</span></span><br><span class="line">        cnt += <span class="built_in">len</span>(items) </span><br><span class="line">        <span class="keyword">for</span> item, rating <span class="keyword">in</span> items.items():</span><br><span class="line">            <span class="keyword">if</span> item <span class="keyword">not</span> <span class="keyword">in</span> self.Q:</span><br><span class="line">                self.Q[item] = [random.random() / math.sqrt(self.F) <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, F)]</span><br><span class="line">                self.bi[item] = <span class="number">0</span></span><br><span class="line">    self.mu /= cnt</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 有了矩阵之后， 就可以进行训练, 这里使用随机梯度下降的方式训练参数P和Q</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(self.max_iter):</span><br><span class="line">        <span class="keyword">for</span> user, items <span class="keyword">in</span> self.rating_data.items():</span><br><span class="line">            <span class="keyword">for</span> item, rui <span class="keyword">in</span> items.items():</span><br><span class="line">                rhat_ui = self.predict(user, item)   <span class="comment"># 得到预测评分</span></span><br><span class="line">                <span class="comment"># 计算误差</span></span><br><span class="line">                e_ui = rui - rhat_ui</span><br><span class="line">                </span><br><span class="line">                self.bu[user] += self.alpha * (e_ui - self.lmbda * self.bu[user])</span><br><span class="line">                self.bi[item] += self.alpha * (e_ui - self.lmbda * self.bi[item])</span><br><span class="line">                <span class="comment"># 随机梯度下降更新梯度</span></span><br><span class="line">                <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, self.F):</span><br><span class="line">                    self.P[user][k] += self.alpha * (e_ui*self.Q[item][k] - self.lmbda * self.P[user][k])</span><br><span class="line">                    self.Q[item][k] += self.alpha * (e_ui*self.P[user][k] - self.lmbda * self.Q[item][k])</span><br><span class="line">                </span><br><span class="line">        self.alpha *= <span class="number">0.1</span>    <span class="comment"># 每次迭代步长要逐步缩小</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测user对item的评分， 这里没有使用向量的形式</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">self, user, item</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span>(self.P[user][f] * self.Q[item][f] <span class="keyword">for</span> f <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, self.F)) + self.bu[user] + self.bi[item] + self.mu   </span><br></pre></td></tr></table></figure>
<p>下面我建立一个字典来存放数据， 之所以用字典， 是因为很多时候矩阵非常的稀疏， 如果用pandas的话， 会出现很多Nan的值， 反而不好处理。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义数据集， 也就是那个表格， 注意这里我们采用字典存放数据， 因为实际情况中数据是非常稀疏的， 很少有情况是现在这样</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadData</span>():</span></span><br><span class="line">    rating_data=&#123;<span class="number">1</span>: &#123;<span class="string">&#x27;A&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;B&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;C&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;D&#x27;</span>: <span class="number">4</span>&#125;,</span><br><span class="line">           <span class="number">2</span>: &#123;<span class="string">&#x27;A&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;B&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;C&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;D&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;E&#x27;</span>: <span class="number">3</span>&#125;,</span><br><span class="line">           <span class="number">3</span>: &#123;<span class="string">&#x27;A&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;B&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;C&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;D&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;E&#x27;</span>: <span class="number">5</span>&#125;,</span><br><span class="line">           <span class="number">4</span>: &#123;<span class="string">&#x27;A&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;B&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;C&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;D&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;E&#x27;</span>: <span class="number">4</span>&#125;,</span><br><span class="line">           <span class="number">5</span>: &#123;<span class="string">&#x27;A&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;B&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;C&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;D&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;E&#x27;</span>: <span class="number">1</span>&#125;</span><br><span class="line">          &#125;</span><br><span class="line">    <span class="keyword">return</span> rating_data</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 接下来就是训练和预测</span></span><br><span class="line">rating_data = loadData()</span><br><span class="line">basicsvd = SVD(rating_data, F=<span class="number">10</span>)</span><br><span class="line">basicsvd.train()</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> [<span class="string">&#x27;E&#x27;</span>]:</span><br><span class="line">    <span class="built_in">print</span>(item, basicsvd.predict(<span class="number">1</span>, item))</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 结果：</span></span><br><span class="line">E <span class="number">3.252210242858994</span></span><br></pre></td></tr></table></figure>
<p>通过这个方式， 得到的预测评分是3.25， 这个和隐向量的维度， 训练次数和训练方式有关， 这里只说一下这个东西应该怎么用， 具体结果可以不用纠结。</p>
<p><a href="https://github.com/Guadzilla/recpre/tree/master/task5">pytorch的代码实现</a>已给出。</p>
<h2 id="比较SVD与协同过滤的精度，哪一个模型的RMSE评分更低？"><a href="#比较SVD与协同过滤的精度，哪一个模型的RMSE评分更低？" class="headerlink" title="比较SVD与协同过滤的精度，哪一个模型的RMSE评分更低？"></a>比较SVD与协同过滤的精度，哪一个模型的RMSE评分更低？</h2><p>$RMSE=\sqrt{\frac{1}{m}\sum^m_{i=1}(f(x_i)-y_i)^2}$</p>
<p>用基于皮尔逊相似度的 ItemCF 协同过滤算法，推荐的 TopN 物品的评分与对应真实评分计算 RMSE 。</p>
<p>SVD 算法可以计算所有用户对所有物品的评分，所以计算所有验证集的预测得分与对应真实评分计算 RMSE。</p>
<h1 id="课后思考"><a href="#课后思考" class="headerlink" title="课后思考"></a>课后思考</h1><ol>
<li><p>矩阵分解算法后续有哪些改进呢？针对这些改进，是为了解决什么的问题呢？请大家自行探索</p>
<p>SVD，计算复杂度太高。Funk-SVD，用梯度下降法，大大减少复杂度。BiasSVD，消除用户和物品打分偏差。SVD++，考虑邻域影响。TSVD，加入时间信息。</p>
</li>
<li><p>矩阵分解的优缺点分析</p>
<ul>
<li>优点：<ul>
<li>泛化能力强： 一定程度上解决了稀疏问题</li>
<li>空间复杂度低： 由于用户和物品都用隐向量的形式存放， 少了用户和物品相似度矩阵， 空间复杂度由$n^2$降到了$(n+m)*f$</li>
<li>更好的扩展性和灵活性：矩阵分解的最终产物是用户和物品隐向量， 这个深度学习的embedding思想不谋而合， 因此矩阵分解的结果非常便于与其他特征进行组合和拼接， 并可以与深度学习无缝结合。</li>
</ul>
</li>
</ul>
<p>但是， 矩阵分解算法依然是只用到了评分矩阵， 没有考虑到用户特征， 物品特征和上下文特征， 这使得矩阵分解丧失了利用很多有效信息的机会， 同时在缺乏用户历史行为的时候， 无法进行有效的推荐。 所以为了解决这个问题， <strong>逻辑回归模型及后续的因子分解机模型</strong>， 凭借其天然的融合不同特征的能力， 逐渐在推荐系统领域得到了更广泛的应用。</p>
</li>
</ol>
<h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ul>
<li>王喆 - 《深度学习推荐系统》</li>
<li>项亮 - 《推荐系统实战》</li>
<li><a href="https://blog.csdn.net/wuzhongqiang/article/details/108168238">奇异值分解(SVD)的原理详解及推导</a></li>
<li><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=5197422&amp;tag=1">Matrix factorization techniques for recommender systems论文</a></li>
<li><a href="https://blog.csdn.net/wuzhongqiang/article/details/108173885">隐语义模型(LFM)和矩阵分解(MF)</a></li>
</ul>
]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>推荐系统基础</tag>
      </tags>
  </entry>
  <entry>
    <title>推荐系统基础7</title>
    <url>/2022/04/27/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%807/</url>
    <content><![CDATA[<hr>
<h1 id="任务7：词向量基础"><a href="#任务7：词向量基础" class="headerlink" title="任务7：词向量基础"></a>任务7：词向量基础</h1><ul>
<li>学习<a href="https://cloud.tencent.com/developer/article/1486055">word2vec基础</a></li>
<li>将用户历史观看电影转为列表数据（一个用户一个列表）</li>
<li>使用gensim训练word2vec，然后对用户完成聚类</li>
</ul>
<p>代码地址： <a href="https://github.com/Guadzilla/Basics-of-Recsys">https://github.com/Guadzilla/Basics-of-Recsys</a></p>
<span id="more"></span>
<h1 id="学习资料"><a href="#学习资料" class="headerlink" title="学习资料"></a>学习资料</h1><p>来源 | Analytics Vidhya 【磐创AI导读】：这篇文章主要介绍了如何使用word2vec构建推荐系统。想要获取更多的机器学习、深度学习资源，欢迎大家点击上方蓝字关注我们的公众号：磐创AI。</p>
<h2 id="概览"><a href="#概览" class="headerlink" title="概览"></a><strong>概览</strong></h2><ul>
<li>如今，推荐引擎无处不在，人们希望数据科学家知道如何构建一个推荐引擎</li>
<li>Word2vec是一个非常流行的词嵌入，用于执行各种NLP任务</li>
<li>我们将使用word2vec来构建我们自己的推荐系统。就让我们来看看NLP和推荐引擎是如何结合的吧！</li>
</ul>
<p>完整的代码可以从这里下载：</p>
<blockquote>
<p><a href="https://github.com/prateekjoshi565/recommendation_system/blob/master/recommender_2.ipynb">https://github.com/prateekjoshi565/recommendation_system/blob/master/recommender_2.ipynb</a></p>
</blockquote>
<h2 id="介绍"><a href="#介绍" class="headerlink" title="　介绍"></a>　<strong>介绍</strong></h2><p>老实说，你在亚马逊上有注意到网站为你推荐的内容吗（Recommended for you部分)? 自从几年前我发现机器学习可以增强这部分内容以来，我就迷上了它。每次登录Amazon时，我都会密切关注该部分。</p>
<p>Netflix、谷歌、亚马逊、Flipkart等公司花费数百万美元完善他们的推荐引擎是有原因的，因为这是一个强大的信息获取渠道并且提高了消费者的体验。</p>
<p>让我用一个最近的例子来说明这种作用。我去了一个很受欢迎的网上市场购买一把躺椅，那里有各种各样的躺椅，我喜欢其中的大多数并点击了查看了一把人造革手动躺椅。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/aurj300r0x.jpeg" alt="img"></p>
<p>请注意页面上显示的不同类型的信息，图片的左半部分包含了不同角度的商品图片。右半部分包含有关商品的一些详细信息和部分类似的商品。</p>
<p>而这是我最喜欢的部分，该网站正在向我推荐类似的商品，这为我节省了手动浏览类似躺椅的时间。</p>
<p>在本文中，我们将构建自己的推荐系统。但是我们将从一个独特的视角来处理这个问题。我们将使用一个NLP概念—Word2vec,向用户推荐商品。如果你觉得这个教程让你有点小期待，那就让我们开始吧！</p>
<p>在文中，我会提及一些概念。我建议可以看一下以下这两篇文章来快速复习一下</p>
<blockquote>
<p><a href="https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/?utm_source=blog&amp;utm_medium=how-to-build-recommendation-system-word2vec-python">理解神经网络:</a></p>
<p><a href="https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-recommendation-engine-python/?utm_source=blog&amp;utm_medium=how-to-build-recommendation-system-word2vec-python">构建推荐引擎的综合指南 </a></p>
</blockquote>
<h2 id="word2vec-词的向量表示"><a href="#word2vec-词的向量表示" class="headerlink" title="word2vec - 词的向量表示"></a><strong>word2vec - 词的向量表示</strong></h2><p>我们知道机器很难处理原始文本数据。事实上，除了数值型数据，机器几乎不可能处理其他类型的数据。因此，以向量的形式表示文本几乎一直是所有NLP任务中最重要的步骤。</p>
<p>在这个方向上，最重要的步骤之一就是使用 word2vec embeddings，它是在2013年引入NLP社区的并彻底改变了NLP的整个发展。</p>
<p>事实证明，这些 embeddings在单词类比和单词相似性等任务中是最先进的。word2vec embeddings还能够实现像 <code>King - man +woman ~= Queen</code>之类的任务，这是一个非常神奇的结果。</p>
<p>有两种⁠word2vec模型——Continuous Bag of Words模型和Skip-Gram模型。在本文中，我们将使用Skip-Gram模型。</p>
<p>首先让我们了解word2vec向量或者说embeddings是怎么计算的。</p>
<h2 id="如何获得word2vec-embeddings"><a href="#如何获得word2vec-embeddings" class="headerlink" title="如何获得word2vec embeddings?"></a><strong>如何获得word2vec embeddings?</strong></h2><p>word2vec模型是一个简单的神经网络模型，其只有一个隐含层，该模型的任务是预测句子中每个词的近义词。然而，我们的目标与这项任务无关。我们想要的是一旦模型被训练好，通过模型的<strong>隐含层学习到的权重</strong>。然后可以将这些权重用作单词的embeddings。</p>
<p>让我举个例子来说明word2vec模型是如何工作的。请看下面这句话:</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/39q26nmjqb.png" alt="img"></p>
<p>假设单词“teleport”(用黄色高亮显示)是我们的输入单词。它有一个大小为2的上下文窗口。这意味着我们只考虑输入单词两边相邻的两个单词作为邻近的单词。</p>
<p><em>注意:上下文窗口的大小不是固定的，可以根据我们的需要进行更改。</em></p>
<p>现在，任务是逐个选择邻近的单词(上下文窗口中的单词)，并给出词汇表中每个单词成为选中的邻近单词的概率。这听起来应该挺直观的吧？</p>
<p>让我们再举一个例子来详细了解整个过程。</p>
<h4 id="准备训练数据"><a href="#准备训练数据" class="headerlink" title="准备训练数据"></a><strong>准备训练数据</strong></h4><p>我们需要一个标记数据集来训练神经网络模型。这意味着数据集应该有一组输入和对应输入的输出。在这一点上，你可能有一些问题，像:</p>
<ul>
<li>在哪里可以找到这样的数据集?</li>
<li>这个数据集包含什么?</li>
<li>这个数据有多大?</li>
</ul>
<p>等等。</p>
<p>然而我要告诉你的是：我们可以轻松地创建自己的标记数据来训练word2vec模型。下面我将演示如何从任何文本生成此数据集。让我们使用一个句子并从中创建训练数据。</p>
<p><strong>第一步</strong>: 黄色高亮显示的单词将作为输入，绿色高亮显示的单词将作为输出单词。我们将使用2个单词的窗口大小。让我们从第一个单词作为输入单词开始。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/uqzmaohs5l.png" alt="img"></p>
<p>所以，关于这个输入词的训练样本如下:</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/jikh7horxy.png" alt="img"></p>
<p><strong>第二步</strong>: 接下来，我们将第二个单词作为输入单词。上下文窗口也会随之移动。现在，邻近的单词是“we”、“become”和“what”。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/rrq9cfy6m9.png" alt="img"></p>
<p>新的训练样本将会被添加到之前的训练样本中，如下所示:</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/q4sl5f7xlo.png" alt="img"></p>
<p>我们将重复这些步骤，直到最后一个单词。最后，这句话的完整训练数据如下:</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/djmiyovdx5.png" alt="img"></p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/sflnd9gu81.png" alt="img"></p>
<p>我们从一个句子中抽取了27个训练样本，这是我喜欢处理非结构化数据的许多方面之一——凭空创建了一个标记数据集。</p>
<h4 id="获得-word2vec-Embeddings"><a href="#获得-word2vec-Embeddings" class="headerlink" title="获得 word2vec Embeddings"></a><strong>获得 word2vec Embeddings</strong></h4><p>现在，假设我们有一堆句子，我们用同样的方法从这些句子中提取训练样本。我们最终将获得相当大的训练数据。</p>
<p>假设这个数据集中有5000个惟一的单词，我们希望为每个单词创建大小为100维的向量。然后，对于下面给出的word2vec架构:</p>
<ul>
<li>V = 5000(词汇量)</li>
<li>N = 100(隐藏单元数量或单词embeddings长度)</li>
</ul>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/78ugqeahim.png" alt="img"></p>
<p>输入将是一个<strong>热编码向量</strong>，而输出层将给出词汇表中<strong>每个单词都在其附近的概率</strong>。</p>
<p>一旦对该模型进行训练，我们就可以很容易地提取学习到的权值矩阵 </p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/455ulptdh8.png" alt="img"></p>
<p>x N，并用它来提取单词向量:</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/n01f88stii.png" alt="img"></p>
<p>正如你在上面看到的，权重矩阵的形状为5000 x 100。这个矩阵的第一行对应于词汇表中的第一个单词，第二个对应于第二个单词，以此类推。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/9sffijf6gc.png" alt="img"></p>
<p>这就是我们如何通过word2vec得到固定大小的词向量或embeddings。这个数据集中相似的单词会有相似的向量，即指向相同方向的向量。例如，单词“car”和“jeep”有类似的向量:</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/b5dbjph3bi.png" alt="img"></p>
<p>这是对word2vec如何在NLP中使用的高级概述。</p>
<p>在我们开始构建推荐系统之前，让我问你一个问题。如何将word2vec用于非nlp任务，如商品推荐?我相信自从你读了这篇文章的标题后，你就一直在想这个问题。让我们一起解出这个谜题。</p>
<h2 id="在非文本数据上应用word2vec模型"><a href="#在非文本数据上应用word2vec模型" class="headerlink" title="在非文本数据上应用word2vec模型"></a><strong>在非文本数据上应用word2vec模型</strong></h2><p>你能猜到word2vec用来创建文本向量表示的自然语言的基本特性吗?</p>
<p>是<strong>文本的顺序性</strong>。每个句子或短语都有一个单词序列。如果没有这个顺序，我们将很难理解文本。试着解释下面这句话:</p>
<blockquote>
<p>“these most been languages deciphered written of have already”</p>
</blockquote>
<p>这个句子没有顺序，我们很难理解它，这就是为什么在任何自然语言中，单词的顺序是如此重要。正是这个特性让我想到了其他不像文本具有顺序性质的数据。</p>
<p>其中一类数据是<strong>消费者在电子商务网站的购买行为</strong>。大多数时候，消费者的购买行为都有一个模式，例如，一个从事体育相关活动的人可能有一个类似的在线购买模式:</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/e7v1rjzihf.png" alt="img"></p>
<p>如果我们可以用向量表示每一个商品，那么我们可以很容易地找到相似的商品。因此，如果用户在网上查看一个商品，那么我们可以通过使用商品之间的向量相似性评分轻松地推荐类似商品。</p>
<p>但是我们如何得到这些商品的向量表示呢?我们可以用word2vec模型来得到这些向量吗?</p>
<p>答案当然是可以的! 把消费者的购买历史想象成一句话，而把商品想象成这句话的单词:</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/v2vl4bkiqr.png" alt="img"></p>
<p>更进一步，让我们研究在线零售数据，并使用word2vec构建一个推荐系统。</p>
<h2 id="案例研究-使用Python中的word2vec进行在线商品推荐"><a href="#案例研究-使用Python中的word2vec进行在线商品推荐" class="headerlink" title="案例研究:使用Python中的word2vec进行在线商品推荐"></a><strong>案例研究:使用Python中的word2vec进行在线商品推荐</strong></h2><p>现在让我们再一次确定我们的问题和需求：</p>
<p>我们被要求创建一个系统，根据消费者过去的购买行为，自动向电子商务网站的消费者推荐一定数量的商品。</p>
<p>我们将使用一个在线零售数据集，你可以从这个链接下载:</p>
<blockquote>
<p><a href="https://archive.ics.uci.edu/ml/machine-learning-databases/00352/">https://archive.ics.uci.edu/ml/machine-learning-databases/00352/</a></p>
</blockquote>
<p>详细代码见：<a href="https://github.com/Guadzilla/recpre/blob/master/task7/word2vec_demo.ipynb">recpre/word2vec_demo.ipynb at master · Guadzilla/recpre (github.com)</a> 因为和原文差不多，就不重复介绍了。</p>
<h1 id="在-Movielens-数据集上用-Word2Vec-对用户聚类"><a href="#在-Movielens-数据集上用-Word2Vec-对用户聚类" class="headerlink" title="在 Movielens 数据集上用 Word2Vec 对用户聚类"></a>在 Movielens 数据集上用 Word2Vec 对用户聚类</h1><p>见 word2vec_cluster.ipynb</p>
<h3 id="导入相关包"><a href="#导入相关包" class="headerlink" title="导入相关包"></a>导入相关包</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> umap</span><br><span class="line"><span class="keyword">import</span> umap.plot</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> Word2Vec </span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">&#x27;ignore&#x27;</span>)	</span><br></pre></td></tr></table></figure>
<h3 id="加载数据集"><a href="#加载数据集" class="headerlink" title="加载数据集"></a>加载数据集</h3><p>最后要得到：所有用户的列表 <code>users=[user1,user2,...]</code>，语料库<code>corpus=[[item1,item2,...],[item2,item5,...],...]</code>，和商品字典：<code>items_dict=&#123;item1:&quot;item1的描述&quot;, item2:&quot;item2的描述&quot;,...&#125;</code></p>
<p>这里的语料库其实就是每个用户的购买序列组成的列表，我们把每个商品看作一个词，用户的一个购买序列看作一句话，通过这种方式构建语料库。</p>
<p>构建商品字典是为了方便后续查看相似物品信息。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span>(<span class="params">file_path</span>):</span></span><br><span class="line">    df = pd.read_table(<span class="string">&#x27;../ml-1m/ratings.dat&#x27;</span>, sep=<span class="string">&#x27;::&#x27;</span>, names = [<span class="string">&#x27;userID&#x27;</span>,<span class="string">&#x27;itemID&#x27;</span>,<span class="string">&#x27;Rating&#x27;</span>,<span class="string">&#x27;Zip-code&#x27;</span>])</span><br><span class="line">    movies = pd.read_table(<span class="string">&#x27;../ml-1m/movies.dat&#x27;</span>,sep=<span class="string">&#x27;::&#x27;</span>,names=[<span class="string">&#x27;MovieID&#x27;</span>,<span class="string">&#x27;Title&#x27;</span>,<span class="string">&#x27;Genres&#x27;</span>],encoding=<span class="string">&#x27;ISO-8859-1&#x27;</span>)</span><br><span class="line">    movies[<span class="string">&#x27;content&#x27;</span>] = movies[<span class="string">&#x27;Title&#x27;</span>] + <span class="string">&#x27;__&#x27;</span> + movies[<span class="string">&#x27;Genres&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 所有userID</span></span><br><span class="line">    users = df[<span class="string">&quot;userID&quot;</span>].unique().tolist()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 存储user的购买历史,每个user对应一个list,每个list当作一句话,所有list作为语料库</span></span><br><span class="line">    corpus = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> tqdm(users):</span><br><span class="line">        temp = df[df[<span class="string">&quot;userID&quot;</span>] == i][<span class="string">&quot;itemID&quot;</span>].tolist()</span><br><span class="line">        corpus.append(temp)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 建立商品字典,方便后续查看相似物品信息</span></span><br><span class="line">    items_dict = movies.groupby(<span class="string">&#x27;MovieID&#x27;</span>)[<span class="string">&#x27;content&#x27;</span>].apply(<span class="built_in">list</span>).to_dict()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> users, corpus, items_dict</span><br></pre></td></tr></table></figure>
<p>Movielens-1m 数据集很完整，没有缺失值要处理。</p>
<h3 id="训练-Word2Vec"><a href="#训练-Word2Vec" class="headerlink" title="训练 Word2Vec"></a>训练 Word2Vec</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 训练word2vec模型</span></span><br><span class="line">model = Word2Vec(window = <span class="number">10</span>, sg = <span class="number">1</span>, hs = <span class="number">0</span>, negative = <span class="number">10</span>, alpha=<span class="number">0.03</span>, min_alpha=<span class="number">0.0007</span>, seed = <span class="number">14</span>)</span><br><span class="line">model.build_vocab(corpus, progress_per=<span class="number">200</span>)</span><br><span class="line">model.train(corpus, total_examples = model.corpus_count, epochs=<span class="number">10</span>, report_delay=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 模型训练完成, init_sims()提高内存运行效率</span></span><br><span class="line">model.init_sims(replace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h3 id="查看模型参数"><a href="#查看模型参数" class="headerlink" title="查看模型参数"></a>查看模型参数</h3><p>共有3416个 item embedding ,每个维度为100.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 打印模型</span></span><br><span class="line"><span class="built_in">print</span>(model)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Word2Vec(vocab=3416, vector_size=100, alpha=0.03)</span></span><br></pre></td></tr></table></figure>
<h3 id="查看相似物品"><a href="#查看相似物品" class="headerlink" title="查看相似物品"></a>查看相似物品</h3><p>首先提取所有向量</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 提取向量</span></span><br><span class="line">X = model.wv[model.wv.key_to_index.keys()]</span><br></pre></td></tr></table></figure>
<p>查看 movies 信息，这里我们就选第一个电影 “Toy Story”，看看它的相似电影</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">movies = pd.read_table(<span class="string">&#x27;../ml-1m/movies.dat&#x27;</span>,sep=<span class="string">&#x27;::&#x27;</span>,names=[<span class="string">&#x27;MovieID&#x27;</span>,<span class="string">&#x27;Title&#x27;</span>,<span class="string">&#x27;Genres&#x27;</span>],encoding=<span class="string">&#x27;ISO-8859-1&#x27;</span>)</span><br><span class="line">movies.head()</span><br></pre></td></tr></table></figure>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220427172027663.png" alt="image-20220427172027663" style="zoom:67%;" /></p>
<p>Toy Story 的 key 就是对应的 MovieID = 1，通过模型内置的字典找到它对应的索引为 29</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.wv.key_to_index[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 29</span></span><br></pre></td></tr></table></figure>
<p>计算并返回与它相似的10部电影</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">similar_products</span>(<span class="params">v, n = <span class="number">10</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    返回最相似的n个物品</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 为输入向量提取最相似的商品</span></span><br><span class="line">    ms = model.wv.similar_by_vector(v, topn= n+<span class="number">1</span>)[<span class="number">1</span>:]</span><br><span class="line">    <span class="comment"># 提取相似产品的名称和相似度评分</span></span><br><span class="line">    new_ms = []</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> ms:</span><br><span class="line">        pair = (items_dict[j[<span class="number">0</span>]][<span class="number">0</span>], j[<span class="number">1</span>])</span><br><span class="line">        new_ms.append(pair)</span><br><span class="line">    <span class="keyword">return</span> new_ms   </span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> similar_products(X[<span class="number">29</span>]):</span><br><span class="line">    <span class="built_in">print</span>(i[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Aladdin (1992)__Animation|Children&#x27;s|Comedy|Musical</span></span><br><span class="line"><span class="string">Silence of the Lambs, The (1991)__Drama|Thriller</span></span><br><span class="line"><span class="string">Train of Life (Train De Vie) (1998)__Comedy|Drama</span></span><br><span class="line"><span class="string">Home Alone (1990)__Children&#x27;s|Comedy</span></span><br><span class="line"><span class="string">Jumanji (1995)__Adventure|Children&#x27;s|Fantasy</span></span><br><span class="line"><span class="string">Waiting to Exhale (1995)__Comedy|Drama</span></span><br><span class="line"><span class="string">Ghost (1990)__Comedy|Romance|Thriller</span></span><br><span class="line"><span class="string">Beavis and Butt-head Do America (1996)__Animation|Comedy</span></span><br><span class="line"><span class="string">Tom and Huck (1995)__Adventure|Children&#x27;s</span></span><br><span class="line"><span class="string">Brady Bunch Movie, The (1995)__Comedy</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p>Toy Story 的类别是 Animation|Children’s|Comedy ，与它相似的电影也大都是动画片、儿童电影，说明 word2vec 一定程度上的确给相似的电影学到了相似的embedding.</p>
<p>其中第2部 Silence of the Lambs , 现实中喜欢玩具总动员的大多是有冒险精神的大人，所以性格的另一面喜欢沉默的羔羊也是可以的（狡辩）</p>
<h3 id="对物品和用户向量进行聚类"><a href="#对物品和用户向量进行聚类" class="headerlink" title="对物品和用户向量进行聚类"></a>对物品和用户向量进行聚类</h3><p>定义用来聚类和画图的函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 两种画图方法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">visualize_emb</span>(<span class="params">X</span>):</span></span><br><span class="line">    cluster_embedding = umap.UMAP(n_neighbors=<span class="number">30</span>, min_dist=<span class="number">0.0</span>,</span><br><span class="line">    n_components=<span class="number">2</span>, random_state=<span class="number">42</span>).fit_transform(X)</span><br><span class="line">    plt.figure(figsize=(<span class="number">10</span>,<span class="number">9</span>))</span><br><span class="line">    plt.scatter(cluster_embedding[:, <span class="number">0</span>], cluster_embedding[:, <span class="number">1</span>], s=<span class="number">3</span>, cmap=<span class="string">&#x27;Spectral&#x27;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">umap_plot_emb</span>(<span class="params">X</span>):</span></span><br><span class="line">    cluster_embedding = umap.UMAP(n_neighbors=<span class="number">30</span>, min_dist=<span class="number">0.0</span>,</span><br><span class="line">    n_components=<span class="number">2</span>, random_state=<span class="number">42</span>).fit(X)</span><br><span class="line">    umap.plot.points(cluster_embedding)</span><br></pre></td></tr></table></figure>
<h4 id="可视化物品向量"><a href="#可视化物品向量" class="headerlink" title="可视化物品向量"></a>可视化物品向量</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">visualize_emb(X)</span><br><span class="line">umap_plot_emb(X)</span><br></pre></td></tr></table></figure>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220427164950421.png" alt="image-20220427164950421" style="zoom:50%;" /><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220427165014070.png" alt="image-20220427165014070" style="zoom: 41%;" /></p>
<p>居然类似一个圆？意思是接近随机生成吗？</p>
<h4 id="可视化用户向量"><a href="#可视化用户向量" class="headerlink" title="可视化用户向量"></a>可视化用户向量</h4><p>用户的向量可以用看过的电影的向量求均值表示，也可以对一部分求均值表示</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">aggregate_vectors</span>(<span class="params">products</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    返回用户向量</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    product_vec = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> products:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            product_vec.append(model.wv[i])</span><br><span class="line">        <span class="keyword">except</span> KeyError:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">return</span> np.mean(product_vec, axis=<span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">usersVec_all_item = []  <span class="comment"># 对看过的所有电影的向量求均值</span></span><br><span class="line">usersVec_last_10 = []   <span class="comment"># 对看过的最后10个电影的向量求均值</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(users)):</span><br><span class="line">    usersVec_all_item.append(aggregate_vectors(corpus[i]))</span><br><span class="line">    usersVec_last_10.append(aggregate_vectors(corpus[i][-<span class="number">10</span>:]))</span><br></pre></td></tr></table></figure>
<p>用户向量 = 看过的所有电影的向量的均值，可视化结果</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">visualize_emb(usersVec_all_item)</span><br><span class="line">umap_plot_emb(usersVec_all_item)</span><br></pre></td></tr></table></figure>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220427165406864.png" alt="image-20220427165406864" style="zoom:50%;" /><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220427165450274.png" alt="image-20220427165450274" style="zoom:41%;" /></p>
<p>用户向量 = 看过的最后10部电影的向量的均值，可视化结果</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">visualize_emb(usersVec_last_10)</span><br><span class="line">umap_plot_emb(usersVec_last_10)</span><br></pre></td></tr></table></figure>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220427165724232.png" alt="image-20220427165724232" style="zoom:50%;" /><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220427165735996.png" alt="image-20220427165735996" style="zoom:41%;" /></p>
<p>至此完成了利用word2vec训练得到物品的embedding，再用物品embedding表示用户embedding，最后对用户embedding聚类。</p>
<h1 id="在-Movielens-数据集上用-Word2Vec-对用户进行推荐"><a href="#在-Movielens-数据集上用-Word2Vec-对用户进行推荐" class="headerlink" title="在 Movielens 数据集上用 Word2Vec 对用户进行推荐"></a>在 Movielens 数据集上用 Word2Vec 对用户进行推荐</h1><p>见 word2vec_rec.py</p>
<p>如果要对用户推荐商品，就要比上面的简单对用户进行聚类稍微麻烦一点，因为需要划分训练集和验证集。划分数据集的代码如下：</p>
<p>对数据集进行划分，我们在这一步需要获取的是，训练集、测试集出现了哪些用户，训练集的语料库，以及商品字典。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span>(<span class="params">file_path</span>):</span></span><br><span class="line">    data = pd.read_table(<span class="string">&#x27;../ml-1m/ratings.dat&#x27;</span>, sep=<span class="string">&#x27;::&#x27;</span>, names = [<span class="string">&#x27;userID&#x27;</span>,<span class="string">&#x27;itemID&#x27;</span>,<span class="string">&#x27;Rating&#x27;</span>,<span class="string">&#x27;Zip-code&#x27;</span>])</span><br><span class="line">    movies = pd.read_table(<span class="string">&#x27;../ml-1m/movies.dat&#x27;</span>,sep=<span class="string">&#x27;::&#x27;</span>,names=[<span class="string">&#x27;MovieID&#x27;</span>,<span class="string">&#x27;Title&#x27;</span>,<span class="string">&#x27;Genres&#x27;</span>],encoding=<span class="string">&#x27;ISO-8859-1&#x27;</span>)</span><br><span class="line">    movies[<span class="string">&#x27;content&#x27;</span>] = movies[<span class="string">&#x27;Title&#x27;</span>] + <span class="string">&#x27;__&#x27;</span> + movies[<span class="string">&#x27;Genres&#x27;</span>]</span><br><span class="line">    <span class="comment"># </span></span><br><span class="line">    tra_data, val_data = train_test_split(data, test_size=<span class="number">0.2</span>)</span><br><span class="line">    users_train = tra_data[<span class="string">&#x27;userID&#x27;</span>].unique().tolist()</span><br><span class="line">    users_valid = val_data[<span class="string">&#x27;userID&#x27;</span>].unique().tolist()</span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;训练集&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 存储消费者的购买历史</span></span><br><span class="line">    train_users = &#123;&#125;</span><br><span class="line">    train_corpus = []</span><br><span class="line">    <span class="comment"># 用 itemID 填充列表</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> tqdm(users_train):</span><br><span class="line">        temp = tra_data[tra_data[<span class="string">&quot;userID&quot;</span>] == i][<span class="string">&quot;itemID&quot;</span>].tolist()</span><br><span class="line">        train_users[i] = temp</span><br><span class="line">        train_corpus.append(temp)</span><br><span class="line">    <span class="string">&quot;&quot;&quot;验证集&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 存储消费者的购买历史</span></span><br><span class="line">    valid_users = &#123;&#125;</span><br><span class="line">    valid_corpus = []</span><br><span class="line">    <span class="comment"># 用商品代码填充列表</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> tqdm(users_valid):</span><br><span class="line">        temp = val_data[val_data[<span class="string">&quot;userID&quot;</span>] == i][<span class="string">&quot;itemID&quot;</span>].tolist()</span><br><span class="line">        valid_users[i] = temp</span><br><span class="line">        valid_corpus.append(temp)</span><br><span class="line">    <span class="string">&quot;&quot;&quot;建立商品字典&quot;&quot;&quot;</span></span><br><span class="line">    items_dict = movies.groupby(<span class="string">&#x27;MovieID&#x27;</span>)[<span class="string">&#x27;content&#x27;</span>].apply(<span class="built_in">list</span>).to_dict()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> train_users, train_corpus, valid_users, valid_corpus, items_dict</span><br></pre></td></tr></table></figure>
<p>其它步骤与前面的相似，具体看代码吧。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">train_users, train_corpus, valid_users, valid_corpus, items_dict = load_data(<span class="string">&#x27;../ml-1m&#x27;</span>)</span><br><span class="line"><span class="comment"># train_users: &#123;user1:[item1,item2,...],user2:[item2,item5,...],...&#125;</span></span><br><span class="line"><span class="comment"># train_corpus: [[item1,item2,...],[item2,item5,...],...]</span></span><br><span class="line"><span class="comment"># 训练word2vec模型</span></span><br><span class="line">model = Word2Vec(window = <span class="number">10</span>, sg = <span class="number">1</span>, hs = <span class="number">0</span>, negative = <span class="number">10</span>, alpha=<span class="number">0.03</span>, min_alpha=<span class="number">0.0007</span>, seed = <span class="number">14</span>)</span><br><span class="line">model.build_vocab(train_corpus, progress_per=<span class="number">200</span>)</span><br><span class="line">model.train(train_corpus, total_examples = model.corpus_count, epochs=<span class="number">10</span>, report_delay=<span class="number">1</span>)</span><br><span class="line">model.init_sims(replace=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 打印模型</span></span><br><span class="line"><span class="built_in">print</span>(model)</span><br><span class="line"><span class="comment"># 提取向量</span></span><br><span class="line">X = model.wv[model.wv.key_to_index.keys()]</span><br><span class="line"><span class="comment"># 可视化</span></span><br><span class="line"><span class="comment">#visualize_emb(X)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">similar_products_idx</span>(<span class="params">v, n = <span class="number">10</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    返回最相似的n个物品</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 为输入向量提取最相似的商品</span></span><br><span class="line">    ms = model.wv.similar_by_vector(v, topn= n+<span class="number">1</span>)[<span class="number">1</span>:]</span><br><span class="line">    <span class="keyword">return</span> ms  </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">aggregate_vectors</span>(<span class="params">products</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    返回购买记录的平均向量</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    product_vec = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> products:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            product_vec.append(model.wv[i])</span><br><span class="line">        <span class="keyword">except</span> KeyError:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">return</span> np.mean(product_vec, axis=<span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 推荐TopN相似商品</span></span><br><span class="line">rec_dict = &#123;&#125;</span><br><span class="line">rel_dict = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> valid_users:    <span class="comment"># valid_users:&#123;user1:[item1,item2,...],user2:[item2,item5,...],...&#125;</span></span><br><span class="line">    <span class="keyword">if</span> user <span class="keyword">not</span> <span class="keyword">in</span> train_users:</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    user_vec = aggregate_vectors(train_users[user][-<span class="number">10</span>:])</span><br><span class="line">    similar_items = similar_products_idx(user_vec,<span class="number">10</span>)</span><br><span class="line">    rec_dict[user] = [x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> similar_items]</span><br><span class="line">    rel_dict[user] = valid_users[user]</span><br><span class="line"></span><br><span class="line"><span class="comment"># evaluate</span></span><br><span class="line">rec_eval(rec_dict,rel_dict,train_users)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>评估结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">recall: <span class="number">1.31</span></span><br><span class="line">precision <span class="number">4.34</span></span><br><span class="line">coverage <span class="number">71.29</span></span><br><span class="line">Popularity <span class="number">4.93</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>推荐系统基础</tag>
      </tags>
  </entry>
  <entry>
    <title>推荐系统基础8</title>
    <url>/2022/04/27/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%808/</url>
    <content><![CDATA[<hr>
<h1 id="任务8：向量召回基础"><a href="#任务8：向量召回基础" class="headerlink" title="任务8：向量召回基础"></a>任务8：向量召回基础</h1><ul>
<li>基于任务7的基础上，使用编码后的用户向量，计算用户相似度。</li>
<li>参考User-CF的过程，通过用户相似度得到电影推荐</li>
</ul>
<p>代码地址： <a href="https://github.com/Guadzilla/Basics-of-Recsys">https://github.com/Guadzilla/Basics-of-Recsys</a></p>
<span id="more"></span>
<p>任务 7 中，我们在 Movielens-1m 数据集上用 word2vec 训练得到了 电影的”词向量”（物品向量、item embedding，都是一个意思，下面都用 item embedding），再对用户所有看过的电影的 embedding 取均值作为”句向量“（用户向量，user embedding）。</p>
<p>在任务 7 的 word2vec_rec.py 里，实现了在 Movielens 数据集上用 Word2Vec 对用户进行推荐，主要步骤为：<strong>划分数据集、训练word2vec模型、计算用户向量</strong>、TopN推荐（计算相似物品）。</p>
<p>任务 8 就以它为基础，稍作修改，主要步骤为：<strong>划分数据集、训练word2vec模型、计算用户向量</strong>、计算用户相似度、建立向量库、构建用户相似性矩阵、利用UserCF算法进行TopN推荐。因为前三步骤和之前类似，不做过多介绍，这里重点介绍后四步。</p>
<h2 id="计算用户相似度"><a href="#计算用户相似度" class="headerlink" title="计算用户相似度"></a>计算用户相似度</h2><p>计算用户相似度的思路是，利用 faiss 库建立向量库，存入所有 user embedding，再用 user embedding 对向量库进行检索，取出 TopK 个最相似的用户，用户后续的 UserCF 推荐。</p>
<p>因为用 faiss 构建向量库会对 userID 重新从0开始索引，取出结果也是根据重新索引后的 index 来取，所以有必要提前对训练集的 userID 重建索引，用一个字典映射来实现。</p>
<p>实际上在 load 数据集的时候已经对 userID 和 itemID 重新索引了，这里为什么还要再做一次索引呢？</p>
<p>答：为了方便索引，使得训练集和验证集可以直接对向量库进行索引。具体来说，划分数据集会造成训练集和验证集上的用户数量不一致，也就是说可能有一部分用户只在验证集出现，训练集里没有他。例如：全部数据集有10个用户，并且已经对他们从零开始编号，<code>userID=[0,1,2,3,4,5,6,7,8,9]</code>。随机划分数据集以后，训练集里可能就只有<code>userID = [0,2,3,4,5,6,7,8]</code> 这8个用户了，验证集里只有 <code>userID = [0,1,2,3,4,5,6,7,9]</code> 9个用户。此时如果直接把训练集的 user embedding 直接放到索引库里，faiss 为向量库构建的索引为<code>index = [0,1,2,3,4,5,6,7]</code>，对应关系是<code>index=0对应userID=0</code>，<code>index=1对应userID=2</code>，…，这个对应关系必须保存下来，否则验证集只有 userID ，无法定位到 userID 对应哪个索引，也就无法提取该 user embedding。如果用字典 <code>MAP(key=userID,value=index)</code>保存下来这个对应关系，在训练集上就可以用<code>MAP[userID]</code>作为索引从向量库提取 user embedding ；验证集上，首先把在验证集首次出现的用户单独保存，剩下<code>userID = [0,2,3,4,5,6,7]</code>，其余一样。</p>
<p>在这里使用了另一种思路，先对训练集 <code>userID = [0,2,3,4,5,6,7,8]</code> 重新索引成 <code>userID = [0,1,2,3,4,5,6,7]</code> ，并保存下这个字典<code>MAP(key=userID,value=index)</code>，再把验证集  <code>userID = [0,1,2,3,4,5,6,7,9]</code> 的 userID 都 MAP 到 index 上，其中 1 和 9 在训练集上没有出现，单独保存，剩下的<code>userID = [0,2,3,4,5,6,7]</code>再做映射，变成 <code>userID = [0,1,2,3,4,5,6]</code>，这样做方便在以后就可以直接用 userID 访问向量库。</p>
<p>两种方式都可以，第一种方式的缺点是频繁访问字典，但优点是不用 in place 修改数据；第二种方式正好相反，不用频繁访问字典，但是需要 in place 修改数据。个人习惯用第二种方式，一劳永逸。</p>
<p>下面是代码部分：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">TopK = <span class="number">100</span></span><br><span class="line">TopN = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对train_user重建索引, 求用户向量, 求重索引的new_train_users, 只需要一次循环就能做完</span></span><br><span class="line"><span class="comment"># 已有 train_users  # train_users: &#123;user1:[item1,item2,...], user2:[item3,item4,...],...&#125;</span></span><br><span class="line">count = <span class="number">0</span></span><br><span class="line">MAP = <span class="built_in">dict</span>()   <span class="comment"># 对train_user重建索引,从0开始   MAP: &#123;userID:faiss_idx, ...&#125;</span></span><br><span class="line">usersVec = []  <span class="comment"># 求用户向量, aggregate_vectors():对看过的所有电影的向量求均值</span></span><br><span class="line">new_train_users = &#123;&#125;    <span class="comment"># new_train_users: &#123;faiss_idx1:[item1,item2,...],faiss_idx2:[item3,item4,...],...&#125;</span></span><br><span class="line"><span class="keyword">for</span> user,items <span class="keyword">in</span> train_users.items():</span><br><span class="line">    MAP[user] = count</span><br><span class="line">    new_train_users[count] = items</span><br><span class="line">    usersVec.append(aggregate_vectors(train_corpus[count]))</span><br><span class="line">    count += <span class="number">1</span></span><br><span class="line"><span class="keyword">del</span> train_users</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对valid_users的userid按MAP重索引,得到new_valid_users: &#123;faiss_idx1:[item1,item2,...],faiss_idx2:[item3,item4,...],...&#125;</span></span><br><span class="line">new_valid_users = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> user,items <span class="keyword">in</span> valid_users.items():</span><br><span class="line">    <span class="keyword">if</span> user <span class="keyword">not</span> <span class="keyword">in</span> new_train_users:</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    new_valid_users[MAP[user]] = items</span><br><span class="line"><span class="keyword">del</span> valid_users</span><br></pre></td></tr></table></figure>
<h2 id="建立向量索引库"><a href="#建立向量索引库" class="headerlink" title="建立向量索引库"></a>建立向量索引库</h2><p>要用 faiss 计算余弦相似度，需要注意的是 faiss 自带的两种常见相似算法是：<code>faiss.IndexFlatL2()</code>用来计算向量距离，和<code>faiss.IndexFlatIP()</code>用来计算向量内积。计算余弦相似度可以用向量内积形式，但前提是需要先把向量转成单位向量，faiss 自带了 <code>faiss.normalize_L2()</code>就是用来单位化向量的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 建立用户向量索引库</span></span><br><span class="line"><span class="keyword">import</span> faiss</span><br><span class="line">usersVec = np.array(usersVec)</span><br><span class="line">normalize_L2(usersVec)   <span class="comment"># 这是inplace操作 (计算余弦相似度,先正则化,再计算内积)</span></span><br><span class="line">index = faiss.IndexFlatIP(<span class="number">100</span>)</span><br><span class="line">index.add(usersVec)</span><br></pre></td></tr></table></figure>
<h2 id="构建用户相似性矩阵"><a href="#构建用户相似性矩阵" class="headerlink" title="构建用户相似性矩阵"></a>构建用户相似性矩阵</h2><p>构建用户相似性的思路是用字典保存，因为矩阵太稀疏，浪费空间。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 找TopK相似用户</span></span><br><span class="line">D, I = index.search(usersVec[<span class="built_in">list</span>(new_valid_users.keys())], TopK+<span class="number">1</span>) <span class="comment"># TopK要+1,因为底下计算相似度会计算自身一次</span></span><br><span class="line">similar_users_idxs = I</span><br><span class="line">similar_users = &#123;&#125;  <span class="comment"># similar_users: &#123;faiss_idx1:&#123;faiss_idx2:score,faiss_idx4:score,...&#125;,faiss_idx2:&#123;...&#125;,...&#125;</span></span><br><span class="line"><span class="keyword">for</span> idx,val_user <span class="keyword">in</span> <span class="built_in">enumerate</span>(new_valid_users):</span><br><span class="line">    similar_users[val_user] = <span class="built_in">dict</span>()</span><br><span class="line">    <span class="keyword">for</span> iidx,s_user <span class="keyword">in</span> <span class="built_in">enumerate</span>(similar_users_idxs[idx]):</span><br><span class="line">        similar_users[val_user][s_user] = D[idx][iidx]</span><br></pre></td></tr></table></figure>
<h2 id="利用-UserCF-算法进行-TopN-推荐"><a href="#利用-UserCF-算法进行-TopN-推荐" class="headerlink" title="利用 UserCF 算法进行 TopN 推荐"></a>利用 UserCF 算法进行 TopN 推荐</h2><p>UserCF 的思路，先找到和目标用户最相似的 TopK 个用户（相似度用于计算目标用户对陌生物品的得分）再计算这些用户看过的、且目标用户没看过的电影的得分，对有得分的物品进行排序，进行 TopN 推荐。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 推荐TopN相似商品</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;开始生成推荐列表...&#x27;</span>)</span><br><span class="line">rec_dict = &#123;&#125;</span><br><span class="line">rel_dict = new_valid_users</span><br><span class="line"><span class="keyword">for</span> val_user <span class="keyword">in</span> tqdm(new_valid_users):   <span class="comment"># new_valid_users: &#123;faiss_idx1:[item1,item2,...],faiss_idx2:[item3,item4,...],...&#125;</span></span><br><span class="line">    rec_dict[val_user] = <span class="built_in">dict</span>()</span><br><span class="line">    <span class="keyword">for</span> user <span class="keyword">in</span> similar_users[val_user]:</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> new_train_users[user]:</span><br><span class="line">            <span class="keyword">if</span> item <span class="keyword">not</span> <span class="keyword">in</span> new_train_users[val_user]:</span><br><span class="line">                <span class="keyword">if</span> item <span class="keyword">not</span> <span class="keyword">in</span> rec_dict[val_user]:</span><br><span class="line">                    rec_dict[val_user][item]=<span class="number">0</span></span><br><span class="line">                rec_dict[val_user][item] += similar_users[val_user][user]</span><br><span class="line"><span class="comment"># rec_dict: &#123;faiss_idx1:&#123;item2:score,item4:score,...&#125;,faiss_idx2:&#123;...&#125;,...&#125;</span></span><br><span class="line"><span class="comment"># 先选出每个user的TopN &quot;item-score&quot; 对，再提出item到最后的推荐列表, 变换后的rec_dict: &#123;faiss_idx1:[item2,item4,...],faiss_idx2:[item3,item4,...],...&#125;</span></span><br><span class="line">rec_dict = &#123;k: <span class="built_in">sorted</span>(v.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>)[:TopN] <span class="keyword">for</span> k, v <span class="keyword">in</span> rec_dict.items()&#125;</span><br><span class="line">rec_dict = &#123;k: <span class="built_in">list</span>([x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> v]) <span class="keyword">for</span> k, v <span class="keyword">in</span> rec_dict.items()&#125;</span><br></pre></td></tr></table></figure>
<p>最后进行评估，验证集上实验结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># evaluate</span></span><br><span class="line">rec_eval(rec_dict,rel_dict,new_train_users)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">recall: 6.01</span></span><br><span class="line"><span class="string">precision 19.91</span></span><br><span class="line"><span class="string">coverage 9.38</span></span><br><span class="line"><span class="string">Popularity 7.459</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p>完整代码见 <a href="https://github.com/Guadzilla/recpre/tree/master/task8/word2vec_recall.py">recpre/task8 at master · Guadzilla/recpre (github.com)</a></p>
]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>推荐系统基础</tag>
      </tags>
  </entry>
  <entry>
    <title>推荐系统基础9</title>
    <url>/2022/04/28/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%809/</url>
    <content><![CDATA[<hr>
<h1 id="任务9：多路召回实践"><a href="#任务9：多路召回实践" class="headerlink" title="任务9：多路召回实践"></a>任务9：多路召回实践</h1><ul>
<li>基于任务3、任务5、任务6、任务7、任务8，总共5个召回模型，进行多路召回。</li>
<li>可以考虑对每个召回模型的物品打分进行相加，也可以加权求和。</li>
<li>分别计算每个模型 &amp; 多路召回模型的Top10、Top20、Top50的命中率。</li>
</ul>
<p>代码地址： <a href="https://github.com/Guadzilla/Basics-of-Recsys">https://github.com/Guadzilla/Basics-of-Recsys</a></p>
<span id="more"></span>
<p>根据前面完成的任务，5个召回模型分别是 ItemCF、UserCF、MF、SlopeOne、Word2Vec。</p>
<p><strong>任务目标</strong>：对用户是否评分做出预测，即评分只有0和1（数据集中用户评分过的电影评分都取1，未评分过的电影评分都取0），进行推荐 TopN 推荐。</p>
<p>这样的话 SlopeOne 没法做了，因为评分只有0和1，有评分的哪些物品的评分都是1，物品评分之间没有均差。当然也可以把任务改为预测5分制的评分；或者两阶段预测，先预测是否会评分，再预测会评多少分。那样都需要对模型做不少修改，暂时没有精力做，图省事就只实现了4个召回模型 ItemCF、UserCF、MF、Word2Vec。</p>
<p><strong>进行多路召回（模型融合），步骤大致分为以下几步</strong>：</p>
<ol>
<li>划分数据集并保存</li>
<li>各模型读取数据进行训练</li>
<li>各模型进行预测，保存评估指标、保存预测结果</li>
<li>读取所有预测结果进行加权（取均值、或者训练得到权重），进行最终评估</li>
</ol>
<h2 id="划分数据集"><a href="#划分数据集" class="headerlink" title="划分数据集"></a>划分数据集</h2><p>多路召回，首先保证各个模型上数据集划分是一致的。所以考虑把划分数据集这一部分独立出来单独运行。划分完的数据要保存下来，方便后续各个模型加载并独立做实验。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_and_save</span>(<span class="params">load_path, save_path, test_rate = <span class="number">0.1</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    加载数据，分割训练集、验证集</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    data = pd.read_table(os.path.join(load_path,<span class="string">&#x27;ratings.dat&#x27;</span>), sep=<span class="string">&#x27;::&#x27;</span>, names = [<span class="string">&#x27;userID&#x27;</span>,<span class="string">&#x27;itemID&#x27;</span>,<span class="string">&#x27;Rating&#x27;</span>,<span class="string">&#x27;Zip-code&#x27;</span>])</span><br><span class="line">    data = data[[<span class="string">&#x27;userID&#x27;</span>,<span class="string">&#x27;itemID&#x27;</span>]]</span><br><span class="line">    uid_lbe = LabelEncoder()</span><br><span class="line">    data[<span class="string">&#x27;userID&#x27;</span>] = uid_lbe.fit_transform(data[<span class="string">&#x27;userID&#x27;</span>])</span><br><span class="line">    iid_lbe = LabelEncoder()</span><br><span class="line">    data[<span class="string">&#x27;itemID&#x27;</span>] = iid_lbe.fit_transform(data[<span class="string">&#x27;itemID&#x27;</span>])</span><br><span class="line"></span><br><span class="line">    train_data, valid_data = train_test_split(data,test_size=test_rate)</span><br><span class="line">    </span><br><span class="line">    train_users = train_data.groupby(<span class="string">&#x27;userID&#x27;</span>)[<span class="string">&#x27;itemID&#x27;</span>].apply(<span class="built_in">list</span>).to_dict()</span><br><span class="line">    valid_users = valid_data.groupby(<span class="string">&#x27;userID&#x27;</span>)[<span class="string">&#x27;itemID&#x27;</span>].apply(<span class="built_in">list</span>).to_dict()</span><br><span class="line"></span><br><span class="line">    train_data.to_csv(<span class="string">&#x27;train_data.csv&#x27;</span>, index=<span class="literal">False</span>)</span><br><span class="line">    valid_data.to_csv(<span class="string">&#x27;valid_data.csv&#x27;</span>, index=<span class="literal">False</span>)</span><br><span class="line">    pickle.dump(train_users, <span class="built_in">open</span>(os.path.join(save_path,<span class="string">&#x27;train_users.txt&#x27;</span>), <span class="string">&#x27;wb&#x27;</span>))</span><br><span class="line">    pickle.dump(valid_users, <span class="built_in">open</span>(os.path.join(save_path,<span class="string">&#x27;valid_users.txt&#x27;</span>), <span class="string">&#x27;wb&#x27;</span>))</span><br><span class="line"></span><br><span class="line">load_and_save(load_path=<span class="string">&#x27;../ml-1m&#x27;</span>, save_path=<span class="string">&#x27;./data&#x27;</span>, test_rate=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure>
<h2 id="训练各模型"><a href="#训练各模型" class="headerlink" title="训练各模型"></a>训练各模型</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ItemCF.py   	--&gt;   基于物品的协同过滤算法</span><br><span class="line">UserCF.py   	--&gt;   基于用户的协同过滤算法</span><br><span class="line">MF.py			--&gt;   梯度下降矩阵分解算法</span><br><span class="line">Word2Vec.py		--&gt;	  word2vec算法</span><br><span class="line"><span class="comment"># 各模型细节就不展示了</span></span><br></pre></td></tr></table></figure>
<h2 id="保存各模型指标"><a href="#保存各模型指标" class="headerlink" title="保存各模型指标"></a>保存各模型指标</h2><p>所有模型的 TopK 都取100，各模型在验证集上的评估指标为：</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220429174656385.png" alt="image-20220429174656385"></p>
<p>保存各模型的预测结果，注意保存的是没有截断 TopN 的推荐列表：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_rec_dict</span>(<span class="params">save_path,rec_dict</span>):</span></span><br><span class="line">    pickle.dump(rec_dict, <span class="built_in">open</span>(os.path.join(save_path,<span class="string">&#x27;word2vec_rec_dict.txt&#x27;</span>), <span class="string">&#x27;wb&#x27;</span>))</span><br></pre></td></tr></table></figure>
<p>保存的格式为字典：<code>Top50_rec_dict=&#123;uid1:&#123;iid1:score,iid3:score,...&#125;, uid2:&#123;iid2:score,iid3:score,...&#125;,...&#125;</code></p>
<h2 id="模型融合"><a href="#模型融合" class="headerlink" title="模型融合"></a>模型融合</h2><p>观察各模型保存的预测得分结果，发现 MF 和另外几个算法得到的数据的值域不在同一区间，如果直接取均值的话， MF 的影响就很小了，所以先对每个用户的物品得分列表做 softmax 再取均值</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220428202209856.png" alt="image-20220428202209856"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_rec_dict</span>(<span class="params">file_path, model_name</span>):</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(os.path.join(file_path,<span class="string">f&#x27;<span class="subst">&#123;model_name&#125;</span>_rec_dict.txt&#x27;</span>),<span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        Rec_dict = pickle.load(f)</span><br><span class="line">    <span class="comment"># Rec_dict:&#123;uid1:[(iid1,score),(iid2,score),...],uid2:[(...),(...),...],...&#125;</span></span><br><span class="line">    new_rec_dict = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> uid, iid_score_list <span class="keyword">in</span> Rec_dict.items():</span><br><span class="line">        new_rec_dict[uid] = <span class="built_in">dict</span>()</span><br><span class="line">        score_list = []</span><br><span class="line">        iid_list = []</span><br><span class="line">        <span class="keyword">for</span> iid, score <span class="keyword">in</span> iid_score_list.items():</span><br><span class="line">            score_list.append(score)</span><br><span class="line">            iid_list.append(iid)</span><br><span class="line">        prob = softmax(score_list)      <span class="comment"># 把物品评分转化成概率</span></span><br><span class="line">        <span class="keyword">for</span> iid, p <span class="keyword">in</span> <span class="built_in">zip</span>(iid_list, prob):</span><br><span class="line">            new_rec_dict[uid][iid] = p</span><br><span class="line">    <span class="keyword">return</span> new_rec_dict</span><br></pre></td></tr></table></figure>
<p>加权取平均融合：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mixup</span>(<span class="params">*rec_dicts</span>):</span></span><br><span class="line">    new_rec_dict = &#123;&#125;</span><br><span class="line">    num = &#123;&#125;    <span class="comment"># 记录目标用户被推荐同一商品几次，后续做除数</span></span><br><span class="line">    <span class="keyword">for</span> rec_dict <span class="keyword">in</span> tqdm(rec_dicts):</span><br><span class="line">        <span class="keyword">for</span> uid <span class="keyword">in</span> rec_dict:</span><br><span class="line">            <span class="keyword">if</span> uid <span class="keyword">not</span> <span class="keyword">in</span> new_rec_dict:</span><br><span class="line">                new_rec_dict[uid] = &#123;&#125;</span><br><span class="line">                num[uid] = &#123;&#125;</span><br><span class="line">            <span class="keyword">for</span> iid, prob <span class="keyword">in</span> rec_dict[uid].items():</span><br><span class="line">                <span class="keyword">if</span> iid <span class="keyword">not</span> <span class="keyword">in</span> new_rec_dict[uid]:</span><br><span class="line">                    new_rec_dict[uid][iid] = <span class="number">0</span></span><br><span class="line">                    num[uid][iid] = <span class="number">0</span></span><br><span class="line">                new_rec_dict[uid][iid] += prob</span><br><span class="line">                num[uid][iid] += <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> uid,iid_score <span class="keyword">in</span> new_rec_dict.items():</span><br><span class="line">        <span class="keyword">for</span> iid <span class="keyword">in</span> iid_score:</span><br><span class="line">            new_rec_dict[uid][iid] /= num[uid][iid]</span><br><span class="line">    <span class="keyword">return</span> new_rec_dict</span><br><span class="line"></span><br><span class="line">new_rec_dict = mixup(*models_rec_dict)</span><br></pre></td></tr></table></figure>
<p>TopN 推荐：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># TopN推荐</span></span><br><span class="line">Top50_rec_dict = &#123;k: <span class="built_in">sorted</span>(v.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>)[:<span class="number">50</span>] <span class="keyword">for</span> k, v <span class="keyword">in</span> new_rec_dict.items()&#125;</span><br><span class="line">Top50_rec_dict = &#123;k: <span class="built_in">list</span>([x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> v]) <span class="keyword">for</span> k, v <span class="keyword">in</span> Top50_rec_dict.items()&#125;</span><br><span class="line">Top10_rec_dict = &#123;k: v[:<span class="number">10</span>] <span class="keyword">for</span> k, v <span class="keyword">in</span> Top50_rec_dict.items()&#125;</span><br><span class="line">Top20_rec_dict = &#123;k: v[:<span class="number">20</span>] <span class="keyword">for</span> k, v <span class="keyword">in</span> Top50_rec_dict.items()&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算评价指标</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stack_eval</span>(<span class="params">val_rec_items, val_user_items, trn_user_items</span>):</span></span><br><span class="line">    recall = Recall(val_rec_items, val_user_items)</span><br><span class="line">    precision = Precision(val_rec_items, val_user_items)</span><br><span class="line">    coverage = Coverage(val_rec_items, trn_user_items)</span><br><span class="line">    popularity = Popularity(val_rec_items, trn_user_items)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;recall&#125;</span>\t<span class="subst">&#123;precision&#125;</span>\t<span class="subst">&#123;coverage&#125;</span>\t<span class="subst">&#123;popularity&#125;</span>&#x27;</span>,end=<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Top 10,20,50:&#x27;</span>)</span><br><span class="line">bag_eval(Top10_rec_dict,valid_users,train_users)</span><br><span class="line">bag_eval(Top20_rec_dict,valid_users,train_users)</span><br><span class="line">bag_eval(Top50_rec_dict,valid_users,train_users)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;\nDone.&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>实验结果：最优结果红色加粗，次优结果橙色加粗。前四行是单模型，后面的都是融合模型，取每个模型的首字母表示每个模型。</p>
<p><img src="https://wjm-images.oss-cn-beijing.aliyuncs.com/img-hosting/image-20220429174808948.png" alt="image-20220429174808948"></p>
<p>居然是单模型UserCF最优…</p>
]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>推荐系统基础</tag>
      </tags>
  </entry>
</search>
